== [[DataSourceStrategy]] DataSourceStrategy Execution Planning Strategy

`DataSourceStrategy` is an link:spark-sql-SparkStrategy.adoc[execution planning strategy] (of link:spark-sql-SparkPlanner.adoc[SparkPlanner]) that plans link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation] logical operators as link:spark-sql-SparkPlan-RowDataSourceScanExec.adoc[RowDataSourceScanExec] physical operators (possibly under `FilterExec` and `ProjectExec` operators).

[[apply]]
[[selection-requirements]]
.DataSourceStrategy's Selection Requirements (in execution order)
[cols="1,2",options="header",width="100%"]
|===
| Logical Operator
| Selection Requirements

| [[CatalystScan]] link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation] with a link:spark-sql-CatalystScan.adoc[CatalystScan] relation
| Uses <<pruneFilterProjectRaw, pruneFilterProjectRaw>>

`CatalystScan` does not seem to be used in Spark SQL.

| [[PrunedFilteredScan]] link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation] with link:spark-sql-PrunedFilteredScan.adoc[PrunedFilteredScan] relation
a| Uses <<pruneFilterProject, pruneFilterProject>>

NOTE: Matches link:spark-sql-JDBCRelation.adoc[JDBCRelation] exclusively (as a link:spark-sql-PrunedFilteredScan.adoc[PrunedFilteredScan])

| [[PrunedScan]] link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation] with a link:spark-sql-PrunedScan.adoc[PrunedScan] relation
a| Uses <<pruneFilterProject, pruneFilterProject>>

NOTE: `PrunedScan` does not seem to be used in Spark SQL.

| [[TableScan]] link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation] with a link:spark-sql-TableScan.adoc[TableScan] relation
a|

NOTE: Matches `KafkaRelation` exclusively (as a link:spark-sql-TableScan.adoc[TableScan])
|===

[source, scala]
----
import org.apache.spark.sql.execution.datasources.DataSourceStrategy
val strategy = DataSourceStrategy(spark.sessionState.conf)

import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
val plan: LogicalPlan = ???

val sparkPlan = strategy(plan).head
----

NOTE: `DataSourceStrategy` uses link:spark-sql-PhysicalOperation.adoc[PhysicalOperation] Scala extractor object to destructure a logical query plan.

=== [[pruneFilterProject]] `pruneFilterProject` Internal Method

[source, scala]
----
pruneFilterProject(
  relation: LogicalRelation,
  projects: Seq[NamedExpression],
  filterPredicates: Seq[Expression],
  scanBuilder: (Seq[Attribute], Array[Filter]) => RDD[InternalRow])
----

`pruneFilterProject` simply calls <<pruneFilterProjectRaw, pruneFilterProjectRaw>> with `scanBuilder` ignoring the `Seq[Expression]` input parameter.

NOTE: `pruneFilterProject` is used when `DataSourceStrategy` execution planning strategy is <<apply, executed>> (for link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation] logical operators with a link:spark-sql-PrunedFilteredScan.adoc[PrunedFilteredScan] or a link:spark-sql-PrunedScan.adoc[PrunedScan]).

=== [[pruneFilterProjectRaw]] Creating RowDataSourceScanExec (under FilterExec and ProjectExec Operators) -- `pruneFilterProjectRaw` Internal Method

[source, scala]
----
pruneFilterProjectRaw(
  relation: LogicalRelation,
  projects: Seq[NamedExpression],
  filterPredicates: Seq[Expression],
  scanBuilder: (Seq[Attribute], Seq[Expression], Seq[Filter]) => RDD[InternalRow]): SparkPlan
----

`pruneFilterProjectRaw` creates a link:spark-sql-SparkPlan-RowDataSourceScanExec.adoc[RowDataSourceScanExec] (possibly as a child of a `FilterExec` and a `ProjectExec`).

NOTE: `pruneFilterProjectRaw` is used exclusively when `DataSourceStrategy` execution planning strategy is <<apply, executed>> and <<pruneFilterProject, pruneFilterProject>>.

=== [[selectFilters]] `selectFilters` Method

[source, scala]
----
selectFilters(
  relation: BaseRelation,
  predicates: Seq[Expression]): (Seq[Expression], Seq[Filter], Set[Filter])
----

`selectFilters`...FIXME

NOTE: `selectFilters` is used exclusively when `DataSourceStrategy` execution planning strategy is requested to <<pruneFilterProjectRaw, create a RowDataSourceScanExec (possibly under FilterExec and ProjectExec operators)>> (which is when `DataSourceStrategy` is <<apply, executed>> and <<pruneFilterProject, pruneFilterProject>>).

=== [[translateFilter]] Translating Catalyst Expression Into Data Source Filter Predicate -- `translateFilter` Method

[source, scala]
----
translateFilter(predicate: Expression): Option[Filter]
----

`translateFilter` translates a Catalyst link:spark-sql-Expression.adoc[expression] into a corresponding link:spark-sql-Filter.adoc[Filter predicate] (if possible):

* EqualTo
* EqualNullSafe
* GreaterThan
* LessThan
* GreaterThanOrEqual
* LessThanOrEqual
* link:spark-sql-Expression-InSet.adoc[InSet]
* link:spark-sql-Expression-In.adoc[In]
* IsNull
* IsNotNull
* And
* Or
* Not
* StartsWith
* EndsWith
* Contains

NOTE: The expressions and their corresponding data source filters have the same names but belong to different packages (except `InSet` and `In` expressions).

[NOTE]
====
`translateFilter` is used when:

* `FileSourceScanExec` is link:spark-sql-SparkPlan-FileSourceScanExec.adoc#creating-instance[created] (and initializes link:spark-sql-SparkPlan-FileSourceScanExec.adoc#pushedDownFilters[pushedDownFilters])

* `DataSourceStrategy` is requested to <<selectFilters, selectFilters>>

* `PushDownOperatorsToDataSource` logical optimization is link:spark-sql-SparkOptimizer-PushDownOperatorsToDataSource.adoc#apply[executed] (for link:spark-sql-LogicalPlan-DataSourceV2Relation.adoc[DataSourceV2Relation] leaf operators with a link:spark-sql-SupportsPushDownFilters.adoc[SupportsPushDownFilters] data source reader)
====

=== [[toCatalystRDD]] Converting RDD of Rows to Catalyst RDD (RDD of InternalRows) -- `toCatalystRDD` Internal Method

[source, scala]
----
toCatalystRDD(
  relation: LogicalRelation,
  output: Seq[Attribute],
  rdd: RDD[Row]): RDD[InternalRow]
toCatalystRDD(relation: LogicalRelation, rdd: RDD[Row]) // <1>
----
<1> Calls the former `toCatalystRDD` with the link:spark-sql-LogicalPlan-LogicalRelation.adoc#output[output] of the `LogicalRelation`

`toCatalystRDD` branches off per the link:spark-sql-BaseRelation.adoc#needConversion[needConversion] flag of the link:spark-sql-LogicalPlan-LogicalRelation.adoc#relation[BaseRelation] of the input link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation].

When enabled (`true`), `toCatalystRDD` link:spark-sql-RDDConversions.adoc#rowToRowRdd[converts the objects inside Rows to Catalyst types].

Otherwise, `toCatalystRDD` simply casts the input `RDD[Row]` to a `RDD[InternalRow]` (as a simple untyped Scala type conversion).

NOTE: link:spark-sql-BaseRelation.adoc#needConversion[needConversion] flag is enabled (`true`) by default.

NOTE: `toCatalystRDD` is used when `DataSourceStrategy` execution planning strategy is <<apply, executed>>.
