== [[Optimizer]] Catalyst Optimizer -- Generic Rule-Based Logical Optimizer

`Optimizer` (aka *Catalyst Optimizer*) is the generic *rule-based logical query plan optimizer* in Spark SQL that uses link:spark-sql-catalyst.adoc[Catalyst Framework] to optimize link:spark-sql-LogicalPlan.adoc[logical query plans] using <<batches, optimization rules>>.

NOTE: link:spark-sql-SparkOptimizer.adoc[SparkOptimizer] is the one and only custom `Optimizer`.

`Optimizer` is available as link:spark-sql-SessionState.adoc#optimizer[optimizer] of a `SessionState`.

[source, scala]
----
val spark: SparkSession = ...
spark.sessionState.optimizer
----

[[execute]]
`Optimizer` is a link:spark-sql-catalyst-RuleExecutor.adoc[RuleExecutor] that defines <<batches, collection of logical optimization rules>>.

[[extendedOperatorOptimizationRules]]
`Optimizer` has an extension point for additional rules for *operator optimization rules*.

[source, scala]
----
extendedOperatorOptimizationRules: Seq[Rule[LogicalPlan]]
----

The additional operator optimization rules are executed right after <<Operator_Optimization_before_Inferring_Filters, Operator Optimization before Inferring Filters>> and <<Operator_Optimization_after_Inferring_Filters, Operator Optimization after Inferring Filters>>.

[[batches]]
.Optimizer's Default Optimization Batches (in the order of execution)
[cols="2,1,3,3",options="header",width="100%"]
|===
^.^| Batch Name
^.^| Strategy
| Rules
| Description

^.^| [[Eliminate_Distinct]] Eliminate Distinct
^.^| `Once`
| [[EliminateDistinct]] EliminateDistinct
|

.7+^.^| [[Finish_Analysis]] Finish Analysis
.7+^.^| `Once`
| [[EliminateSubqueryAliases]] EliminateSubqueryAliases
|

| [[EliminateView]] EliminateView
|

| [[ReplaceExpressions]] link:spark-sql-Optimizer-ReplaceExpressions.adoc[ReplaceExpressions]
|

| [[ComputeCurrentTime]] link:spark-sql-Optimizer-GetCurrentDatabase.adoc#ComputeCurrentTime[ComputeCurrentTime]
|

| [[GetCurrentDatabase]] link:spark-sql-Optimizer-GetCurrentDatabase.adoc#GetCurrentDatabase[GetCurrentDatabase]
|

| [[RewriteDistinctAggregates]] RewriteDistinctAggregates
|

| [[ReplaceDeduplicateWithAggregate]] ReplaceDeduplicateWithAggregate
|

^.^| [[Union]] Union
^.^| `Once`
| [[CombineUnions]] CombineUnions
|

^.^| [[Pullup-Correlated-Expressions]] Pullup Correlated Expressions
^.^| `Once`
| [[PullupCorrelatedPredicates]] link:spark-sql-Optimizer-PullupCorrelatedPredicates.adoc[PullupCorrelatedPredicates]
|

^.^| [[Subquery]] Subquery
^.^| `Once`
| [[OptimizeSubqueries]] link:spark-sql-Optimizer-OptimizeSubqueries.adoc[OptimizeSubqueries]
|

.4+^.^| [[Replace-Operators]] Replace Operators
.4+^.^| <<fixedPoint, FixedPoint>>
| ReplaceIntersectWithSemiJoin
|
| ReplaceExceptWithFilter
|

| ReplaceExceptWithAntiJoin
|

| ReplaceDistinctWithAggregate
|

.2+^.^| [[Aggregate]] Aggregate
.2+^.^| <<fixedPoint, FixedPoint>>
| RemoveLiteralFromGroupExpressions
|

| RemoveRepetitionFromGroupExpressions
|

.38+^.^| [[Operator_Optimization_before_Inferring_Filters]] Operator Optimization before Inferring Filters
.38+^.^| <<fixedPoint, FixedPoint>>

| PushProjectionThroughUnion
|

| [[ReorderJoin]] link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin]
|

| EliminateOuterJoin
|

| [[PushPredicateThroughJoin]] link:spark-sql-Optimizer-PushPredicateThroughJoin.adoc[PushPredicateThroughJoin]
|

| [[PushDownPredicate]] link:spark-sql-Optimizer-PushDownPredicate.adoc[PushDownPredicate]
|

| [[LimitPushDown]] link:spark-sql-Optimizer-LimitPushDown.adoc[LimitPushDown]
|

| link:spark-sql-Optimizer-ColumnPruning.adoc[ColumnPruning]
|

| InferFiltersFromConstraints
|

| CollapseRepartition
|

| CollapseProject
|

| CollapseWindow
|

| CombineFilters
|

| CombineLimits
|

| CombineUnions
|

| [[NullPropagation]] link:spark-sql-Optimizer-NullPropagation.adoc[NullPropagation]
|

| ConstantPropagation
|

| FoldablePropagation
|

| [[OptimizeIn]] link:spark-sql-Optimizer-OptimizeIn.adoc[OptimizeIn]
|

| [[ConstantFolding]] link:spark-sql-Optimizer-ConstantFolding.adoc[ConstantFolding]
|

| ReorderAssociativeOperator
|

| LikeSimplification
|

| BooleanSimplification
|

| SimplifyConditionals
|

| RemoveDispensableExpressions
|

| SimplifyBinaryComparison
|

| PruneFilters
|

| EliminateSorts
|

| [[SimplifyCasts]] link:spark-sql-Optimizer-SimplifyCasts.adoc[SimplifyCasts]
|

| SimplifyCaseConversionExpressions
|

| [[RewriteCorrelatedScalarSubquery]] link:spark-sql-Optimizer-RewriteCorrelatedScalarSubquery.adoc[RewriteCorrelatedScalarSubquery]
|

| [[EliminateSerialization]] link:spark-sql-Optimizer-EliminateSerialization.adoc[EliminateSerialization]
|

| RemoveRedundantAliases
|

| RemoveRedundantProject
|

| SimplifyCreateStructOps
|

| SimplifyCreateArrayOps
|

| SimplifyCreateMapOps
|

| CombineConcats
|

| <<extendedOperatorOptimizationRules, extendedOperatorOptimizationRules>>
|

.1+^.^| [[Infer_Filters]] Infer Filters
.1+^.^| `Once`
| [[InferFiltersFromConstraints]] InferFiltersFromConstraints
|

.1+^.^| [[Operator_Optimization_after_Inferring_Filters]] Operator Optimization after Inferring Filters
.1+^.^| <<fixedPoint, FixedPoint>>
| The same as <<Operator_Optimization_before_Inferring_Filters, Operator Optimization before Inferring Filters>>
|

^.^| [[Join-Reorder]] Join Reorder
^.^| `Once`
| [[CostBasedJoinReorder]] link:spark-sql-Optimizer-CostBasedJoinReorder.adoc[CostBasedJoinReorder]
|

^.^| [[Decimal-Optimizations]] Decimal Optimizations
^.^| <<fixedPoint, FixedPoint>>
| [[DecimalAggregates]] link:spark-sql-Optimizer-DecimalAggregates.adoc[DecimalAggregates]
|

.2+^.^| [[Object_Expressions_Optimization]] Object Expressions Optimization
.2+^.^| <<fixedPoint, FixedPoint>>
| EliminateMapObjects
|

| [[CombineTypedFilters]] link:spark-sql-Optimizer-CombineTypedFilters.adoc[CombineTypedFilters]
|

.2+^.^| [[LocalRelation]] LocalRelation
.2+^.^| <<fixedPoint, FixedPoint>>
| ConvertToLocalRelation
|

| link:spark-sql-Optimizer-PropagateEmptyRelation.adoc[PropagateEmptyRelation]
|

^.^| [[Check_Cartesian_Products]] Check Cartesian Products
^.^| `Once`
| CheckCartesianProducts
|

.4+^.^| [[RewriteSubquery]] RewriteSubquery
.4+^.^| `Once`
| [[RewritePredicateSubquery]] link:spark-sql-Optimizer-RewritePredicateSubquery.adoc[RewritePredicateSubquery]
|

| [[ColumnPruning]] link:spark-sql-Optimizer-ColumnPruning.adoc[ColumnPruning]
|

| [[CollapseProject]] CollapseProject
|

| [[RemoveRedundantProject]] RemoveRedundantProject
|
|===

TIP: Consult the https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L48-L137[sources of `Optimizer`] for the up-to-date list of the optimization rules.

NOTE: *Catalyst* is a Spark SQL framework for manipulating trees. It can work with trees of relational operators and expressions in link:spark-sql-LogicalPlan.adoc[logical plans] before they end up as link:spark-sql-SparkPlan.adoc[physical execution plans].

[source, scala]
----
scala> sql("select 1 + 1 + 1").explain(true)
== Parsed Logical Plan ==
'Project [unresolvedalias(((1 + 1) + 1), None)]
+- OneRowRelation$

== Analyzed Logical Plan ==
((1 + 1) + 1): int
Project [((1 + 1) + 1) AS ((1 + 1) + 1)#4]
+- OneRowRelation$

== Optimized Logical Plan ==
Project [3 AS ((1 + 1) + 1)#4]
+- OneRowRelation$

== Physical Plan ==
*Project [3 AS ((1 + 1) + 1)#4]
+- Scan OneRowRelation[]
----

[[internal-properties]]
.Optimizer's Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Name
| Initial Value
| Description

| [[fixedPoint]] `fixedPoint`
| `FixedPoint` with the number of iterations as defined by link:spark-sql-CatalystConf.adoc#optimizerMaxIterations[spark.sql.optimizer.maxIterations]
| Used in <<Replace-Operators, Replace Operators>>, <<Aggregate, Aggregate>>, <<Operator-Optimizations, Operator Optimizations>>, <<Decimal-Optimizations, Decimal Optimizations>>, <<Typed-Filter-Optimization, Typed Filter Optimization>> and <<LocalRelation, LocalRelation>> batches (and also indirectly in the User Provided Optimizers rule batch in link:spark-sql-SparkOptimizer.adoc#User-Provided-Optimizers[SparkOptimizer]).
|===

=== [[creating-instance]] Creating Optimizer Instance

`Optimizer` takes the following when created:

* [[sessionCatalog]] link:spark-sql-SessionCatalog.adoc[SessionCatalog]
* [[conf]] link:spark-sql-CatalystConf.adoc[CatalystConf]

`Optimizer` initializes the <<internal-properties, internal properties>>.
