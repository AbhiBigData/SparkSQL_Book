== [[Optimizer]] Catalyst Optimizer -- Generic Logical Query Plan Optimizer

`Optimizer` (aka *Catalyst Optimizer*) is the base of <<extensions, logical query plan optimizers>> that defines the <<batches, base logical optimizations>> and the <<extendedOperatorOptimizationRules, extendedOperatorOptimizationRules>> extension point for additional optimizations in the *Operator Optimization* batch.

[[extensions]]
NOTE: <<spark-sql-SparkOptimizer.adoc#, SparkOptimizer>> is the one and only direct implementation of the `Optimizer` Contract in Spark SQL.

`Optimizer` is a <<spark-sql-catalyst-RuleExecutor.adoc#, RuleExecutor>> of <<spark-sql-LogicalPlan.adoc#, LogicalPlan>> (i.e. `RuleExecutor[LogicalPlan]`).

```
Optimizer: Analyzed Logical Plan ==> Optimized Logical Plan
```

`Optimizer` is available as the <<spark-sql-SessionState.adoc#optimizer, optimizer>> property of a session-specific `SessionState`.

[source, scala]
----
scala> :type spark
org.apache.spark.sql.SparkSession

scala> :type spark.sessionState.optimizer
org.apache.spark.sql.catalyst.optimizer.Optimizer
----

You can access the optimized logical plan of a structured query (as a <<spark-sql-Dataset.adoc#, Dataset>>) using <<spark-sql-dataset-operators.adoc#explain, Dataset.explain>> basic action (with `extended` flag enabled) or SQL's `EXPLAIN EXTENDED` SQL command.

[source, scala]
----
// sample structured query
val inventory = spark
  .range(5)
  .withColumn("new_column", 'id + 5 as "plus5")

// Using explain operator (with extended flag enabled)
scala> inventory.explain(extended = true)
== Parsed Logical Plan ==
'Project [id#0L, ('id + 5) AS plus5#2 AS new_column#3]
+- AnalysisBarrier
      +- Range (0, 5, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint, new_column: bigint
Project [id#0L, (id#0L + cast(5 as bigint)) AS new_column#3L]
+- Range (0, 5, step=1, splits=Some(8))

== Optimized Logical Plan ==
Project [id#0L, (id#0L + 5) AS new_column#3L]
+- Range (0, 5, step=1, splits=Some(8))

== Physical Plan ==
*(1) Project [id#0L, (id#0L + 5) AS new_column#3L]
+- *(1) Range (0, 5, step=1, splits=8)
----

Alternatively, you can access the analyzed logical plan using `QueryExecution` and its <<spark-sql-QueryExecution.adoc#optimizedPlan, optimizedPlan>> property  (that together with `numberedTreeString` method is a very good "debugging" tool).

[source, scala]
----
val optimizedPlan = inventory.queryExecution.optimizedPlan
scala> println(optimizedPlan.numberedTreeString)
00 Project [id#0L, (id#0L + 5) AS new_column#3L]
01 +- Range (0, 5, step=1, splits=Some(8))
----

[[batches]]
.Optimizer's Batches and Base Logical Optimizations (in the order of execution)
[cols="2,1,3,3",options="header",width="100%"]
|===
^.^| Batch Name
^.^| Strategy
| Rules
| Description

^.^| [[Eliminate_Distinct]] Eliminate Distinct
^.^| `Once`
| [[EliminateDistinct]] EliminateDistinct
|

.7+^.^| [[Finish_Analysis]] Finish Analysis
.7+^.^| `Once`
| [[EliminateSubqueryAliases]] <<spark-sql-Optimizer-EliminateSubqueryAliases.adoc#, EliminateSubqueryAliases>>
| Removes (eliminates) <<spark-sql-LogicalPlan-SubqueryAlias.adoc#, SubqueryAlias>> unary logical operators from a <<spark-sql-LogicalPlan.adoc#, logical plan>>

| [[EliminateView]] <<spark-sql-Optimizer-EliminateView.adoc#, EliminateView>>
| Removes (eliminates) <<spark-sql-LogicalPlan-View.adoc#, View>> unary logical operators from a <<spark-sql-LogicalPlan.adoc#, logical plan>> and replaces them with their <<spark-sql-LogicalPlan-View.adoc#child, child>> logical operator

| [[ReplaceExpressions]] <<spark-sql-Optimizer-ReplaceExpressions.adoc#, ReplaceExpressions>>
| Replaces <<spark-sql-Expression-RuntimeReplaceable.adoc#, RuntimeReplaceable>> expressions with their single child expression

| [[ComputeCurrentTime]] <<spark-sql-Optimizer-ComputeCurrentTime.adoc#, ComputeCurrentTime>>
|

| [[GetCurrentDatabase]] <<spark-sql-Optimizer-GetCurrentDatabase.adoc#, GetCurrentDatabase>>
|

| [[RewriteDistinctAggregates]] RewriteDistinctAggregates
|

| [[ReplaceDeduplicateWithAggregate]] ReplaceDeduplicateWithAggregate
|

^.^| [[Union]] Union
^.^| `Once`
| [[CombineUnions]] <<spark-sql-Optimizer-CombineUnions.adoc#, CombineUnions>>
|

^.^| [[Pullup-Correlated-Expressions]] Pullup Correlated Expressions
^.^| `Once`
| [[PullupCorrelatedPredicates]] link:spark-sql-Optimizer-PullupCorrelatedPredicates.adoc[PullupCorrelatedPredicates]
|

^.^| [[Subquery]] Subquery
^.^| `Once`
| [[OptimizeSubqueries]] link:spark-sql-Optimizer-OptimizeSubqueries.adoc[OptimizeSubqueries]
|

.4+^.^| [[Replace-Operators]] Replace Operators
.4+^.^| <<fixedPoint, FixedPoint>>
| ReplaceIntersectWithSemiJoin
|
| ReplaceExceptWithFilter
|

| ReplaceExceptWithAntiJoin
|

| ReplaceDistinctWithAggregate
|

.2+^.^| [[Aggregate]] Aggregate
.2+^.^| <<fixedPoint, FixedPoint>>
| RemoveLiteralFromGroupExpressions
|

| RemoveRepetitionFromGroupExpressions
|

.38+^.^| [[Operator_Optimization]][[Operator_Optimization_before_Inferring_Filters]] Operator Optimization before Inferring Filters
.38+^.^| <<fixedPoint, FixedPoint>>

| PushProjectionThroughUnion
|

| [[ReorderJoin]] link:spark-sql-Optimizer-ReorderJoin.adoc[ReorderJoin]
|

| EliminateOuterJoin
|

| <<spark-sql-Optimizer-PushPredicateThroughJoin.adoc#, PushPredicateThroughJoin>>
| [[PushPredicateThroughJoin]]

| <<spark-sql-Optimizer-PushDownPredicate.adoc#, PushDownPredicate>>
| [[PushDownPredicate]]

| <<spark-sql-Optimizer-LimitPushDown.adoc#, LimitPushDown>>
| [[LimitPushDown]]

| <<spark-sql-Optimizer-ColumnPruning.adoc#, ColumnPruning>>
|

| InferFiltersFromConstraints
|

| CollapseRepartition
|

| CollapseProject
|

| <<spark-sql-Optimizer-CollapseWindow.adoc#, CollapseWindow>>
| [[CollapseWindow]] Collapses two adjacent Window logical operators

| CombineFilters
|

| CombineLimits
|

| CombineUnions
|

| [[NullPropagation]] link:spark-sql-Optimizer-NullPropagation.adoc[NullPropagation]
|

| ConstantPropagation
|

| FoldablePropagation
|

| [[OptimizeIn]] link:spark-sql-Optimizer-OptimizeIn.adoc[OptimizeIn]
|

| [[ConstantFolding]] link:spark-sql-Optimizer-ConstantFolding.adoc[ConstantFolding]
|

| ReorderAssociativeOperator
|

| LikeSimplification
|

| BooleanSimplification
|

| SimplifyConditionals
|

| RemoveDispensableExpressions
|

| SimplifyBinaryComparison
|

| PruneFilters
|

| EliminateSorts
|

| [[SimplifyCasts]] link:spark-sql-Optimizer-SimplifyCasts.adoc[SimplifyCasts]
|

| SimplifyCaseConversionExpressions
|

| [[RewriteCorrelatedScalarSubquery]] link:spark-sql-Optimizer-RewriteCorrelatedScalarSubquery.adoc[RewriteCorrelatedScalarSubquery]
|

| [[EliminateSerialization]] link:spark-sql-Optimizer-EliminateSerialization.adoc[EliminateSerialization]
|

| RemoveRedundantAliases
|

| RemoveRedundantProject
|

| SimplifyCreateStructOps
|

| SimplifyCreateArrayOps
|

| SimplifyCreateMapOps
|

| CombineConcats
|

| <<extendedOperatorOptimizationRules, extendedOperatorOptimizationRules>>
|

.1+^.^| [[Infer_Filters]] Infer Filters
.1+^.^| `Once`
| [[InferFiltersFromConstraints]] InferFiltersFromConstraints
|

.1+^.^| [[Operator_Optimization_after_Inferring_Filters]] Operator Optimization after Inferring Filters
.1+^.^| <<fixedPoint, FixedPoint>>
| The same as <<Operator_Optimization_before_Inferring_Filters, Operator Optimization before Inferring Filters>>
|

^.^| [[Join-Reorder]][[Join_Reorder]] Join Reorder
^.^| `Once`
| [[CostBasedJoinReorder]] link:spark-sql-Optimizer-CostBasedJoinReorder.adoc[CostBasedJoinReorder]
| Reorders <<spark-sql-LogicalPlan-Join.adoc#, Join>> logical operators

^.^| [[Decimal-Optimizations]][[Decimal_Optimizations]] Decimal Optimizations
^.^| <<fixedPoint, FixedPoint>>
| [[DecimalAggregates]] link:spark-sql-Optimizer-DecimalAggregates.adoc[DecimalAggregates]
|

.2+^.^| [[Object_Expressions_Optimization]] Object Expressions Optimization
.2+^.^| <<fixedPoint, FixedPoint>>
| EliminateMapObjects
|

| [[CombineTypedFilters]] link:spark-sql-Optimizer-CombineTypedFilters.adoc[CombineTypedFilters]
|

.2+^.^| [[LocalRelation]] LocalRelation
.2+^.^| <<fixedPoint, FixedPoint>>
| ConvertToLocalRelation
|

| link:spark-sql-Optimizer-PropagateEmptyRelation.adoc[PropagateEmptyRelation]
|

^.^| [[Check_Cartesian_Products]] Check Cartesian Products
^.^| `Once`
| CheckCartesianProducts
|

.4+^.^| [[RewriteSubquery]] RewriteSubquery
.4+^.^| `Once`
| [[RewritePredicateSubquery]] link:spark-sql-Optimizer-RewritePredicateSubquery.adoc[RewritePredicateSubquery]
|

| [[ColumnPruning]] link:spark-sql-Optimizer-ColumnPruning.adoc[ColumnPruning]
|

| [[CollapseProject]] CollapseProject
|

| [[RemoveRedundantProject]] RemoveRedundantProject
|
|===

TIP: Consult the https://github.com/apache/spark/blob/v2.3.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L49-L92[sources] of the `Optimizer` class for the up-to-date list of the optimization rules.

[[internal-properties]]
.Optimizer's Internal Registries and Counters
[cols="1,1,2",options="header",width="100%"]
|===
| Name
| Initial Value
| Description

| [[fixedPoint]] `fixedPoint`
| `FixedPoint` with the number of iterations as defined by link:spark-sql-CatalystConf.adoc#optimizerMaxIterations[spark.sql.optimizer.maxIterations]
| Used in <<Replace-Operators, Replace Operators>>, <<Aggregate, Aggregate>>, <<Operator-Optimizations, Operator Optimizations>>, <<Decimal-Optimizations, Decimal Optimizations>>, <<Typed-Filter-Optimization, Typed Filter Optimization>> and <<LocalRelation, LocalRelation>> batches (and also indirectly in the User Provided Optimizers rule batch in link:spark-sql-SparkOptimizer.adoc#User-Provided-Optimizers[SparkOptimizer]).
|===

=== [[creating-instance]] Creating Optimizer Instance

`Optimizer` takes the following when created:

* [[sessionCatalog]] <<spark-sql-SessionCatalog.adoc#, SessionCatalog>>

`Optimizer` initializes the <<internal-properties, internal properties>>.

NOTE: `Optimizer` is a Scala abstract class and cannot be <<creating-instance, created>> directly. It is created indirectly when the <<extensions, concrete Optimizers>> are.

=== [[extendedOperatorOptimizationRules]] Additional Operator Optimization Rules -- `extendedOperatorOptimizationRules` Extension Point

[source, scala]
----
extendedOperatorOptimizationRules: Seq[Rule[LogicalPlan]]
----

`extendedOperatorOptimizationRules` extension point defines additional rules for the Operator Optimization batch.

NOTE: `extendedOperatorOptimizationRules` rules are executed right after <<Operator_Optimization_before_Inferring_Filters, Operator Optimization before Inferring Filters>> and <<Operator_Optimization_after_Inferring_Filters, Operator Optimization after Inferring Filters>>.
