== [[ParquetFileFormat]] ParquetFileFormat

[[shortName]]
`ParquetFileFormat` is a link:spark-sql-FileFormat.adoc[FileFormat] for *parquet* data source format (i.e. link:spark-sql-DataSourceRegister.adoc#shortName[registers itself to handle files in parquet format] and convert them to Spark SQL rows).

NOTE: `parquet` is the link:spark-sql-DataFrameReader.adoc#source[default data source format] in Spark SQL.

[source, scala]
----
// All the following queries are equivalent
// schema has to be specified manually
import org.apache.spark.sql.types.StructType
val schema = StructType($"id".int :: Nil)

spark.read.schema(schema).format("parquet").load("parquet-datasets")
spark.read.schema(schema).parquet("parquet-datasets")
spark.read.schema(schema).load("parquet-datasets")
----

[[isSplitable]]
`ParquetFileFormat` is splitable, i.e. FIXME

[[supportBatch]]
`ParquetFileFormat` uses link:spark-sql-FileFormat.adoc#supportBatch[supportBatch] flag to...FIXME

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[prepareWrite]] `prepareWrite` Method

[source, scala]
----
prepareWrite(
  sparkSession: SparkSession,
  job: Job,
  options: Map[String, String],
  dataSchema: StructType): OutputWriterFactory
----

NOTE: `prepareWrite` is part of link:spark-sql-FileFormat.adoc#prepareWrite[FileFormat Contract] to...FIXME.

`prepareWrite`...FIXME

=== [[inferSchema]] `inferSchema` Method

[source, scala]
----
inferSchema(
  sparkSession: SparkSession,
  parameters: Map[String, String],
  files: Seq[FileStatus]): Option[StructType]
----

NOTE: `inferSchema` is part of link:spark-sql-FileFormat.adoc#inferSchema[FileFormat Contract] to...FIXME.

`inferSchema`...FIXME

=== [[vectorTypes]] `vectorTypes` Method

[source, scala]
----
vectorTypes: Option[Seq[String]]
----

NOTE: `vectorTypes` is part of link:spark-sql-ColumnarBatchScan.adoc#vectorTypes[ColumnarBatchScan Contract] to...FIXME.

`vectorTypes`...FIXME

=== [[buildReaderWithPartitionValues]] Building Data Reader With Partition Column Values Appended -- `buildReaderWithPartitionValues` Method

[source, scala]
----
buildReaderWithPartitionValues(
  sparkSession: SparkSession,
  dataSchema: StructType,
  partitionSchema: StructType,
  requiredSchema: StructType,
  filters: Seq[Filter],
  options: Map[String, String],
  hadoopConf: Configuration): (PartitionedFile) => Iterator[InternalRow]
----

NOTE: `buildReaderWithPartitionValues` is part of link:spark-sql-FileFormat.adoc#buildReaderWithPartitionValues[FileFormat Contract] to build a data reader with the partition column values appended.

`buildReaderWithPartitionValues` sets the <<options, configuration options>> in the input `hadoopConf`.

[[options]]
.Configuration Options
[cols="1,2",options="header",width="100%"]
|===
| Name
| Value

| `parquet.read.support.class`
| `org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport`

| `org.apache.spark.sql.parquet.row.requested_schema`
| link:spark-sql-DataType.adoc#json[JSON] representation of `requiredSchema`

| `org.apache.spark.sql.parquet.row.attributes`
| link:spark-sql-DataType.adoc#json[JSON] representation of `requiredSchema`

| `spark.sql.session.timeZone`
| link:spark-sql-properties.adoc#spark.sql.session.timeZone[spark.sql.session.timeZone]

| `spark.sql.parquet.binaryAsString`
| link:spark-sql-properties.adoc#spark.sql.parquet.binaryAsString[spark.sql.parquet.binaryAsString]

| `spark.sql.parquet.int96AsTimestamp`
| link:spark-sql-properties.adoc#spark.sql.parquet.int96AsTimestamp[spark.sql.parquet.int96AsTimestamp]
|===

`buildReaderWithPartitionValues` requests `ParquetWriteSupport` to `setSchema`.

CAUTION: FIXME Finish me
