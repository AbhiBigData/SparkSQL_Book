== [[FileSourceStrategy]] FileSourceStrategy Execution Planning Strategy for LogicalRelations with HadoopFsRelation

`FileSourceStrategy` is an link:spark-sql-SparkStrategy.adoc[execution planning strategy] that <<apply, plans scans over collections of files>> (possibly partitioned or bucketed).

`FileSourceStrategy` is part of link:spark-sql-SparkPlanner.adoc#strategies[predefined strategies] of the link:spark-sql-SparkPlanner.adoc[Spark Planner].

[source, scala]
----
import org.apache.spark.sql.execution.datasources.FileSourceStrategy

// Enable INFO logging level to see the details of the strategy
val logger = FileSourceStrategy.getClass.getName.replace("$", "")
import org.apache.log4j.{Level, Logger}
Logger.getLogger(logger).setLevel(Level.INFO)

// Create a bucketed data source table
val tableName = "bucketed_4_id"
spark
  .range(100)
  .write
  .bucketBy(4, "id")
  .sortBy("id")
  .mode("overwrite")
  .saveAsTable(tableName)
val q = spark.table(tableName)
val plan = q.queryExecution.optimizedPlan

val executionPlan = FileSourceStrategy(plan).head

scala> println(executionPlan.numberedTreeString)
00 FileScan parquet default.bucketed_4_id[id#140L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/jacek/dev/apps/spark-2.3.0-bin-hadoop2.7/spark-warehouse/bucketed_4..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>

import org.apache.spark.sql.execution.FileSourceScanExec
val scan = executionPlan.collectFirst { case fsse: FileSourceScanExec => fsse }.get

scala> :type scan
org.apache.spark.sql.execution.FileSourceScanExec
----

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.sql.execution.datasources.FileSourceStrategy` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.datasources.FileSourceStrategy=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[collectProjectsAndFilters]] `collectProjectsAndFilters` Method

[source, scala]
----
collectProjectsAndFilters(plan: LogicalPlan):
  (Option[Seq[NamedExpression]], Seq[Expression], LogicalPlan, Map[Attribute, Expression])
----

`collectProjectsAndFilters` is a pattern used to destructure a link:spark-sql-LogicalPlan.adoc[LogicalPlan] that can be `Project` or `Filter`. Any other `LogicalPlan` give an _all-empty_ response.

=== [[apply]] Applying FileSourceStrategy Strategy to Logical Plan (Executing FileSourceStrategy) -- `apply` Method

[source, scala]
----
apply(plan: LogicalPlan): Seq[SparkPlan]
----

NOTE: `apply` is part of link:spark-sql-catalyst-GenericStrategy.adoc#apply[GenericStrategy Contract] to generate a collection of link:spark-sql-SparkPlan.adoc[SparkPlans] for a given link:spark-sql-LogicalPlan.adoc[logical plan].

`apply` uses link:spark-sql-PhysicalOperation.adoc[PhysicalOperation] Scala extractor object to destructure a logical query plan into a tuple of projection and filter expressions together with a leaf logical operator.

`apply` only works with link:spark-sql-LogicalPlan.adoc[logical plans] that are actually a link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation] with a link:spark-sql-BaseRelation-HadoopFsRelation.adoc[HadoopFsRelation] (possibly as a child of link:spark-sql-LogicalPlan-Project.adoc[Project] and link:spark-sql-LogicalPlan-Filter.adoc[Filter] logical operators).

`apply` computes `partitionKeyFilters` expression set with the filter expressions that are a subset of the link:spark-sql-BaseRelation-HadoopFsRelation.adoc#partitionSchema[partitionSchema] of the `HadoopFsRelation`.

`apply` prints out the following INFO message to the logs:

```
Pruning directories with: [partitionKeyFilters]
```

`apply` computes `afterScanFilters` predicate link:spark-sql-Expression.adoc[expressions] that should be evaluated after the scan.

`apply` prints out the following INFO message to the logs:

```
Post-Scan Filters: [afterScanFilters]
```

`apply` computes `readDataColumns` link:spark-sql-Expression-Attribute.adoc[attributes] that are the required attributes except the partition columns.

`apply` prints out the following INFO message to the logs:

```
Output Data Schema: [outputSchema]
```

`apply` creates a link:spark-sql-SparkPlan-FileSourceScanExec.adoc#creating-instance[FileSourceScanExec] physical operator.

If there are any `afterScanFilter` predicate expressions, `apply` creates a `FilterExec` physical operator with them and the `FileSourceScanExec` operator.

If the output of the FilterExec physical operator is not the `projects` expressions, `apply` creates a link:spark-sql-SparkPlan-ProjectExec.adoc#creating-instance[ProjectExec] physical operator with them and the `FilterExec` or the `FileSourceScanExec` operators.
