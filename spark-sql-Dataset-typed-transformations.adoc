== Dataset API -- Typed Transformations

*Typed transformations* are part of the Dataset API for transforming a `Dataset` with an <<spark-sql-Encoder.adoc#, Encoder>> (except the <<spark-sql-RowEncoder.adoc#, RowEncoder>>).

[[methods]]
.Dataset API's Typed Transformations
[cols="1,2",options="header",width="100%"]
|===
| Transformation
| Description

| <<as, as>>
a|

[source, scala]
----
as(alias: String): Dataset[T]
as(alias: Symbol): Dataset[T]
----

| <<coalesce, coalesce>>
a|

[source, scala]
----
coalesce(numPartitions: Int): Dataset[T]
----

| <<dropDuplicates, dropDuplicates>>
a|

[source, scala]
----
dropDuplicates(): Dataset[T]
dropDuplicates(colNames: Array[String]): Dataset[T]
dropDuplicates(colNames: Seq[String]): Dataset[T]
dropDuplicates(col1: String, cols: String*): Dataset[T]
----

| <<except, except>>
a|

[source, scala]
----
except(other: Dataset[T]): Dataset[T]
----

| <<filter, filter>>
a|

[source, scala]
----
filter(condition: Column): Dataset[T]
filter(conditionExpr: String): Dataset[T]
filter(func: T => Boolean): Dataset[T]
----

| <<hint, hint>>
a|

[source, scala]
----
hint(name: String, parameters: Any*): Dataset[T]
----

| <<intersect, intersect>>
a|

[source, scala]
----
intersect(other: Dataset[T]): Dataset[T]
----

| <<joinWith, joinWith>>
a|

[source, scala]
----
joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)]
joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)]
----

| <<limit, limit>>
a|

[source, scala]
----
limit(n: Int): Dataset[T]
----

| <<map, map>>
a|

[source, scala]
----
map[U: Encoder](func: T => U): Dataset[U]
----

| <<mapPartitions, mapPartitions>>
a|

[source, scala]
----
mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U]
----

| <<randomSplit, randomSplit>>
a|

[source, scala]
----
randomSplit(weights: Array[Double]): Array[Dataset[T]]
randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]]
----

| <<repartition, repartition>>
a|

[source, scala]
----
repartition(partitionExprs: Column*): Dataset[T]
repartition(numPartitions: Int): Dataset[T]
repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T]
----

| <<repartitionByRange, repartitionByRange>>
a|

(*New in 2.3.0*)

[source, scala]
----
repartitionByRange(partitionExprs: Column*): Dataset[T]
repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T]
----

| <<sample, sample>>
a|

[source, scala]
----
sample(withReplacement: Boolean, fraction: Double): Dataset[T]
sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T]
sample(fraction: Double): Dataset[T]  // <1>
sample(fraction: Double, seed: Long): Dataset[T]  // <2>
----
<1> *New in 2.3.0*
<2> *New in 2.3.0*

| <<select, select>>
a|

[source, scala]
----
select(cols: Column*): DataFrame
select(col: String, cols: String*): DataFrame
select[U1](c1: TypedColumn[T, U1]): Dataset[U1]
select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)]
select[U1, U2, U3](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)]
select[U1, U2, U3, U4](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3],
  c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)]
select[U1, U2, U3, U4, U5](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3],
  c4: TypedColumn[T, U4],
  c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)]
----

| <<sort, sort>>
a|

[source, scala]
----
sort(sortExprs: Column*): Dataset[T]
sort(sortCol: String, sortCols: String*): Dataset[T]
----

| <<sortWithinPartitions, sortWithinPartitions>>
a|

[source, scala]
----
sortWithinPartitions(sortExprs: Column*): Dataset[T]
sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T]
----

| <<union, union>>
a|

[source, scala]
----
union(other: Dataset[T]): Dataset[T]
----

| <<unionByName, unionByName>>
a| (*New in 2.3.0*)

[source, scala]
----
unionByName(other: Dataset[T]): Dataset[T]
----

| <<withWatermark, withWatermark>>
a|

[source, scala]
----
withWatermark(eventTime: String, delayThreshold: String): Dataset[T]
----
|===

=== [[as]] `as` Typed Transformation

[source, scala]
----
as(alias: String): Dataset[T]
as(alias: Symbol): Dataset[T]
----

`as`...FIXME

=== [[coalesce]] `coalesce` Typed Transformation

[source, scala]
----
coalesce(numPartitions: Int): Dataset[T]
----

`coalesce`...FIXME

=== [[dropDuplicates]] `dropDuplicates` Typed Transformation

[source, scala]
----
dropDuplicates(): Dataset[T]
dropDuplicates(colNames: Array[String]): Dataset[T]
dropDuplicates(colNames: Seq[String]): Dataset[T]
dropDuplicates(col1: String, cols: String*): Dataset[T]
----

`dropDuplicates`...FIXME

=== [[except]] `except` Typed Transformation

[source, scala]
----
except(other: Dataset[T]): Dataset[T]
----

`except`...FIXME

=== [[filter]] `filter` Typed Transformation

[source, scala]
----
filter(condition: Column): Dataset[T]
filter(conditionExpr: String): Dataset[T]
filter(func: T => Boolean): Dataset[T]
----

`filter`...FIXME

=== [[hint]] `hint` Typed Transformation

[source, scala]
----
hint(name: String, parameters: Any*): Dataset[T]
----

`hint`...FIXME

=== [[intersect]] `intersect` Typed Transformation

[source, scala]
----
intersect(other: Dataset[T]): Dataset[T]
----

`intersect`...FIXME

=== [[joinWith]] `joinWith` Typed Transformation

[source, scala]
----
joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)]
joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)]
----

`joinWith`...FIXME

=== [[limit]] `limit` Typed Transformation

[source, scala]
----
limit(n: Int): Dataset[T]
----

`limit`...FIXME

=== [[map]] `map` Typed Transformation

[source, scala]
----
map[U : Encoder](func: T => U): Dataset[U]
----

`map`...FIXME

=== [[mapPartitions]] `mapPartitions` Typed Transformation

[source, scala]
----
mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U]
----

`mapPartitions`...FIXME

=== [[randomSplit]] `randomSplit` Typed Transformation

[source, scala]
----
randomSplit(weights: Array[Double]): Array[Dataset[T]]
randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]]
----

`randomSplit`...FIXME

=== [[repartition]] `repartition` Typed Transformation

[source, scala]
----
repartition(partitionExprs: Column*): Dataset[T]
repartition(numPartitions: Int): Dataset[T]
repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T]
----

`repartition`...FIXME

=== [[repartitionByRange]] `repartitionByRange` Typed Transformation

[source, scala]
----
repartitionByRange(partitionExprs: Column*): Dataset[T] // <1>
repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T]
----
<1> Uses <<spark-sql-properties.adoc#spark.sql.shuffle.partitions, spark.sql.shuffle.partitions>> configuration property for the number of partitions to use

(*New in 2.3.0*) `repartitionByRange` simply <<spark-sql-Dataset.adoc#withTypedPlan, creates a Dataset>> with a <<spark-sql-LogicalPlan-Repartition-RepartitionByExpression.adoc#RepartitionByExpression, RepartitionByExpression>> logical operator.

[source, scala]
----
scala> spark.version
res1: String = 2.3.1

val q = spark.range(10).repartitionByRange(numPartitions = 5, $"id")
scala> println(q.queryExecution.logical.numberedTreeString)
00 'RepartitionByExpression ['id ASC NULLS FIRST], 5
01 +- AnalysisBarrier
02       +- Range (0, 10, step=1, splits=Some(8))

scala> println(q.queryExecution.toRdd.getNumPartitions)
5

scala> println(q.queryExecution.toRdd.toDebugString)
(5) ShuffledRowRDD[18] at toRdd at <console>:26 []
 +-(8) MapPartitionsRDD[17] at toRdd at <console>:26 []
    |  MapPartitionsRDD[13] at toRdd at <console>:26 []
    |  MapPartitionsRDD[12] at toRdd at <console>:26 []
    |  ParallelCollectionRDD[11] at toRdd at <console>:26 []
----

`repartitionByRange` uses a `SortOrder` with the `Ascending` sort order, i.e. _ascending nulls first_, when no explicit sort order is specified.

`repartitionByRange` throws a `IllegalArgumentException` when no `partitionExprs` partition-by expression is specified.

```
requirement failed: At least one partition-by expression must be specified.
```

=== [[sample]] `sample` Typed Transformation

[source, scala]
----
sample(withReplacement: Boolean, fraction: Double): Dataset[T]
sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T]
sample(fraction: Double): Dataset[T]  // <1>
sample(fraction: Double, seed: Long): Dataset[T]  // <2>
----
<1> *New in 2.3.0*
<2> *New in 2.3.0*

`sample`...FIXME

=== [[select]] `select` Typed Transformation

[source, scala]
----
select(cols: Column*): DataFrame
select(col: String, cols: String*): DataFrame
select[U1](c1: TypedColumn[T, U1]): Dataset[U1]
select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)]
select[U1, U2, U3](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)]
select[U1, U2, U3, U4](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3],
  c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)]
select[U1, U2, U3, U4, U5](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3],
  c4: TypedColumn[T, U4],
  c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)]
----

`select`...FIXME

=== [[sort]] `sort` Typed Transformation

[source, scala]
----
sort(sortExprs: Column*): Dataset[T]
sort(sortCol: String, sortCols: String*): Dataset[T]
----

`sort`...FIXME

=== [[sortWithinPartitions]] `sortWithinPartitions` Typed Transformation

[source, scala]
----
sortWithinPartitions(sortExprs: Column*): Dataset[T]
sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T]
----

`sortWithinPartitions`...FIXME

=== [[union]] `union` Typed Transformation

[source, scala]
----
union(other: Dataset[T]): Dataset[T]
----

`union`...FIXME

=== [[unionByName]] `unionByName` Typed Transformation

[source, scala]
----
unionByName(other: Dataset[T]): Dataset[T]
----

(*New in 2.3.0*) `unionByName`...FIXME

=== [[withWatermark]] `withWatermark` Typed Transformation

[source, scala]
----
withWatermark(eventTime: String, delayThreshold: String): Dataset[T]
----

`withWatermark`...FIXME
