== [[VectorizedParquetRecordReader]] VectorizedParquetRecordReader

`VectorizedParquetRecordReader` is a `SpecificParquetRecordReaderBase` for parquet file format that directly materialize to Java `Objects`.

`VectorizedParquetRecordReader` is <<creating-instance, created>> exclusively when `ParquetFileFormat` is requested to link:spark-sql-ParquetFileFormat.adoc#buildReaderWithPartitionValues[build a data reader with partition column values appended] (when link:spark-sql-properties.adoc#spark.sql.parquet.enableVectorizedReader[spark.sql.parquet.enableVectorizedReader] configuration property is enabled and the result schema uses link:spark-sql-DataType.adoc#AtomicType[AtomicType] data types only).

[NOTE]
====
link:spark-sql-properties.adoc#spark.sql.parquet.enableVectorizedReader[spark.sql.parquet.enableVectorizedReader] configuration property is on by default.

[source, scala]
----
scala> spark.version
res0: String = 2.3.0

val isParquetVectorizedReaderEnabled = spark.conf.get("spark.sql.parquet.enableVectorizedReader").toBoolean
assert(isParquetVectorizedReaderEnabled, "spark.sql.parquet.enableVectorizedReader should be enabled by default")
----
====

`VectorizedParquetRecordReader` uses `OFF_HEAP` <<MEMORY_MODE, memory mode>> when link:spark-sql-properties.adoc#spark.sql.columnVector.offheap.enabled[spark.sql.columnVector.offheap.enabled] internal configuration property is enabled (which is not by default).

[[internal-registries]]
.VectorizedParquetRecordReader's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[columnarBatch]] `columnarBatch`
| link:spark-sql-ColumnarBatch.adoc[ColumnarBatch]

| [[MEMORY_MODE]] `MEMORY_MODE`
a| Memory mode of the <<columnarBatch, ColumnarBatch>>

* `OFF_HEAP` (when <<useOffHeap, useOffHeap>> is on)
* `ON_HEAP`
|===

=== [[nextKeyValue]] `nextKeyValue` Method

[source, java]
----
boolean nextKeyValue() throws IOException
----

NOTE: `nextKeyValue` is part of Hadoop's https://hadoop.apache.org/docs/r2.7.4/api/org/apache/hadoop/mapred/RecordReader.html[RecordReader] to read (key, value) pairs from a Hadoop https://hadoop.apache.org/docs/r2.7.4/api/org/apache/hadoop/mapred/InputSplit.html[InputSplit] to present a record-oriented view.

`nextKeyValue`...FIXME

NOTE: `nextKeyValue` is used when...FIXME

=== [[resultBatch]] `resultBatch` Method

[source, java]
----
ColumnarBatch resultBatch()
----

`resultBatch` gives <<columnarBatch, columnarBatch>> if available or does <<initBatch, initBatch>>.

NOTE: `resultBatch` is used exclusively when `VectorizedParquetRecordReader` is requested to <<nextKeyValue, nextKeyValue>>.

=== [[initBatch]] `initBatch` Method

[source, java]
----
void initBatch(StructType partitionColumns, InternalRow partitionValues)
// private
private void initBatch()
private void initBatch(
  MemoryMode memMode,
  StructType partitionColumns,
  InternalRow partitionValues)
----

`initBatch`...FIXME

[NOTE]
====
`initBatch` is used when:

* `VectorizedParquetRecordReader` is requested for <<resultBatch, resultBatch>>

* `ParquetFileFormat` is requested to link:spark-sql-ParquetFileFormat.adoc#buildReaderWithPartitionValues[build a data reader with partition column values appended]
====

=== [[creating-instance]] Creating VectorizedParquetRecordReader Instance

`VectorizedParquetRecordReader` takes the following when created:

* [[convertTz]] `TimeZone` (`null` when no timezone conversion is expected)
* [[useOffHeap]] `useOffHeap` flag

`VectorizedParquetRecordReader` initializes the <<internal-registries, internal registries and counters>>.

=== [[initialize]] `initialize` Method

[source, java]
----
void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)
----

NOTE: `initialize` is part of `SpecificParquetRecordReaderBase` Contract to...FIXME.

`initialize`...FIXME

=== [[enableReturningBatches]] `enableReturningBatches` Method

[source, java]
----
void enableReturningBatches()
----

`enableReturningBatches`...FIXME

NOTE: `enableReturningBatches` is used when...FIXME
