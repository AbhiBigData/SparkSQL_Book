== [[FileFormat]] FileFormat

`FileFormat` is the <<contract, contract>> for <<implementations, data sources>> that <<buildReader, read>> and <<prepareWrite, write>> data stored in files.

[[contract]]
.FileFormat Contract
[cols="1m,2",options="header",width="100%"]
|===
| Method
| Description

| buildReader
a| [[buildReader]]

[source, scala]
----
buildReader(
  sparkSession: SparkSession,
  dataSchema: StructType,
  partitionSchema: StructType,
  requiredSchema: StructType,
  filters: Seq[Filter],
  options: Map[String, String],
  hadoopConf: Configuration): PartitionedFile => Iterator[InternalRow]
----

Used when...FIXME

| buildReaderWithPartitionValues
a| [[buildReaderWithPartitionValues]]

[source, scala]
----
buildReaderWithPartitionValues(
  sparkSession: SparkSession,
  dataSchema: StructType,
  partitionSchema: StructType,
  requiredSchema: StructType,
  filters: Seq[Filter],
  options: Map[String, String],
  hadoopConf: Configuration): PartitionedFile => Iterator[InternalRow]
----

Used when...FIXME

| inferSchema
a| [[inferSchema]]

[source, scala]
----
inferSchema(
  sparkSession: SparkSession,
  options: Map[String, String],
  files: Seq[FileStatus]): Option[StructType]
----

Returns the <<spark-sql-StructType.adoc#, schema>> of the given files (as Hadoop's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileStatus.html[FileStatuses]) if supported. Otherwise, `None` should be returned.

Used when:

* `HiveMetastoreCatalog` is requested to <<spark-sql-HiveMetastoreCatalog.adoc#inferIfNeeded, inferIfNeeded>> (when `RelationConversions` logical evaluation rule is requested to <<spark-sql-Analyzer-RelationConversions.adoc#convert, convert a HiveTableRelation to a LogicalRelation>> for `parquet`, `native` and `hive` ORC storage formats)

* `DataSource` is requested to <<spark-sql-DataSource.adoc#getOrInferFileFormatSchema, getOrInferFileFormatSchema>> and <<spark-sql-DataSource.adoc#resolveRelation, resolveRelation>>

| isSplitable
a| [[isSplitable]]

[source, scala]
----
isSplitable(
  sparkSession: SparkSession,
  options: Map[String, String],
  path: Path): Boolean
----

Used when...FIXME

| prepareWrite
a| [[prepareWrite]]

[source, scala]
----
prepareWrite(
  sparkSession: SparkSession,
  job: Job,
  options: Map[String, String],
  dataSchema: StructType): OutputWriterFactory
----

Prepares a write job and returns a `OutputWriterFactory`

Used exclusively when `FileFormatWriter` is requested to <<spark-sql-FileFormatWriter.adoc#write, write query result>>

| supportBatch
a| [[supportBatch]]

[source, scala]
----
supportBatch(
  sparkSession: SparkSession,
  dataSchema: StructType): Boolean
----

Used when...FIXME

| vectorTypes
a| [[vectorTypes]]

[source, java]
----
vectorTypes(
  requiredSchema: StructType,
  partitionSchema: StructType,
  sqlConf: SQLConf): Option[Seq[String]]
----

Used when...FIXME
|===

[[implementations]]
.FileFormats
[width="100%",cols="1,2",options="header"]
|===
| FileFormat
| Description

| <<spark-sql-HiveFileFormat.adoc#, HiveFileFormat>>
| [[HiveFileFormat]] Writes hive tables

| <<spark-sql-OrcFileFormat.adoc#, OrcFileFormat>>
| [[OrcFileFormat]]

| <<spark-sql-ParquetFileFormat.adoc#, ParquetFileFormat>>
| [[ParquetFileFormat]]

| <<spark-sql-TextBasedFileFormat.adoc#, TextBasedFileFormat>>
| [[TextBasedFileFormat]]

| <<spark-sql-CSVFileFormat.adoc#, CSVFileFormat>>
| [[CSVFileFormat]]

| <<spark-sql-JsonFileFormat.adoc#, JsonFileFormat>>
| [[JsonFileFormat]]

| `LibSVMFileFormat`
| [[LibSVMFileFormat]] (Spark MLlib)

| <<spark-sql-TextFileFormat.adoc#, TextFileFormat>>
| [[TextFileFormat]]
|===

=== [[buildReaderWithPartitionValues]] Building Data Reader With Partition Column Values Appended -- `buildReaderWithPartitionValues` Method

[source, scala]
----
buildReaderWithPartitionValues(
  sparkSession: SparkSession,
  dataSchema: StructType,
  partitionSchema: StructType,
  requiredSchema: StructType,
  filters: Seq[Filter],
  options: Map[String, String],
  hadoopConf: Configuration): PartitionedFile => Iterator[InternalRow]
----

`buildReaderWithPartitionValues` is simply an enhanced <<buildReader, buildReader>> that appends link:spark-sql-PartitionedFile.adoc#partitionValues[partition column values] to the internal rows produced by the reader function from <<buildReader, buildReader>>.

Internally, `buildReaderWithPartitionValues` <<buildReader, builds a data reader>> with the input parameters and gives a *data reader function* (of a link:spark-sql-PartitionedFile.adoc[PartitionedFile] to an `Iterator[InternalRow]`) that does the following:

. Creates a converter by requesting `GenerateUnsafeProjection` to link:spark-sql-GenerateUnsafeProjection.adoc#generate[generate an UnsafeProjection] for the attributes of the input `requiredSchema` and `partitionSchema`

. Applies the data reader to a `PartitionedFile` and converts the result using the converter on the joined row with the link:spark-sql-PartitionedFile.adoc#partitionValues[partition column values] appended.

NOTE: `buildReaderWithPartitionValues` is used exclusively when `FileSourceScanExec` physical operator is requested for the link:spark-sql-SparkPlan-FileSourceScanExec.adoc#inputRDDs[input RDDs].

=== [[isSplitable]] Is File Splitable Or Not -- `isSplitable` Method

[source, scala]
----
isSplitable(
  sparkSession: SparkSession,
  options: Map[String, String],
  path: Path): Boolean
----

`isSplitable` always gives `false` by default.

NOTE: link:spark-sql-TextBasedFileFormat.adoc[TextBasedFileFormat] comes with `isSplitable` that uses a Hadoop compression codec to know whether a given file is splitable or not.

NOTE: `isSplitable` is used exclusively when `FileSourceScanExec` is requested to link:spark-sql-SparkPlan-FileSourceScanExec.adoc#createNonBucketedReadRDD[createNonBucketedReadRDD].

=== [[supportBatch]] `supportBatch` Method

[source, scala]
----
supportBatch(sparkSession: SparkSession, dataSchema: StructType): Boolean
----

`supportBatch` is always disabled, i.e. `false` (and is expected to be overriden by `FileFormats` that want to support link:spark-sql-vectorized-parquet-reader.adoc[vectorized decoding]).

[NOTE]
====
`supportBatch` is used when:

* `FileSourceScanExec` physical operator is link:spark-sql-SparkPlan-FileSourceScanExec.adoc#creating-instance[created] (and initializes link:spark-sql-SparkPlan-FileSourceScanExec.adoc#supportsBatch[supportsBatch] flag)

* `OrcFileFormat` is requested to link:spark-sql-OrcFileFormat.adoc#buildReaderWithPartitionValues[buildReaderWithPartitionValues]

* `ParquetFileFormat` is requested to link:spark-sql-ParquetFileFormat.adoc#buildReaderWithPartitionValues[buildReaderWithPartitionValues]
====

=== [[vectorTypes]] `vectorTypes` Method

[source, scala]
----
vectorTypes(
  requiredSchema: StructType,
  partitionSchema: StructType,
  sqlConf: SQLConf): Option[Seq[String]]
----

`vectorTypes` returns `None`, i.e. no class names of link:spark-sql-ColumnVector.adoc[ColumnVectors] for every column used in a columnar batch.

NOTE: `vectorTypes` is used exclusively when `FileSourceScanExec` physical operator is requested for link:spark-sql-SparkPlan-FileSourceScanExec.adoc#vectorTypes[concrete ColumnVector class names].

=== [[buildReader]] Building Catalyst Data Reader -- `buildReader` Method

[source, scala]
----
buildReader(
  sparkSession: SparkSession,
  dataSchema: StructType,
  partitionSchema: StructType,
  requiredSchema: StructType,
  filters: Seq[Filter],
  options: Map[String, String],
  hadoopConf: Configuration): PartitionedFile => Iterator[InternalRow]
----

`buildReader` simply reports a `UnsupportedOperationException`:

```
buildReader is not supported for [className]
```

NOTE: `buildReader` is used exclusively when `FileFormat` is requested to <<buildReaderWithPartitionValues, build a data reader with partition column values appended>>.
