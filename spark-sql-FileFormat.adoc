== [[FileFormat]] FileFormat

`FileFormat` is the <<contract, contract>> in Spark SQL to <<buildReader, read>> and <<prepareWrite, write>> data stored in files (to/from the link:spark-sql-InternalRow.adoc[InternalRow] format).

[[contract]]
[source, scala]
----
package org.apache.spark.sql.execution.datasources

trait FileFormat {
  // only required methods that have no implementation
  // the others follow
  def inferSchema(
    sparkSession: SparkSession,
    options: Map[String, String],
    files: Seq[FileStatus]): Option[StructType]
  def prepareWrite(
    sparkSession: SparkSession,
    job: Job,
    options: Map[String, String],
    dataSchema: StructType): OutputWriterFactory
}
----

.(Subset of) FileFormat Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[inferSchema]] `inferSchema`
| Used when...

| [[prepareWrite]] `prepareWrite`
| Used exclusively when `FileFormatWriter` is requested to link:spark-sql-FileFormatWriter.adoc#write[write the result of a structured query].
|===

[[vectorTypes]]
`vectorTypes`...FIXME

=== [[buildReaderWithPartitionValues]] Building Data Reader With Partition Column Values Appended -- `buildReaderWithPartitionValues` Method

[source, scala]
----
buildReaderWithPartitionValues(
  sparkSession: SparkSession,
  dataSchema: StructType,
  partitionSchema: StructType,
  requiredSchema: StructType,
  filters: Seq[Filter],
  options: Map[String, String],
  hadoopConf: Configuration): PartitionedFile => Iterator[InternalRow]
----

`buildReaderWithPartitionValues` is simply an enhanced <<buildReader, buildReader>> that appends link:spark-sql-PartitionedFile.adoc#partitionValues[partition column values] to the internal rows produced by the reader function from <<buildReader, buildReader>>.

Internally, `buildReaderWithPartitionValues` <<buildReader, builds a data reader>> with the input parameters and gives a *data reader function* (of a link:spark-sql-PartitionedFile.adoc[PartitionedFile] to an `Iterator[InternalRow]`) that does the following:

. Creates a converter by requesting `GenerateUnsafeProjection` to link:spark-sql-GenerateUnsafeProjection.adoc#generate[generate an UnsafeProjection] for the attributes of the input `requiredSchema` and `partitionSchema`

. Applies the data reader to a `PartitionedFile` and converts the result using the converter on the joined row with the link:spark-sql-PartitionedFile.adoc#partitionValues[partition column values] appended.

NOTE: `buildReaderWithPartitionValues` is used exclusively when `FileSourceScanExec` is requested for the link:spark-sql-SparkPlan-FileSourceScanExec.adoc#inputRDDs[input RDDs].

=== [[buildReader]] Building Data Reader -- `buildReader` Method

[source, scala]
----
buildReader(
  sparkSession: SparkSession,
  dataSchema: StructType,
  partitionSchema: StructType,
  requiredSchema: StructType,
  filters: Seq[Filter],
  options: Map[String, String],
  hadoopConf: Configuration): PartitionedFile => Iterator[InternalRow]
----

`buildReader`...FIXME

NOTE: `buildReader` is used exclusively when `FileFormat` is requested to <<buildReaderWithPartitionValues, build a data reader with partition column values appended>>.

=== [[isSplitable]] Is File Splitable Or Not -- `isSplitable` Method

[source, scala]
----
isSplitable(
  sparkSession: SparkSession,
  options: Map[String, String],
  path: Path): Boolean
----

`isSplitable` always gives `false` by default.

NOTE: link:spark-sql-TextBasedFileFormat.adoc[TextBasedFileFormat] comes with `isSplitable` that uses a Hadoop compression codec to know whether a given file is splitable or not.

NOTE: `isSplitable` is used exclusively when `FileSourceScanExec` is requested to link:spark-sql-SparkPlan-FileSourceScanExec.adoc#createNonBucketedReadRDD[createNonBucketedReadRDD].

=== [[supportBatch]] `supportBatch` Method

[source, scala]
----
supportBatch(sparkSession: SparkSession, dataSchema: StructType): Boolean
----

`supportBatch` is always disabled, i.e. `false` (and is expected to be overriden by `FileFormats` that want to support link:spark-sql-vectorized-parquet-reader.adoc[vectorized decoding]).

[NOTE]
====
`supportBatch` is used when:

* `FileSourceScanExec` physical operator is link:spark-sql-SparkPlan-FileSourceScanExec.adoc#creating-instance[created] (and initializes link:spark-sql-SparkPlan-FileSourceScanExec.adoc#supportsBatch[supportsBatch] flag)

* `OrcFileFormat` is requested to link:spark-sql-OrcFileFormat.adoc#buildReaderWithPartitionValues[buildReaderWithPartitionValues]

* `ParquetFileFormat` is requested to link:spark-sql-ParquetFileFormat.adoc#buildReaderWithPartitionValues[buildReaderWithPartitionValues]
====
