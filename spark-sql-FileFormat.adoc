== [[FileFormat]] FileFormat

`FileFormat` is the <<contract, contract>> for <<implementations, data source providers>> that want to <<buildReader, read>> and <<prepareWrite, write>> data stored in files (to/from the link:spark-sql-InternalRow.adoc[InternalRow] format).

[[contract]]
[source, scala]
----
package org.apache.spark.sql.execution.datasources

trait FileFormat {
  // only required methods that have no implementation
  // the others follow
  def inferSchema(
    sparkSession: SparkSession,
    options: Map[String, String],
    files: Seq[FileStatus]): Option[StructType]
  def prepareWrite(
    sparkSession: SparkSession,
    job: Job,
    options: Map[String, String],
    dataSchema: StructType): OutputWriterFactory
}
----

.(Subset of) FileFormat Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[inferSchema]] `inferSchema`
| Used when...

| [[prepareWrite]] `prepareWrite`
| Used exclusively when `FileFormatWriter` is requested to link:spark-sql-FileFormatWriter.adoc#write[write the result of a structured query].
|===

[[implementations]]
.FileFormats
[width="100%",cols="1,2",options="header"]
|===
| FileFormat
| Description

| `HiveFileFormat`
| [[HiveFileFormat]]

| <<spark-sql-OrcFileFormat.adoc#, OrcFileFormat>>
| [[OrcFileFormat]]

| <<spark-sql-ParquetFileFormat.adoc#, ParquetFileFormat>>
| [[ParquetFileFormat]]

| <<spark-sql-TextBasedFileFormat.adoc#, TextBasedFileFormat>>
| [[TextBasedFileFormat]]

| <<spark-sql-CSVFileFormat.adoc#, CSVFileFormat>>
| [[CSVFileFormat]]

| <<spark-sql-JsonFileFormat.adoc#, JsonFileFormat>>
| [[JsonFileFormat]]

| `LibSVMFileFormat`
| [[LibSVMFileFormat]] (Spark MLlib)

| <<spark-sql-TextFileFormat.adoc#, TextFileFormat>>
| [[TextFileFormat]]
|===

=== [[buildReaderWithPartitionValues]] Building Data Reader With Partition Column Values Appended -- `buildReaderWithPartitionValues` Method

[source, scala]
----
buildReaderWithPartitionValues(
  sparkSession: SparkSession,
  dataSchema: StructType,
  partitionSchema: StructType,
  requiredSchema: StructType,
  filters: Seq[Filter],
  options: Map[String, String],
  hadoopConf: Configuration): PartitionedFile => Iterator[InternalRow]
----

`buildReaderWithPartitionValues` is simply an enhanced <<buildReader, buildReader>> that appends link:spark-sql-PartitionedFile.adoc#partitionValues[partition column values] to the internal rows produced by the reader function from <<buildReader, buildReader>>.

Internally, `buildReaderWithPartitionValues` <<buildReader, builds a data reader>> with the input parameters and gives a *data reader function* (of a link:spark-sql-PartitionedFile.adoc[PartitionedFile] to an `Iterator[InternalRow]`) that does the following:

. Creates a converter by requesting `GenerateUnsafeProjection` to link:spark-sql-GenerateUnsafeProjection.adoc#generate[generate an UnsafeProjection] for the attributes of the input `requiredSchema` and `partitionSchema`

. Applies the data reader to a `PartitionedFile` and converts the result using the converter on the joined row with the link:spark-sql-PartitionedFile.adoc#partitionValues[partition column values] appended.

NOTE: `buildReaderWithPartitionValues` is used exclusively when `FileSourceScanExec` physical operator is requested for the link:spark-sql-SparkPlan-FileSourceScanExec.adoc#inputRDDs[input RDDs].

=== [[isSplitable]] Is File Splitable Or Not -- `isSplitable` Method

[source, scala]
----
isSplitable(
  sparkSession: SparkSession,
  options: Map[String, String],
  path: Path): Boolean
----

`isSplitable` always gives `false` by default.

NOTE: link:spark-sql-TextBasedFileFormat.adoc[TextBasedFileFormat] comes with `isSplitable` that uses a Hadoop compression codec to know whether a given file is splitable or not.

NOTE: `isSplitable` is used exclusively when `FileSourceScanExec` is requested to link:spark-sql-SparkPlan-FileSourceScanExec.adoc#createNonBucketedReadRDD[createNonBucketedReadRDD].

=== [[supportBatch]] `supportBatch` Method

[source, scala]
----
supportBatch(sparkSession: SparkSession, dataSchema: StructType): Boolean
----

`supportBatch` is always disabled, i.e. `false` (and is expected to be overriden by `FileFormats` that want to support link:spark-sql-vectorized-parquet-reader.adoc[vectorized decoding]).

[NOTE]
====
`supportBatch` is used when:

* `FileSourceScanExec` physical operator is link:spark-sql-SparkPlan-FileSourceScanExec.adoc#creating-instance[created] (and initializes link:spark-sql-SparkPlan-FileSourceScanExec.adoc#supportsBatch[supportsBatch] flag)

* `OrcFileFormat` is requested to link:spark-sql-OrcFileFormat.adoc#buildReaderWithPartitionValues[buildReaderWithPartitionValues]

* `ParquetFileFormat` is requested to link:spark-sql-ParquetFileFormat.adoc#buildReaderWithPartitionValues[buildReaderWithPartitionValues]
====

=== [[vectorTypes]] `vectorTypes` Method

[source, scala]
----
vectorTypes(
  requiredSchema: StructType,
  partitionSchema: StructType,
  sqlConf: SQLConf): Option[Seq[String]]
----

`vectorTypes` returns `None`, i.e. no class names of link:spark-sql-ColumnVector.adoc[ColumnVectors] for every column used in a columnar batch.

NOTE: `vectorTypes` is used exclusively when `FileSourceScanExec` physical operator is requested for link:spark-sql-SparkPlan-FileSourceScanExec.adoc#vectorTypes[concrete ColumnVector class names].

=== [[buildReader]] Building Catalyst Data Reader -- `buildReader` Method

[source, scala]
----
buildReader(
  sparkSession: SparkSession,
  dataSchema: StructType,
  partitionSchema: StructType,
  requiredSchema: StructType,
  filters: Seq[Filter],
  options: Map[String, String],
  hadoopConf: Configuration): PartitionedFile => Iterator[InternalRow]
----

`buildReader` simply reports a `UnsupportedOperationException`:

```
buildReader is not supported for [className]
```

NOTE: `buildReader` is used exclusively when `FileFormat` is requested to <<buildReaderWithPartitionValues, build a data reader with partition column values appended>>.
