== Dataset API -- Untyped Transformations

*Untyped transformations* are part of the Dataset API for transforming a `Dataset` to a <<spark-sql-DataFrame.adoc#, DataFrame>>, a <<spark-sql-Column.adoc#, Column>>, a <<spark-sql-RelationalGroupedDataset.adoc#, RelationalGroupedDataset>>, a <<spark-sql-DataFrameNaFunctions.adoc#, DataFrameNaFunctions>> or a <<spark-sql-DataFrameStatFunctions.adoc#, DataFrameStatFunctions>> (and hence _untyped_).

[[methods]]
.Dataset API's Untyped Transformations
[cols="1,2",options="header",width="100%"]
|===
| Transformation
| Description

| <<agg, agg>>
a|

[source, scala]
----
agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame
agg(exprs: Map[String, String]): DataFrame
agg(exprs: java.util.Map[String, String]): DataFrame
agg(expr: Column, exprs: Column*): DataFrame
----

| <<apply, apply>>
a| Selects a column based on the column name (i.e. maps a `Dataset` onto a `Column`)

[source, scala]
----
apply(colName: String): Column
----

| <<checkpoint, checkpoint>>
a|

[source, scala]
----
checkpoint(): Dataset[T]
checkpoint(eager: Boolean): Dataset[T]
----

| <<col, col>>
a| Selects a column based on the column name (i.e. maps a `Dataset` onto a `Column`)

[source, scala]
----
col(colName: String): Column
----

| <<colRegex, colRegex>>
a| (*New in 2.3.0*) Selects a column based on the column name specified as a regex (i.e. maps a `Dataset` onto a `Column`)

[source, scala]
----
colRegex(colName: String): Column
----

| <<crossJoin, crossJoin>>
a|

[source, scala]
----
crossJoin(right: Dataset[_]): DataFrame
----

| <<cube, cube>>
a|

[source, scala]
----
cube(cols: Column*): RelationalGroupedDataset
cube(col1: String, cols: String*): RelationalGroupedDataset
----

| <<drop, drop>>
a|

[source, scala]
----
drop(colName: String): DataFrame
drop(colNames: String*): DataFrame
drop(col: Column): DataFrame
----

| <<groupBy, groupBy>>
a|

[source, scala]
----
groupBy(cols: Column*): RelationalGroupedDataset
groupBy(col1: String, cols: String*): RelationalGroupedDataset
----

| <<join, join>>
a|

[source, scala]
----
join(right: Dataset[_]): DataFrame
join(right: Dataset[_], usingColumn: String): DataFrame
join(right: Dataset[_], usingColumns: Seq[String]): DataFrame
join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame
join(right: Dataset[_], joinExprs: Column): DataFrame
join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame
----

| <<localCheckpoint, localCheckpoint>>
a| (*New in 2.3.0*)

[source, scala]
----
localCheckpoint(): Dataset[T]
localCheckpoint(eager: Boolean): Dataset[T]
----

| <<na, na>>
a|

[source, scala]
----
na: DataFrameNaFunctions
----

| <<rollup, rollup>>
a|

[source, scala]
----
rollup(cols: Column*): RelationalGroupedDataset
rollup(col1: String, cols: String*): RelationalGroupedDataset
----

| <<select, select>>
a|

[source, scala]
----
select(cols: Column*): DataFrame
select(col: String, cols: String*): DataFrame
----

| <<selectExpr, selectExpr>>
a|

[source, scala]
----
selectExpr(exprs: String*): DataFrame
----

| <<stat, stat>>
a|

[source, scala]
----
stat: DataFrameStatFunctions
----

| <<toDF, toDF>>
a|

[source, scala]
----
toDF(): DataFrame
----

| <<withColumn, withColumn>>
a|

[source, scala]
----
withColumn(colName: String, col: Column): DataFrame
----

| <<withColumnRenamed, withColumnRenamed>>
a|

[source, scala]
----
withColumnRenamed(existingName: String, newName: String): DataFrame
----
|===

=== [[agg]] `agg` Untyped Transformation

[source, scala]
----
agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame
agg(exprs: Map[String, String]): DataFrame
agg(exprs: java.util.Map[String, String]): DataFrame
agg(expr: Column, exprs: Column*): DataFrame
----

`agg`...FIXME

=== [[apply]] `apply` Untyped Transformation

[source, scala]
----
apply(colName: String): Column
----

`apply` selects a column based on the column name (i.e. maps a `Dataset` onto a `Column`).

=== [[checkpoint]] `checkpoint` Untyped Transformation

[source, scala]
----
checkpoint(): Dataset[T]
checkpoint(eager: Boolean): Dataset[T]
----

`checkpoint`...FIXME

=== [[col]] `col` Untyped Transformation

[source, scala]
----
col(colName: String): Column
----

`col` selects a column based on the column name (i.e. maps a `Dataset` onto a `Column`).

Internally, `col` branches off per the input column name.

If the column name is `*` (a star), `col` simply creates a <<spark-sql-Column.adoc#apply, Column>> with <<spark-sql-Expression-ResolvedStar.adoc#, ResolvedStar>> expression (with the <<spark-sql-catalyst-QueryPlan.adoc#output, schema output attributes>> of the <<spark-sql-QueryExecution.adoc#analyzed, analyzed logical plan>> of the <<spark-sql-Dataset.adoc#queryExecution, QueryExecution>>).

Otherwise, `col` uses <<colRegex, colRegex>> untyped transformation when <<spark-sql-properties.adoc#spark.sql.parser.quotedRegexColumnNames, spark.sql.parser.quotedRegexColumnNames>> configuration property is enabled.

In the case when the column name is not `*` and <<spark-sql-properties.adoc#spark.sql.parser.quotedRegexColumnNames, spark.sql.parser.quotedRegexColumnNames>> configuration property is disabled, `col` creates a <<spark-sql-Column.adoc#apply, Column>> with the column name <<spark-sql-Dataset.adoc#resolve, resolved>> (as a <<spark-sql-Expression-NamedExpression.adoc#, NamedExpression>>).

=== [[colRegex]] `colRegex` Untyped Transformation

[source, scala]
----
colRegex(colName: String): Column
----

(*New in 2.3.0*) `colRegex` selects a column based on the column name specified as a regex (i.e. maps a `Dataset` onto a `Column`).

NOTE: `colRegex` is used in <<col, col>> when <<spark-sql-properties.adoc#spark.sql.parser.quotedRegexColumnNames, spark.sql.parser.quotedRegexColumnNames>> configuration property is enabled (and the column name is not `*`).

Internally, `colRegex` matches the input column name to different regular expressions (in the order):

. For column names with quotes without a qualifier, `colRegex` simply creates a <<spark-sql-Column.adoc#apply, Column>> with a <<spark-sql-Expression-UnresolvedRegex.adoc#, UnresolvedRegex>> (with no table)

. For column names with quotes with a qualifier, `colRegex` simply creates a <<spark-sql-Column.adoc#apply, Column>> with a <<spark-sql-Expression-UnresolvedRegex.adoc#, UnresolvedRegex>> (with a table specified)

. For other column names, `colRegex` (behaves like <<col, col>> and) creates a <<spark-sql-Column.adoc#apply, Column>> with the column name <<spark-sql-Dataset.adoc#resolve, resolved>> (as a <<spark-sql-Expression-NamedExpression.adoc#, NamedExpression>>)

=== [[crossJoin]] `crossJoin` Untyped Transformation

[source, scala]
----
crossJoin(right: Dataset[_]): DataFrame
----

`crossJoin`...FIXME

=== [[cube]] `cube` Untyped Transformation

[source, scala]
----
cube(cols: Column*): RelationalGroupedDataset
cube(col1: String, cols: String*): RelationalGroupedDataset
----

`cube`...FIXME

=== [[drop]] `drop` Untyped Transformation

[source, scala]
----
drop(colName: String): DataFrame
drop(colNames: String*): DataFrame
drop(col: Column): DataFrame
----

`drop`...FIXME

=== [[groupBy]] `groupBy` Untyped Transformation

[source, scala]
----
groupBy(cols: Column*): RelationalGroupedDataset
groupBy(col1: String, cols: String*): RelationalGroupedDataset
----

`groupBy`...FIXME

=== [[join]] `join` Untyped Transformation

[source, scala]
----
join(right: Dataset[_]): DataFrame
join(right: Dataset[_], usingColumn: String): DataFrame
join(right: Dataset[_], usingColumns: Seq[String]): DataFrame
join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame
join(right: Dataset[_], joinExprs: Column): DataFrame
join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame
----

`join`...FIXME

=== [[localCheckpoint]] `localCheckpoint` Untyped Transformation

[source, scala]
----
localCheckpoint(): Dataset[T]
localCheckpoint(eager: Boolean): Dataset[T]
----

(*New in 2.3.0*) `localCheckpoint`...FIXME

=== [[na]] `na` Untyped Transformation

[source, scala]
----
na: DataFrameNaFunctions
----

`na` simply creates a <<spark-sql-DataFrameNaFunctions.adoc#, DataFrameNaFunctions>> to work with missing data.

=== [[rollup]] `rollup` Untyped Transformation

[source, scala]
----
rollup(cols: Column*): RelationalGroupedDataset
rollup(col1: String, cols: String*): RelationalGroupedDataset
----

`rollup`...FIXME

=== [[select]] `select` Untyped Transformation

[source, scala]
----
select(cols: Column*): DataFrame
select(col: String, cols: String*): DataFrame
----

`select`...FIXME

=== [[selectExpr]] `selectExpr` Untyped Transformation

[source, scala]
----
selectExpr(exprs: String*): DataFrame
----

`selectExpr`...FIXME

=== [[stat]] `stat` Untyped Transformation

[source, scala]
----
stat: DataFrameStatFunctions
----

`stat` simply creates a <<spark-sql-DataFrameStatFunctions.adoc#, DataFrameStatFunctions>> to work with statistic functions.

=== [[toDF]] `toDF` Untyped Transformation

[source, scala]
----
toDF(): DataFrame
----

`toDF`...FIXME

=== [[withColumn]] `withColumn` Untyped Transformation

[source, scala]
----
withColumn(colName: String, col: Column): DataFrame
----

`withColumn`...FIXME

=== [[withColumnRenamed]] `withColumnRenamed` Untyped Transformation

[source, scala]
----
withColumnRenamed(existingName: String, newName: String): DataFrame
----

`withColumnRenamed`...FIXME
