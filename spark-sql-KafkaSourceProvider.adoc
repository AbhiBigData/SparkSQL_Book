== [[KafkaSourceProvider]] KafkaSourceProvider

[[shortName]]
`KafkaSourceProvider` is a <<spark-sql-DataSourceRegister.adoc#, DataSourceRegister>> and registers itself to handle *kafka* data source format.

NOTE: `KafkaSourceProvider` uses `META-INF/services/org.apache.spark.sql.sources.DataSourceRegister` file for the registration which is available in the https://github.com/apache/spark/blob/v2.3.1/external/kafka-0-10-sql/src/main/resources/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister[source code] of Apache Spark.

`KafkaSourceProvider` is a link:spark-sql-CreatableRelationProvider.adoc[CreatableRelationProvider] and a <<createRelation-RelationProvider, RelationProvider>>.

[source, scala]
----
// start Spark application like spark-shell with the following package
// --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0
scala> val fromKafkaTopic1 = spark.
  read.
  format("kafka").
  option("subscribe", "topic1").  // subscribe, subscribepattern, or assign
  option("kafka.bootstrap.servers", "localhost:9092").
  load("gauge_one")
----

`KafkaSourceProvider` <<sourceSchema, uses a fixed schema>> (and makes sure that a user did not set a custom one).

[source, scala]
----
import org.apache.spark.sql.types.StructType
val schema = new StructType().add($"id".int)
scala> spark
  .read
  .format("kafka")
  .option("subscribe", "topic1")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .schema(schema) // <-- defining a custom schema is not supported
  .load
org.apache.spark.sql.AnalysisException: kafka does not allow user-specified schemas.;
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:307)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)
  ... 48 elided
----

[NOTE]
====
`KafkaSourceProvider` is also a `StreamSourceProvider`, a `StreamSinkProvider`, a `StreamWriteSupport` and a `ContinuousReadSupport` that are contracts used in Spark Structured Streaming.

You can find more on Spark Structured Streaming in my gitbook https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Spark Structured Streaming].
====

=== [[createRelation-RelationProvider]] Creating BaseRelation -- `createRelation` Method (from RelationProvider)

[source, scala]
----
createRelation(
  sqlContext: SQLContext,
  parameters: Map[String, String]): BaseRelation
----

NOTE: `createRelation` is part of <<spark-sql-RelationProvider.adoc#createRelation, RelationProvider Contract>> to create a <<spark-sql-BaseRelation.adoc#, BaseRelation>> (for reading or writing).

`createRelation` starts by <<validateBatchOptions, validating batch options>> in the input `parameters`.

`createRelation` collects all ``kafka.``-prefixed key options (in the input `parameters`) and creates a local `specifiedKafkaParams` with the keys without the `kafka.` prefix (e.g. `kafka.whatever` is simply `whatever`).

`createRelation` <<getKafkaOffsetRangeLimit, creates a KafkaOffsetRangeLimit>> with the `startingoffsets` offset option key (in the given `parameters`) and `EarliestOffsetRangeLimit` as the default offsets.

createRelation` makes sure that the `KafkaOffsetRangeLimit` is not `EarliestOffsetRangeLimit` or throws a `AssertionError`.

`createRelation` <<getKafkaOffsetRangeLimit, creates another KafkaOffsetRangeLimit>>, but this time with the `endingoffsets` offset option key (in the given `parameters`) and `LatestOffsetRangeLimit` as the default offsets.

`createRelation` makes sure that the `KafkaOffsetRangeLimit` is not `EarliestOffsetRangeLimit` or throws a `AssertionError`.

In the end, `createRelation` creates a <<spark-sql-KafkaRelation.adoc#creating-instance, KafkaRelation>> with the <<strategy, subscription strategy>> (in the given `parameters`), <<failOnDataLoss, failOnDataLoss>> option, and the starting and ending offsets.

=== [[validateBatchOptions]] Validating Batch Options -- `validateBatchOptions` Internal Method

[source, scala]
----
validateBatchOptions(caseInsensitiveParams: Map[String, String]): Unit
----

`validateBatchOptions`...FIXME

NOTE: `validateBatchOptions` is used exclusively when `KafkaSourceProvider` is requested to <<createRelation, create a KafkaRelation>>.

=== [[createRelation-CreatableRelationProvider]] `createRelation` Method (from CreatableRelationProvider)

[source, scala]
----
createRelation(
  sqlContext: SQLContext,
  mode: SaveMode,
  parameters: Map[String, String],
  df: DataFrame): BaseRelation
----

CAUTION: FIXME

NOTE: `createRelation` is part of link:spark-sql-CreatableRelationProvider.adoc#contract[CreatableRelationProvider Contract].

=== [[sourceSchema]] `sourceSchema` Method

[source, scala]
----
sourceSchema(
  sqlContext: SQLContext,
  schema: Option[StructType],
  providerName: String,
  parameters: Map[String, String]): (String, StructType)
----

CAUTION: FIXME

[source, scala]
----
val fromKafka = spark.read.format("kafka")...
scala> fromKafka.printSchema
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)
----

NOTE: `sourceSchema` is part of Structured Streaming's `StreamSourceProvider` Contract.

=== [[getKafkaOffsetRangeLimit]] `getKafkaOffsetRangeLimit` Object Method

[source, scala]
----
getKafkaOffsetRangeLimit(
  params: Map[String, String],
  offsetOptionKey: String,
  defaultOffsets: KafkaOffsetRangeLimit): KafkaOffsetRangeLimit
----

`getKafkaOffsetRangeLimit`...FIXME

NOTE: `getKafkaOffsetRangeLimit` is used when `KafkaSourceProvider` is requested to <<validateBatchOptions, validate batch options>> and <<createRelation-RelationProvider, create a BaseRelation>> (and also in `createSource` and `createContinuousReader` for Spark Structured Streaming).

=== [[strategy]] `strategy` Internal Method

[source, scala]
----
strategy(caseInsensitiveParams: Map[String, String]): ConsumerStrategy
----

`strategy`...FIXME

NOTE: `strategy` is used when `KafkaSourceProvider` is requested to <<createRelation-RelationProvider, create a BaseRelation>> (and also in `createSource` and `createContinuousReader` for Spark Structured Streaming).

=== [[failOnDataLoss]] `failOnDataLoss` Internal Method

[source, scala]
----
failOnDataLoss(caseInsensitiveParams: Map[String, String]): Boolean
----

`failOnDataLoss`...FIXME

NOTE: `failOnDataLoss` is used when `KafkaSourceProvider` is requested to <<createRelation-RelationProvider, create a BaseRelation>> (and also in `createSource` and `createContinuousReader` for Spark Structured Streaming).
