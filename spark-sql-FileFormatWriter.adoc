== [[FileFormatWriter]] FileFormatWriter

`FileFormatWriter` is...FIXME

[[logging]]
[TIP]
====
Enable `ERROR`, `INFO`, `DEBUG` logging level for `org.apache.spark.sql.execution.datasources.FileFormatWriter` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.datasources.FileFormatWriter=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[write]] Writing Query Result -- `write` Method

[source, scala]
----
write(
  sparkSession: SparkSession,
  queryExecution: QueryExecution,
  fileFormat: FileFormat,
  committer: FileCommitProtocol,
  outputSpec: OutputSpec,
  hadoopConf: Configuration,
  partitionColumns: Seq[Attribute],
  bucketSpec: Option[BucketSpec],
  statsTrackers: Seq[WriteJobStatsTracker],
  options: Map[String, String]): Set[String]
----

`write`...FIXME

[NOTE]
====
`write` is used when:

* link:spark-sql-LogicalPlan-InsertIntoHadoopFsRelationCommand.adoc#run[InsertIntoHadoopFsRelationCommand] and link:spark-sql-LogicalPlan-InsertIntoHiveTable.adoc#run[InsertIntoHiveTable] commands are executed

* Spark Structured Streaming's `FileStreamSink` is requested to add a streaming batch.
====
