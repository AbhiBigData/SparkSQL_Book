== [[dsl]] Catalyst DSL -- Implicit Conversions for Catalyst Data Structures

*Catalyst DSL* is a collection of <<implicit-conversions, Scala implicit conversions>> that allow for constructing link:spark-sql-catalyst.adoc[Catalyst data structures], i.e. link:spark-sql-Expression.adoc[expressions] and link:spark-sql-LogicalPlan.adoc[logical plans], more easily.

The goal of Catalyst DSL is to make working with Spark SQL's building blocks easier (e.g. for testing or Spark SQL internals exploration).

Catalyst DSL is part of `package object dsl` with the following Scala `objects` (and their Scala implicit conversions):

* [[expressions]] `expressions` for link:spark-sql-Expression.adoc[Catalyst expressions]
** Creating link:spark-sql-Expression-Literal.adoc[Literals] from Scala (e.g. `Boolean`, `Int`, `String`, `Array[Byte]`) or Java (e.g. `java.sql.Date`, `java.sql.Timestamp`) types
** Creating `UnresolvedAttributes` from Scala's `Symbol` or `$`-prefixed strings (using `$` string interpolator)
** Creating `AttributeReferences` from Scala's `Symbol`
** Creating `Expressions` or `UnresolvedAttributes` right from `Strings`
** FIXME

[source, scala]
----
import org.apache.spark.sql.catalyst.dsl.expressions._
scala> :type $"hello"
org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute
----

[IMPORTANT]
====
Some implicit conversions from the Catalyst DSL interfere with the implicits conversions from `SQLImplicits` that are imported automatically in `spark-shell` (through `spark.implicits._`).

```
scala> 'hello.decimal
<console>:30: error: type mismatch;
 found   : Symbol
 required: ?{def decimal: ?}
Note that implicit conversions are not applicable because they are ambiguous:
 both method symbolToColumn in class SQLImplicits of type (s: Symbol)org.apache.spark.sql.ColumnName
 and method DslSymbol in trait ExpressionConversions of type (sym: Symbol)org.apache.spark.sql.catalyst.dsl.expressions.DslSymbol
 are possible conversion functions from Symbol to ?{def decimal: ?}
       'hello.decimal
       ^
<console>:30: error: value decimal is not a member of Symbol
       'hello.decimal
              ^
```

Use `sbt console` with Spark libraries defined (in `build.sbt`) instead.

---

You can also disable an implicit conversion using a trick described in https://stackoverflow.com/q/15592324/1305344[How can an implicit be unimported from the Scala repl?]

[source, scala]
----
// HACK: Disable one SparkSession.implicits
// implicit def symbolToColumn(s: Symbol): org.apache.spark.sql.ColumnName
trait ThatWasABadIdea
implicit def symbolToColumn(ack: ThatWasABadIdea) = ack

// HACK: Disable $ implicit conversion
implicit class StringToColumn(val sc: StringContext) {}
----
====

[[example]]
[source, scala]
----
import org.apache.spark.sql.catalyst.dsl.expressions._
import org.apache.spark.sql.catalyst.dsl.plans._

// ExpressionConversions

import org.apache.spark.sql.catalyst.expressions.Literal
scala> val trueLit: Literal = true
trueLit: org.apache.spark.sql.catalyst.expressions.Literal = true

import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute
scala> val name: UnresolvedAttribute = 'name
name: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'name

// NOTE: This conversion may not work, e.g. in spark-shell
// There is another implicit conversion StringToColumn in SQLImplicits
// It is automatically imported in spark-shell
// See :imports
val id: UnresolvedAttribute = $"id"

import org.apache.spark.sql.catalyst.expressions.Expression
scala> val expr: Expression = sum('id)
expr: org.apache.spark.sql.catalyst.expressions.Expression = sum('id)

// implicit class DslSymbol
scala> 'hello.s
res2: String = hello

scala> 'hello.attr
res4: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'hello

// implicit class DslString
scala> "helo".expr
res0: org.apache.spark.sql.catalyst.expressions.Expression = helo

scala> "helo".attr
res1: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'helo

// logical plans

scala> val t1 = table("t1")
t1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
'UnresolvedRelation `t1`

scala> val p = t1.select('*).serialize[String].where('id % 2 == 0)
p: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
'Filter false
+- 'SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#1]
   +- 'Project ['*]
      +- 'UnresolvedRelation `t1`

// FIXME Does not work because SimpleAnalyzer's catalog is empty
// the p plan references a t1 table
import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer
scala> p.analyze
----

[[implicit-conversions]]
.Catalyst DSL's Implicit Conversions
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| `ExpressionConversions`
a| [[ExpressionConversions]]

* Adds <<ImplicitOperators, ImplicitOperators>> operators to link:spark-sql-Expression.adoc[Catalyst expressions]

* Converts Scala native types (e.g. `Boolean`, `Long`, `String`, `Date`, `Timestamp`) and Spark SQL types (i.e. `Decimal`) to link:spark-sql-Expression-Literal.adoc[Literal expressions]

* Converts Scala's `Symbol` to `UnresolvedAttribute` and `AttributeReference` expressions

* Converts `$"col name"` to an `UnresolvedAttribute` expression

* [[star]] Adds aggregate and non-aggregate functions to link:spark-sql-Expression.adoc[Catalyst expressions] (e.g. `sum`, `count`, `upper`, `star`, `callFunction`, `windowSpec`, `windowExpr`)
+
[source, scala]
----
star(names: String*): Expression
----
+
[source, scala]
----
import org.apache.spark.sql.catalyst.dsl.expressions._
val s = star()

import org.apache.spark.sql.catalyst.analysis.UnresolvedStar
assert(s.isInstanceOf[UnresolvedStar])

val s = star("a", "b")
scala> println(s)
WrappedArray(a, b).*
----

* [[function]][[distinctFunction]] Creates link:spark-sql-Expression-UnresolvedFunction.adoc[UnresolvedFunction] (`function` and `distinctFunction` operators)
+
[source, scala]
----
function(exprs: Expression*): UnresolvedFunction
distinctFunction(exprs: Expression*): UnresolvedFunction
----
+
[source, scala]
----
import org.apache.spark.sql.catalyst.dsl.expressions._

// Works with Scala Symbols only
val f = 'f.function()
scala> :type f
org.apache.spark.sql.catalyst.analysis.UnresolvedFunction

scala> f.isDistinct
res0: Boolean = false

val g = 'g.distinctFunction()
scala> g.isDistinct
res1: Boolean = true
----

* [[DslAttribute]][[notNull]][[canBeNull]] Adds `canBeNull` and `notNull` methods to create a `AttributeReference` with `nullability` on or off, respectively
+
[source, scala]
----
notNull: AttributeReference
canBeNull: AttributeReference
----

* [[at]] Adds `at` method to `AttributeReferences` to create a link:spark-sql-Expression-BoundReference.adoc[BoundReference]
+
[source, scala]
----
at(ordinal: Int): BoundReference
----
+
[source, scala]
----
import org.apache.spark.sql.catalyst.dsl.expressions._
val boundRef = 'hello.string.at(4)
scala> println(boundRef)
input[4, string, true]
----

| `ImplicitOperators`
| [[ImplicitOperators]][[in]] Operators for link:spark-sql-Expression.adoc[expressions], i.e. `in`.

| [[plans]] `plans`
a|

* [[hint]] `hint` for a link:spark-sql-LogicalPlan-UnresolvedHint.adoc[UnresolvedHint] logical operator
+
[source, scala]
----
hint(name: String, parameters: Any*): LogicalPlan
----

* [[join]] `join` for a link:spark-sql-LogicalPlan-Join.adoc[Join] logical operator
+
[source, scala]
----
join(
  otherPlan: LogicalPlan,
  joinType: JoinType = Inner,
  condition: Option[Expression] = None): LogicalPlan
----

* [[table]] `table` for a link:spark-sql-LogicalPlan-UnresolvedRelation.adoc[UnresolvedRelation] logical operator
+
[source, scala]
----
table(ref: String): LogicalPlan
table(db: String, ref: String): LogicalPlan
----

* [[DslLogicalPlan]] Logical operators (e.g. `select`, `where`, `filter`, `serialize`, `groupBy`, `window`, `generate`)
|===
