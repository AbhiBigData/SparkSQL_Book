== [[BaseRelation]] BaseRelation -- Collection of Tuples with Schema

`BaseRelation` is the <<contract, contract>> in Spark SQL to model a collection of tuples (from a data source) with a <<schema, schema>>.

NOTE: "Data source", "relation" and "table" are often used as synonyms.

[[sizeInBytes]]
`BaseRelation` can optionally provide information about its estimated size in bytes (as `sizeInBytes`) that defaults to link:spark-sql-properties.adoc#spark.sql.defaultSizeInBytes[spark.sql.defaultSizeInBytes] internal property (i.e. infinite).

[source, scala]
----
sizeInBytes: Long
----

`BaseRelation` uses <<needConversion, needConversion>> flag to control type conversion of objects inside link:spark-sql-Row.adoc[Rows] to Catalyst types, e.g. `java.lang.String` to `UTF8String`.

NOTE: It is recommended that custom data sources (outside Spark SQL) should leave <<needConversion, needConversion>> flag enabled, i.e. `true`.

[[unhandledFilters]]
`BaseRelation` computes the list of `Filter` that this data source may not be able to handle.

[[implementations]]
.BaseRelations
[width="100%",cols="1,2",options="header"]
|===
| BaseRelation
| Description

| link:spark-sql-BaseRelation-HadoopFsRelation.adoc[HadoopFsRelation]
|

| link:spark-sql-BaseRelation-JDBCRelation.adoc[JDBCRelation]
|

| `KafkaRelation`
| Structured Streaming's `BaseRelation` for datasets with records from Apache Kafka
|===

NOTE: `BaseRelation` is "created" using ``DataSource``'s link:spark-sql-DataSource.adoc#resolveRelation[resolveRelation].

NOTE: `BaseRelation` is transformed into a link:spark-sql-DataFrame.adoc[DataFrame] using link:spark-sql-SparkSession.adoc#baseRelationToDataFrame[SparkSession.baseRelationToDataFrame].

=== [[contract]] BaseRelation Contract

[source, scala]
----
package org.apache.spark.sql.sources

abstract class BaseRelation {
  // only required methods that have no implementation
  def schema: StructType
  def sqlContext: SQLContext
}
----

.(Subset of) BaseRelation Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[schema]] `schema`
| link:spark-sql-StructType.adoc[StructType]

| [[sqlContext]] `sqlContext`
| link:spark-sql-SQLContext.adoc[SQLContext]
|===

=== [[needConversion]] Should Objects Inside Rows Be Converted to Catalyst Types? -- `needConversion` Method

[source, scala]
----
needConversion: Boolean
----

`needConversion` flag is enabled (`true`) by default.

NOTE: It is recommended to leave `needConversion` enabled for data sources outside Spark SQL.

NOTE: `needConversion` is used exclusively when `DataSourceStrategy` execution planning strategy is link:spark-sql-SparkStrategy-DataSourceStrategy.adoc#apply[executed] (and link:spark-sql-SparkStrategy-DataSourceStrategy.adoc#toCatalystRDD[converts an RDD of Rows to Catalyst RDD (RDD of InternalRows)]).
