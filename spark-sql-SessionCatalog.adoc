== [[SessionCatalog]] SessionCatalog -- Session-Scoped Catalog of Relational Entities

`SessionCatalog` is the catalog of (the metadata of) session-scoped relational temporary and permanent relational entities, i.e. databases, tables, views, partitions, and functions.

For the metadata of permanent entities (i.e. <<getTableMetadata, tables>>) `SessionCatalog` requests <<externalCatalog, ExternalCatalog>>.

NOTE: `SessionCatalog` is a layer over <<externalCatalog, ExternalCatalog>> in a link:spark-sql-SparkSession.adoc#sessionState[SparkSession] which allows for different metastores (i.e. `in-memory` or `hive`) to be used.

`SessionCatalog` is available through link:spark-sql-SessionState.adoc#catalog[SessionState] (of a link:spark-sql-SparkSession.adoc#sessionState[SparkSession]).

[source, scala]
----
scala> spark.version
res0: String = 2.3.0

scala> :type spark.sessionState.catalog
org.apache.spark.sql.catalyst.catalog.SessionCatalog
----

`SessionCatalog` is <<creating-instance, created>> when `SessionState` sets link:spark-sql-SessionState.adoc#catalog[catalog].

[[internal-registries]]
.SessionCatalog's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[currentDb]] `currentDb`
| FIXME

Used when...FIXME

| [[functionResourceLoader]] `functionResourceLoader`
| FIXME

Used when...FIXME

| [[tableRelationCache]] `tableRelationCache`
| A cache of fully-qualified table names to link:spark-sql-LogicalPlan.adoc[table relation plans] (i.e. `LogicalPlan`).

Used when `SessionCatalog` <<refreshTable, refreshes a table>>

| [[tempTables]] `tempTables`
| FIXME

Used when...FIXME

| [[tempViews]] `tempViews`
| Registry of temporary views (i.e. non-global temporary tables)
|===

=== [[requireTableExists]] `requireTableExists` Internal Method

[source, scala]
----
requireTableExists(name: TableIdentifier): Unit
----

`requireTableExists`...FIXME

NOTE: `requireTableExists` is used when...FIXME

=== [[databaseExists]] `databaseExists` Method

[source, scala]
----
databaseExists(db: String): Boolean
----

`databaseExists`...FIXME

NOTE: `databaseExists` is used when...FIXME

=== [[listTables]] `listTables` Method

[source, scala]
----
listTables(db: String): Seq[TableIdentifier]
listTables(db: String, pattern: String): Seq[TableIdentifier]
----

`listTables`...FIXME

NOTE: `listTables` is used when...FIXME

=== [[isTemporaryTable]] `isTemporaryTable` Method

[source, scala]
----
isTemporaryTable(name: TableIdentifier): Boolean
----

`isTemporaryTable`...FIXME

NOTE: `isTemporaryTable` is used when...FIXME

=== [[alterPartitions]] `alterPartitions` Method

[source, scala]
----
alterPartitions(tableName: TableIdentifier, parts: Seq[CatalogTablePartition]): Unit
----

`alterPartitions`...FIXME

NOTE: `alterPartitions` is used when...FIXME

=== [[listPartitions]] `listPartitions` Method

[source, scala]
----
listPartitions(
  tableName: TableIdentifier,
  partialSpec: Option[TablePartitionSpec] = None): Seq[CatalogTablePartition]
----

`listPartitions`...FIXME

NOTE: `listPartitions` is used when...FIXME

=== [[alterTable]] `alterTable` Method

[source, scala]
----
alterTable(tableDefinition: CatalogTable): Unit
----

`alterTable`...FIXME

[NOTE]
====
`alterTable` is used when the following logical commands are executed:

* `AlterTableSetPropertiesCommand`, `AlterTableUnsetPropertiesCommand`, `AlterTableChangeColumnCommand`, `AlterTableSerDePropertiesCommand`, `AlterTableRecoverPartitionsCommand`, `AlterTableSetLocationCommand`, link:spark-sql-LogicalPlan-AlterViewAsCommand.adoc#run[AlterViewAsCommand] (for link:spark-sql-LogicalPlan-AlterViewAsCommand.adoc#alterPermanentView[permanent views])
====

=== [[alterTableStats]] Altering Table Statistics in Metastore (and Invalidating Internal Cache) -- `alterTableStats` Method

[source, scala]
----
alterTableStats(identifier: TableIdentifier, newStats: Option[CatalogStatistics]): Unit
----

`alterTableStats` requests <<externalCatalog, ExternalCatalog>> to link:spark-sql-ExternalCatalog.adoc#alterTableStats[alter the statistics of the table] (per `identifier`) followed by <<refreshTable, invalidating the table relation cache>>.

`alterTableStats` reports a `NoSuchDatabaseException` if the <<databaseExists, database does not exist>>.

`alterTableStats` reports a `NoSuchTableException` if the <<tableExists, table does not exist>>.

[NOTE]
====
`alterTableStats` is used when the following logical commands are executed:

1. link:spark-sql-LogicalPlan-AnalyzeTableCommand.adoc#run[AnalyzeTableCommand], link:spark-sql-LogicalPlan-AnalyzeColumnCommand.adoc#run[AnalyzeColumnCommand], `AlterTableAddPartitionCommand`, `TruncateTableCommand`

1. (*indirectly* through `CommandUtils` when requested for link:spark-sql-CommandUtils.adoc#updateTableStats[updating existing table statistics]) link:spark-sql-LogicalPlan-InsertIntoHiveTable.adoc#run[InsertIntoHiveTable], link:spark-sql-LogicalPlan-InsertIntoHadoopFsRelationCommand.adoc#run[InsertIntoHadoopFsRelationCommand], `AlterTableDropPartitionCommand`, `AlterTableSetLocationCommand` and `LoadDataCommand`
====

=== [[tableExists]] `tableExists` Method

[source, scala]
----
tableExists(name: TableIdentifier): Boolean
----

`tableExists`...FIXME

NOTE: `tableExists` is used when...FIXME

=== [[functionExists]] `functionExists` Method

CAUTION: FIXME

[NOTE]
====
`functionExists` is used in:

* link:spark-sql-LookupFunctions.adoc[LookupFunctions] logical rule (to make sure that link:spark-sql-Expression-UnresolvedFunction.adoc[UnresolvedFunction] can be resolved, i.e. is registered with `SessionCatalog`)

* `CatalogImpl` to link:spark-sql-CatalogImpl.adoc#functionExists[check if a function exists in a database]

* ...
====

=== [[listFunctions]] `listFunctions` Method

CAUTION: FIXME

=== [[refreshTable]] Invalidating Table Relation Cache (aka Refreshing Table) -- `refreshTable` Method

[source, scala]
----
refreshTable(name: TableIdentifier): Unit
----

`refreshTable`...FIXME

NOTE: `refreshTable` is used when...FIXME

=== [[createTempFunction]] `createTempFunction` Method

CAUTION: FIXME

=== [[loadFunctionResources]] `loadFunctionResources` Method

CAUTION: FIXME

=== [[alterTempViewDefinition]] `alterTempViewDefinition` Method

[source, scala]
----
alterTempViewDefinition(name: TableIdentifier, viewDefinition: LogicalPlan): Boolean
----

`alterTempViewDefinition` alters the temporary view by <<createTempView, updating an in-memory temporary table>> (when a database is not specified and the table has already been registered) or a global temporary table (when a database is specified and it is for global temporary tables).

NOTE: "Temporary table" and "temporary view" are synonyms.

`alterTempViewDefinition` returns `true` when an update could be executed and finished successfully.

=== [[createTempView]] `createTempView` Method

CAUTION: FIXME

=== [[createGlobalTempView]] `createGlobalTempView` Method

CAUTION: FIXME

=== [[createTable]] `createTable` Method

CAUTION: FIXME

=== [[creating-instance]] Creating SessionCatalog Instance

`SessionCatalog` takes the following when created:

* [[externalCatalog]] link:spark-sql-ExternalCatalog.adoc[ExternalCatalog]
* [[globalTempViewManager]] `GlobalTempViewManager`
* [[functionResourceLoader]] `FunctionResourceLoader`
* [[functionRegistry]] link:spark-sql-FunctionRegistry.adoc[FunctionRegistry]
* [[conf]] link:spark-sql-CatalystConf.adoc[CatalystConf]
* [[hadoopConf]] Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[Configuration]
* [[parser]] link:spark-sql-ParserInterface.adoc[ParserInterface]

`SessionCatalog` initializes the <<internal-registries, internal registries and counters>>.

=== [[lookupFunction]] Finding Function by Name (Using FunctionRegistry) -- `lookupFunction` Method

[source, scala]
----
lookupFunction(
  name: FunctionIdentifier,
  children: Seq[Expression]): Expression
----

`lookupFunction` finds a function by `name`.

For a function with no database defined that exists in <<functionRegistry, FunctionRegistry>>, `lookupFunction` requests `FunctionRegistry` to link:spark-sql-FunctionRegistry.adoc#lookupFunction[find the function] (by its unqualified name, i.e. with no database).

If the `name` function has the database defined or does not exist in `FunctionRegistry`, `lookupFunction` uses the fully-qualified function `name` to check if the function exists in <<functionRegistry, FunctionRegistry>> (by its fully-qualified name, i.e. with a database).

For other cases, `lookupFunction` requests <<externalCatalog, ExternalCatalog>> to find the function and <<loadFunctionResources, loads its resources>>. It then <<createTempFunction, creates a corresponding temporary function>> and link:spark-sql-FunctionRegistry.adoc#lookupFunction[looks up the function] again.

NOTE: `lookupFunction` is used exclusively when `Analyzer` link:spark-sql-Analyzer.adoc#ResolveFunctions[resolves functions].

=== [[lookupRelation]] Finding Relation in Catalogs (and Creating SubqueryAlias per Table Type) -- `lookupRelation` Method

[source, scala]
----
lookupRelation(name: TableIdentifier): LogicalPlan
----

`lookupRelation` finds the `name` table in the catalogs (i.e. <<globalTempViewManager, GlobalTempViewManager>>, <<externalCatalog, ExternalCatalog>> or <<tempViews, registry of temporary views>>) and gives a `SubqueryAlias` per table type.

[source, scala]
----
scala> spark.version
res0: String = 2.3.0

scala> :type spark.sessionState.catalog
org.apache.spark.sql.catalyst.catalog.SessionCatalog

import spark.sessionState.{catalog => c}
import org.apache.spark.sql.catalyst.TableIdentifier

// Global temp view
val db = spark.sharedState.globalTempViewManager.database
// Make the example reproducible (and so "replace")
spark.range(1).createOrReplaceGlobalTempView("gv1")
val gv1 = TableIdentifier(table = "gv1", database = Some(db))
val plan = c.lookupRelation(gv1)
scala> println(plan.numberedTreeString)
00 SubqueryAlias gv1
01 +- Range (0, 1, step=1, splits=Some(8))

val metastore = spark.sharedState.externalCatalog

// Regular table
val db = spark.catalog.currentDatabase
metastore.dropTable(db, table = "t1", ignoreIfNotExists = true, purge = true)
sql("CREATE TABLE t1 (id LONG) USING parquet")
val t1 = TableIdentifier(table = "t1", database = Some(db))
val plan = c.lookupRelation(t1)
scala> println(plan.numberedTreeString)
00 'SubqueryAlias t1
01 +- 'UnresolvedCatalogRelation `default`.`t1`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe

// Regular view (not temporary view)
// Make the example reproducible
metastore.dropTable(db, table = "v1", ignoreIfNotExists = true, purge = true)
import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable, CatalogTableType}
val v1 = TableIdentifier(table = "v1", database = Some(db))
import org.apache.spark.sql.types.StructType
val schema = new StructType().add($"id".long)
val storage = CatalogStorageFormat(locationUri = None, inputFormat = None, outputFormat = None, serde = None, compressed = false, properties = Map())
val tableDef = CatalogTable(
  identifier = v1,
  tableType = CatalogTableType.VIEW,
  storage,
  schema,
  viewText = Some("SELECT 1") /** Required or RuntimeException reported */)
metastore.createTable(tableDef, ignoreIfExists = false)
val plan = c.lookupRelation(v1)
scala> println(plan.numberedTreeString)
00 'SubqueryAlias v1
01 +- View (`default`.`v1`, [id#77L])
02    +- 'Project [unresolvedalias(1, None)]
03       +- OneRowRelation

// Temporary view
spark.range(1).createOrReplaceTempView("v2")
val v2 = TableIdentifier(table = "v2", database = None)
val plan = c.lookupRelation(v2)
scala> println(plan.numberedTreeString)
00 SubqueryAlias v2
01 +- Range (0, 1, step=1, splits=Some(8))
----

Internally, `lookupRelation` looks up the `name` table using:

1. <<globalTempViewManager, GlobalTempViewManager>> when the database name of the table matches the link:spark-sql-GlobalTempViewManager.adoc#database[name] of `GlobalTempViewManager`

i. Gives `SubqueryAlias` or reports a `NoSuchTableException`

1. <<externalCatalog, ExternalCatalog>> when the database name of the table is specified explicitly or the <<tempViews, registry of temporary views>> does not contain the table

i. Gives `SubqueryAlias` with `View` when the table is a view (aka _temporary table_)

i. Gives `SubqueryAlias` with `UnresolvedCatalogRelation` otherwise

1. The <<tempViews, registry of temporary views>>

i. Gives `SubqueryAlias` with the logical plan per the table as registered in the <<tempViews, registry of temporary views>>

NOTE: `lookupRelation` considers *default* to be the name of the database if the `name` table does not specify the database explicitly.

[NOTE]
====
`lookupRelation` is used when:

1. `DescribeTableCommand` is requested to link:spark-sql-LogicalPlan-DescribeTableCommand.adoc#run[run]

1. `ResolveRelations` logical evaluation rule is requested to link:spark-sql-ResolveRelations.adoc#lookupTableFromCatalog[lookupTableFromCatalog]
====

=== [[getTableMetadata]] Retrieving Table Metadata from External Catalog (Metastore) -- `getTableMetadata` Method

[source, scala]
----
getTableMetadata(name: TableIdentifier): CatalogTable
----

`getTableMetadata` simply requests <<externalCatalog, external catalog>> (metastore) for the link:spark-sql-ExternalCatalog.adoc#getTable[table metadata].

Before requesting the external metastore, `getTableMetadata` makes sure that the <<requireDbExists, database>> and <<requireTableExists, table>> (of the input `TableIdentifier`) both exist. If either does not exist, `getTableMetadata` reports a `NoSuchDatabaseException` or `NoSuchTableException`, respectively.

=== [[getTempViewOrPermanentTableMetadata]] Retrieving Table Metadata -- `getTempViewOrPermanentTableMetadata` Method

[source, scala]
----
getTempViewOrPermanentTableMetadata(name: TableIdentifier): CatalogTable
----

Internally, `getTempViewOrPermanentTableMetadata` branches off per database.

When a database name is not specified, `getTempViewOrPermanentTableMetadata` <<getTempView, finds a local temporary view>> and creates a link:spark-sql-CatalogTable.adoc#creating-instance[CatalogTable] (with `VIEW` link:spark-sql-CatalogTable.adoc#tableType[table type] and an undefined link:spark-sql-CatalogTable.adoc#storage[storage]) or <<getTableMetadata, retrieves the table metadata from an external catalog>>.

With the database name of the link:spark-sql-GlobalTempViewManager.adoc[GlobalTempViewManager], `getTempViewOrPermanentTableMetadata` requests <<globalTempViewManager, GlobalTempViewManager>> for the link:spark-sql-GlobalTempViewManager.adoc#get[global view definition] and creates a link:spark-sql-CatalogTable.adoc#creating-instance[CatalogTable] (with the link:spark-sql-GlobalTempViewManager.adoc#database[name] of `GlobalTempViewManager` in link:spark-sql-CatalogTable.adoc#identifier[table identifier], `VIEW` link:spark-sql-CatalogTable.adoc#tableType[table type] and an undefined link:spark-sql-CatalogTable.adoc#storage[storage]) or reports a `NoSuchTableException`.

With the database name not of `GlobalTempViewManager`, `getTempViewOrPermanentTableMetadata` simply <<getTableMetadata, retrieves the table metadata from an external catalog>>.

[NOTE]
====
`getTempViewOrPermanentTableMetadata` is used when:

1. `CatalogImpl` is requested for link:spark-sql-CatalogImpl.adoc#makeTable[converting TableIdentifier to Table], link:spark-sql-CatalogImpl.adoc#listColumns[listing the columns of a table (as Dataset)] and link:spark-sql-CatalogImpl.adoc#refreshTable[refreshing a table] (i.e. the analyzed logical plan of the table query and re-caching it)

1. `AlterTableAddColumnsCommand`, `CreateTableLikeCommand`, link:spark-sql-LogicalPlan-DescribeColumnCommand.adoc#run[DescribeColumnCommand], `ShowColumnsCommand` and `ShowTablesCommand` logical commands are executed
====

=== [[requireDbExists]] Reporting NoSuchDatabaseException When Specified Database Does Not Exist -- `requireDbExists` Internal Method

[source, scala]
----
requireDbExists(db: String): Unit
----

`requireDbExists` reports a `NoSuchDatabaseException` if the <<databaseExists, specified database does not exist>>. Otherwise, `requireDbExists` does nothing.
