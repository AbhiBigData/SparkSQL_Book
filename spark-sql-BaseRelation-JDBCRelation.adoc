== [[JDBCRelation]] JDBCRelation

`JDBCRelation` is a link:spark-sql-BaseRelation.adoc[BaseRelation] with support for <<InsertableRelation, InsertableRelation>> and <<PrunedFilteredScan, PrunedFilteredScan>>.

`JDBCRelation` is <<creating-instance, created>> when:

* `DataFrameReader` is requested to link:spark-sql-DataFrameReader.adoc#jdbc[load data from external table using JDBC] (with `predicates` for `WHERE` clause per partition)

* `JdbcRelationProvider` link:spark-sql-JdbcRelationProvider.adoc#createRelation-RelationProvider[creates a BaseRelation]

[[toString]]
`JDBCRelation` requests <<jdbcOptions, JDBCOptions>> for the name of the table and the <<parts, number of partitions>> (if defined).

```
JDBCRelation([table]) [numPartitions=[number]]
```

.JDBCRelation in web UI (Details for Query)
image::images/spark-sql-JDBCRelation-webui-query-details.png[align="center"]

```
scala> df.explain
== Physical Plan ==
*Scan JDBCRelation(projects) [numPartitions=1] [id#0,name#1,website#2] ReadSchema: struct<id:int,name:string,website:string>
```

[[sqlContext]]
`JDBCRelation` uses the <<sparkSession, SparkSession>> to return a link:spark-sql-SparkSession.adoc#sqlContext[SQLContext].

=== [[schema]] `schema` Attribute

[source, scala]
----
schema: StructType
----

NOTE: `schema` is part of link:spark-sql-BaseRelation.adoc#schema[BaseRelation Contract] to return the link:spark-sql-StructType.adoc[StructType] that describes the schema of tuples.

`schema`...FIXME

=== [[PrunedFilteredScan]] JDBCRelation as PrunedFilteredScan

`JDBCRelation` is a link:spark-sql-PrunedFilteredScan.adoc[PrunedFilteredScan].

.JDBCRelation as PrunedFilteredScan
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[buildScan]] `buildScan`
| FIXME
|===

=== [[InsertableRelation]] JDBCRelation as InsertableRelation

`JDBCRelation` is a `InsertableRelation`.

.JDBCRelation as InsertableRelation
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[insert]] `insert`
| FIXME
|===

=== [[columnPartition]] `columnPartition` Method

CAUTION: FIXME Is this still in use?

=== [[creating-instance]] Creating JDBCRelation Instance

`JDBCRelation` takes the following when created:

* [[parts]] RDD link:spark-rdd-partitions.adoc[partitions]
* [[jdbcOptions]] link:spark-sql-DataFrameReader.adoc#JDBCOptions[JDBCOptions]
* [[sparkSession]] link:spark-sql-SparkSession.adoc[SparkSession]

=== [[unhandledFilters]] `unhandledFilters` Method

[source, scala]
----
unhandledFilters(filters: Array[Filter]): Array[Filter]
----

NOTE: `unhandledFilters` is part of link:spark-sql-BaseRelation.adoc#unhandledFilters[BaseRelation Contract] to find unhandled link:spark-sql-Filter.adoc[Filters].

`unhandledFilters`...FIXME
