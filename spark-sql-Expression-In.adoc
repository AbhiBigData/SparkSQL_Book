== [[In]] In Predicate Expression

[[dataType]]
`In` is a predicate link:spark-sql-Expression.adoc[expression] (i.e. the result link:spark-sql-Expression.adoc#dataType[data type] is always link:spark-sql-DataType.adoc#BooleanType[boolean]).

`In` is <<creating-instance, created>> when:

* link:spark-sql-Column.adoc#isin[Column.isin] method is used

* `AstBuilder` is requested to link:spark-sql-AstBuilder.adoc#withPredicate[withPredicate]

* Catalyst DSL's link:spark-sql-catalyst-dsl.adoc#in[in] operator is used

* `ResolveSubquery` is requested to link:spark-sql-ResolveSubquery.adoc#resolveSubQueries[resolveSubQueries]

* `InConversion` is requested to link:spark-sql-InConversion.adoc#coerceTypes[coerceTypes]

`In` expression can be <<eval, evaluated>> to a boolean value (i.e. `true` or `false`) or the special value `null`.

[source, scala]
----
import org.apache.spark.sql.functions.lit
val value = lit(null)
val list = Seq(lit(1))
val in = (value isin (list: _*)).expr

scala> println(in.sql)
(NULL IN (1))

import org.apache.spark.sql.catalyst.InternalRow
val input = InternalRow(1, "hello")

// Case 1: value.eval(input) was null => null
val evaluatedValue = in.eval(input)
assert(evaluatedValue == null)

// Case 2: v = e.eval(input) && ordering.equiv(v, evaluatedValue) => true
val value = lit(1)
val list = Seq(lit(1))
val in = (value isin (list: _*)).expr
val evaluatedValue = in.eval(input)
assert(evaluatedValue.asInstanceOf[Boolean])

// Case 3: e.eval(input) = null and no ordering.equiv(v, evaluatedValue) => null
val value = lit(1)
val list = Seq(lit(null), lit(2))
val in = (value isin (list: _*)).expr
scala> println(in.sql)
(1 IN (NULL, 2))

val evaluatedValue = in.eval(input)
assert(evaluatedValue == null)

// Case 4: false
val value = lit(1)
val list = Seq(0, 2, 3).map(lit)
val in = (value isin (list: _*)).expr
scala> println(in.sql)
(1 IN (0, 2, 3))

val evaluatedValue = in.eval(input)
assert(evaluatedValue.asInstanceOf[Boolean] == false)
----

[[creating-instance]]
`In` takes the following when created:

* [[value]] Value link:spark-sql-Expression.adoc[expression]
* [[list]] List link:spark-sql-Expression.adoc[expressions]

NOTE: <<list, List expressions>> must not be `null` (but can have expressions that can be evaluated to `null`).

[[toString]]
`In` has the following text representation:

```
[value] IN [list]
```

[source, scala]
----
import org.apache.spark.sql.catalyst.expressions.{In, Literal}
import org.apache.spark.sql.{functions => f}
val in = In(value = Literal(1), list = Seq(f.array("1", "2", "3").expr))
scala> println(in)
1 IN (array('1, '2, '3))
----

[[sql]]
`In` has the following link:spark-sql-Expression.adoc#sql[SQL representation]:

```
([valueSQL] IN ([listSQL]))
```

[source, scala]
----
import org.apache.spark.sql.catalyst.expressions.{In, Literal}
import org.apache.spark.sql.{functions => f}
val in = In(value = Literal(1), list = Seq(f.array("1", "2", "3").expr))
scala> println(in.sql)
(1 IN (array(`1`, `2`, `3`)))
----

[[inSetConvertible]]
`In` expression is *inSetConvertible* when the <<list, list>> contains link:spark-sql-Expression-Literal.adoc[Literal] expressions only.

[source, scala]
----
// FIXME Example 1: inSetConvertible true

// FIXME Example 2: inSetConvertible false
----

[[internal-registries]]
.In's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[ordering]] `ordering`
| Scala's https://www.scala-lang.org/api/2.11.12/index.html#scala.math.Ordering[Ordering] instance that represents a strategy for sorting instances of a type.

Lazily-instantiated using `TypeUtils.getInterpretedOrdering` for the link:spark-sql-Expression.adoc#dataType[data type] of the <<value, value>> expression.

Used exclusively when `In` is requested to <<eval, evaluate a value>> for a given input row.
|===

=== [[checkInputDataTypes]] `checkInputDataTypes` Method

[source, scala]
----
checkInputDataTypes(): TypeCheckResult
----

NOTE: `checkInputDataTypes` is part of link:spark-sql-Expression.adoc#checkInputDataTypes[Expression Contract] to...FIXME.

`checkInputDataTypes`...FIXME

=== [[eval]] Evaluating Expression -- `eval` Method

[source, scala]
----
eval(input: InternalRow): Any
----

NOTE: `eval` is part of link:spark-sql-Expression.adoc#eval[Expression Contract] to evaluate a Catalyst expression to a JVM object for a given link:spark-sql-InternalRow.adoc[internal binary row].

`eval` requests <<value, value>> expression to link:spark-sql-Expression.adoc#eval[evaluate a value] for the `input` link:spark-sql-InternalRow.adoc[internal row].

If the evaluated value is `null`, `eval` gives `null` too.

`eval` takes every link:spark-sql-Expression.adoc[expression] in <<list, list>> expressions and requests them to evaluate a value for the `input` internal row. If any of the evaluated value is not `null` and equivalent in the <<ordering, ordering>>, `eval` returns `true`.

`eval` records whether any of the expressions in <<list, list>> expressions gave `null` value. If no <<list, list>> expression led to `true` (per <<ordering, ordering>>), `eval` returns `null` if any <<list, list>> expression evaluated to `null` or `false`.

=== [[doGenCode]] Generating Java Source Code For Code-Generated Expression Evaluation -- `doGenCode` Method

[source, scala]
----
doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode
----

NOTE: `doGenCode` is part of link:spark-sql-Expression.adoc#doGenCode[Expression Contract] to generate a Java source code for code-generated expression evaluation.

`doGenCode`...FIXME

[source, scala]
----
import org.apache.spark.sql.catalyst.expressions.Literal
import org.apache.spark.sql.{functions => f}

// Using Catalyst DSL
import org.apache.spark.sql.catalyst.dsl.expressions._
val in = Literal(1) in f.array("1", "2", "3").expr
scala> println(in)
1 IN (array('1, '2, '3))

import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext
val ctx = new CodegenContext

// FIXME Make it work
in.genCode(ctx)
----
