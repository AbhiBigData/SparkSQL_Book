== [[FileSourceScanExec]] FileSourceScanExec Leaf Physical Operator

`FileSourceScanExec` is a link:spark-sql-SparkPlan-DataSourceScanExec.adoc[DataSourceScanExec] (and so indirectly a link:spark-sql-SparkPlan.adoc#LeafExecNode[leaf physical operator]) that...FIXME

`FileSourceScanExec` is <<creating-instance, created>> exclusively when `FileSourceStrategy` execution planning strategy link:spark-sql-SparkStrategy-FileSourceStrategy.adoc#apply[plans a LogicalRelation logical operator with HadoopFsRelation].

[source, scala]
----
val q = spark.read.option("header", true).csv("../datasets/people.csv")
val logicalPlan = q.queryExecution.logical
scala> println(logicalPlan.numberedTreeString)
00 Relation[id#63,name#64,age#65] csv

import org.apache.spark.sql.execution.datasources.FileSourceStrategy
val sparkPlan = FileSourceStrategy(logicalPlan).head
scala> println(sparkPlan.numberedTreeString)
00 FileScan csv [id#63,name#64,age#65] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/jacek/dev/oss/datasets/people.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,name:string,age:string>

import org.apache.spark.sql.execution.FileSourceScanExec
val fileScanExec = sparkPlan.asInstanceOf[FileSourceScanExec]

// You could do the following instead
// as that'd find the scan down the plan
// if the query planning changes
val fileScanExec = sparkPlan.collect { case op: FileSourceScanExec => op }.head
----

`FileSourceScanExec` uses a `HashPartitioning` or an `UnknownPartitioning` <<outputPartitioning, output partitioning scheme>>.

`FileSourceScanExec` supports `ColumnarBatchScan`.

[[inputRDDs]]
`FileSourceScanExec` always gives the single <<inputRDD, inputRDD>> as the link:spark-sql-CodegenSupport.adoc#inputRDDs[only RDD that generates internal rows] (when `WholeStageCodegenExec` is link:spark-sql-SparkPlan-WholeStageCodegenExec.adoc#doExecute[executed]).

[[metrics]]
.FileSourceScanExec's Performance Metrics
[cols="1,2,2",options="header",width="100%"]
|===
| Key
| Name (in web UI)
| Description

| [[metadataTime]] `metadataTime`
| metadata time (ms)
|

| [[numFiles]] `numFiles`
| number of files
|

| [[numOutputRows]] `numOutputRows`
| number of output rows
|

| [[scanTime]] `scanTime`
| scan time
|
|===

As a link:spark-sql-SparkPlan-DataSourceScanExec.adoc[DataSourceScanExec], `FileSourceScanExec` uses *Scan* for the prefix of the link:spark-sql-SparkPlan-DataSourceScanExec.adoc#nodeName[node name].

[source, scala]
----
val fileScanExec: FileSourceScanExec = ... // see the example earlier
scala> println(fileScanExec.nodeName)
Scan csv
----

.FileSourceScanExec in web UI (Details for Query)
image::images/spark-sql-FileSourceScanExec-webui-query-details.png[align="center"]

[[nodeNamePrefix]]
`FileSourceScanExec` uses *File* for `nodeNamePrefix` (that is used for the link:spark-sql-SparkPlan-DataSourceScanExec.adoc#simpleString[simple node description] in query plans).

[source, scala]
----
val fileScanExec: FileSourceScanExec = ... // see the example earlier
scala> println(fileScanExec.nodeNamePrefix)
File

scala> println(fileScanExec.simpleString)
FileScan csv [id#20,name#21,city#22] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/jacek/dev/oss/datasets/people.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,name:string,city:string>
----

[[internal-registries]]
.FileSourceScanExec's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[inputRDD]] `inputRDD`
a| `RDD` of link:spark-sql-InternalRow.adoc[internal binary rows] (i.e. `InternalRow`)

Used when `FileSourceScanExec` is requested for <<inputRDDs, inputRDDs>> and <<doExecute, execution>>.

NOTE: `inputRDD` is a Scala lazy value which is computed once when it is first accessed and cached afterwards.

---

Internally, `inputRDD` requests <<relation, HadoopFsRelation>> to get the underlying link:spark-sql-BaseRelation-HadoopFsRelation.adoc#fileFormat[FileFormat] that is in turn requested to link:spark-sql-FileFormat.adoc#buildReaderWithPartitionValues[buildReaderWithPartitionValues] (with the input parameters from the properties of <<relation, HadoopFsRelation>> and <<pushedDownFilters, pushedDownFilters>>).

In case <<relation, HadoopFsRelation>> has link:spark-sql-BaseRelation-HadoopFsRelation.adoc#bucketSpec[bucketing specification] defined and link:spark-sql-bucketing.adoc#spark.sql.sources.bucketing.enabled[bucketing support is enabled], `inputRDD` <<createBucketedReadRDD, creates a FileScanRDD with bucketing>> (with the bucketing specification, the reader, <<selectedPartitions, selectedPartitions>> and the <<relation, HadoopFsRelation>> itself). Otherwise, `inputRDD` <<createNonBucketedReadRDD, createNonBucketedReadRDD>>.

NOTE: <<createBucketedReadRDD, createBucketedReadRDD>> accepts a bucketing specification while <<createNonBucketedReadRDD, createNonBucketedReadRDD>> does not.

| [[metadata]] `metadata`
a| Metadata (as a collection of key-value pairs)

NOTE: `metadata` is part of link:spark-sql-SparkPlan-DataSourceScanExec.adoc#metadata[DataSourceScanExec Contract] to..FIXME.

| [[needsUnsafeRowConversion]] `needsUnsafeRowConversion`
|

| [[pushedDownFilters]] `pushedDownFilters`
|

| [[supportsBatch]] `supportsBatch`
|
|===

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.sql.execution.FileSourceScanExec` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.FileSourceScanExec=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[vectorTypes]] `vectorTypes` Method

[source, scala]
----
vectorTypes: Option[Seq[String]]
----

NOTE: `vectorTypes` is part of link:spark-sql-ColumnarBatchScan.adoc#vectorTypes[ColumnarBatchScan Contract] to..FIXME.

`vectorTypes`...FIXME

=== [[doExecute]] Executing FileSourceScanExec -- `doExecute` Method

[source, scala]
----
doExecute(): RDD[InternalRow]
----

NOTE: `doExecute` is part of link:spark-sql-SparkPlan.adoc#doExecute[SparkPlan Contract] to describe a distributed computation that is a runtime representation of a structured query as an RDD of internal rows (aka _execute_).

`doExecute`...FIXME

=== [[doProduce]] Generating Java Source Code for Produce Path in Whole-Stage Code Generation -- `doProduce` Method

[source, scala]
----
doProduce(ctx: CodegenContext): String
----

NOTE: `doProduce` is part of link:spark-sql-CodegenSupport.adoc#doProduce[CodegenSupport Contract] to generate the Java source code for link:spark-sql-whole-stage-codegen.adoc#produce-path[produce path] in whole-stage code generation.

`doProduce`...FIXME

=== [[createNonBucketedReadRDD]] `createNonBucketedReadRDD` Internal Method

[source, scala]
----
createNonBucketedReadRDD(
  readFile: (PartitionedFile) => Iterator[InternalRow],
  selectedPartitions: Seq[PartitionDirectory],
  fsRelation: HadoopFsRelation): RDD[InternalRow]
----

`createNonBucketedReadRDD`...FIXME

NOTE: `createNonBucketedReadRDD` is used when...FIXME

=== [[selectedPartitions]] `selectedPartitions` Internal Lazy-Initialized Property

[source, scala]
----
selectedPartitions: Seq[PartitionDirectory]
----

`selectedPartitions`...FIXME

[NOTE]
====
`selectedPartitions` is used when `FileSourceScanExec` calculates:

* <<outputPartitioning, outputPartitioning>> and <<outputOrdering, outputOrdering>> when `spark.sql.sources.bucketing.enabled` Spark property is turned on (which is on by default) and the optional link:spark-sql-BaseRelation-HadoopFsRelation.adoc#bucketSpec[BucketSpec] for <<relation, HadoopFsRelation>> is defined
* <<metadata, metadata>>
* <<inputRDD, inputRDD>>
====

=== [[creating-instance]] Creating FileSourceScanExec Instance

`FileSourceScanExec` takes the following when created:

* [[relation]] link:spark-sql-BaseRelation-HadoopFsRelation.adoc[HadoopFsRelation]
* [[output]] Output schema link:spark-sql-Expression-Attribute.adoc[attributes]
* [[requiredSchema]] link:spark-sql-StructType.adoc[Schema]
* [[partitionFilters]] `partitionFilters` Catalyst link:spark-sql-Expression.adoc[expressions]
* [[dataFilters]] `dataFilters` Catalyst link:spark-sql-Expression.adoc[expressions]
* [[tableIdentifier]] Optional `TableIdentifier`

`FileSourceScanExec` initializes the <<internal-registries, internal registries and counters>>.

=== [[outputPartitioning]] Output Partitioning Scheme -- `outputPartitioning` Attribute

[source, scala]
----
outputPartitioning: Partitioning
----

NOTE: `outputPartitioning` is part of link:spark-sql-SparkPlan.adoc#outputPartitioning[SparkPlan Contract] to specify data partitioning.

`outputPartitioning` may give a link:spark-sql-SparkPlan-Partitioning.adoc#HashPartitioning[HashPartitioning] output partitioning when bucketing is enabled.

CAUTION: FIXME

=== [[createBucketedReadRDD]] Creating FileScanRDD with Bucketing -- `createBucketedReadRDD` Internal Method

[source, scala]
----
createBucketedReadRDD(
  bucketSpec: BucketSpec,
  readFile: (PartitionedFile) => Iterator[InternalRow],
  selectedPartitions: Seq[PartitionDirectory],
  fsRelation: HadoopFsRelation): RDD[InternalRow]
----

`createBucketedReadRDD`...FIXME

NOTE: `createBucketedReadRDD` is used exclusively when `FileSourceScanExec` is requested for <<inputRDD, inputRDD>> (for the very first time after which the result is cached).
