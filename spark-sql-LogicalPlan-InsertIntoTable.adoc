== [[InsertIntoTable]] InsertIntoTable Unary Logical Operator

`InsertIntoTable` is a unary link:spark-sql-LogicalPlan.adoc[logical operator] that is used for the following:

* <<INSERT_OVERWRITE_TABLE, INSERT OVERWRITE TABLE>> and <<INSERT_INTO_TABLE, INSERT INTO TABLE>> SQL commands

* `DataFrameWriter` is requested to link:spark-sql-DataFrameWriter.adoc#insertInto[insert the rows of a DataFrame into a table]

[source, scala]
----
// make sure that the tables are available in a catalog
sql("CREATE TABLE IF NOT EXISTS t1(id long)")
sql("CREATE TABLE IF NOT EXISTS t2(id long)")
val q = sql("INSERT INTO TABLE t2 SELECT * from t1 LIMIT 100")
val plan = q.queryExecution.logical
scala> println(plan.numberedTreeString)
00 'InsertIntoTable 'UnresolvedRelation `t2`, false, false
01 +- 'GlobalLimit 100
02    +- 'LocalLimit 100
03       +- 'Project [*]
04          +- 'UnresolvedRelation `t1`

// Dataset API's version of "INSERT OVERWRITE TABLE" in SQL
spark.range(10).write.mode("overwrite").insertInto("t2")
----

`InsertIntoTable` (with link:spark-sql-LogicalPlan-UnresolvedRelation.adoc[UnresolvedRelation] leaf operator) is <<creating-instance, created>> when:

* [[INSERT_INTO_TABLE]][[INSERT_OVERWRITE_TABLE]] `INSERT INTO (TABLE)` or `INSERT OVERWRITE TABLE` SQL queries are executed (as a link:spark-sql-AstBuilder.adoc#visitSingleInsertQuery[single insert] or a link:spark-sql-AstBuilder.adoc#visitMultiInsertQuery[multi-insert] query)

* `DataFrameWriter` is requested to link:spark-sql-DataFrameWriter.adoc#insertInto[insert a DataFrame into a table]

* `RelationConversions` logical evaluation rule is link:spark-sql-RelationConversions.adoc#apply[executed] (and transforms `InsertIntoTable` operators)

* `CreateHiveTableAsSelectCommand` logical command is <<spark-sql-LogicalPlan-CreateHiveTableAsSelectCommand.adoc#run, executed>>

[[output]]
`InsertIntoTable` has an empty output schema.

[[resolved]]
`InsertIntoTable` can never be link:spark-sql-LogicalPlan.adoc#resolved[resolved] (i.e. `InsertIntoTable` should not be part of a logical plan after analysis that is supposed to be <<logical-conversions, converted to logical commands>> eventually).

[[logical-conversions]]
`InsertIntoTable` is transformed to:

* link:spark-sql-LogicalPlan-InsertIntoHiveTable.adoc[InsertIntoHiveTable] logical command (when link:spark-sql-HiveAnalysis.adoc#apply[HiveAnalysis] resolution rule transforms `InsertIntoTable` with link:spark-sql-LogicalPlan-HiveTableRelation.adoc[HiveTableRelation])

* <<spark-sql-LogicalPlan-InsertIntoDataSourceCommand.adoc#, InsertIntoDataSourceCommand>> logical command (when link:spark-sql-DataSourceAnalysis.adoc[DataSourceAnalysis] posthoc logical resolution rule resolves an `InsertIntoTable` with a <<spark-sql-LogicalPlan-LogicalRelation.adoc#, LogicalRelation>> on a <<spark-sql-InsertableRelation.adoc#, InsertableRelation>>)

* link:spark-sql-LogicalPlan-InsertIntoHadoopFsRelationCommand.adoc[InsertIntoHadoopFsRelationCommand] logical command (when link:spark-sql-DataSourceAnalysis.adoc[DataSourceAnalysis] posthoc logical resolution rule transforms `InsertIntoTable` with `LogicalRelation` on `HadoopFsRelation`)

CAUTION: FIXME What's the difference between HiveAnalysis that converts `InsertIntoTable(r: HiveTableRelation...)` to `InsertIntoHiveTable` and `RelationConversions` that converts `InsertIntoTable(r: HiveTableRelation,...)` to `InsertIntoTable` (with `LogicalRelation`)?

[TIP]
====
Use `insertInto` operator from link:spark-sql-catalyst-dsl.adoc[Catalyst DSL] to create a `InsertIntoTable` logical operator, e.g. for testing or Spark SQL internals exploration.

[source, scala]
----
import org.apache.spark.sql.catalyst.dsl.plans._
val plan = table("a").insertInto(tableName = "t1", overwrite = true)
scala> println(plan.numberedTreeString)
00 'InsertIntoTable 'UnresolvedRelation `t1`, true, false
01 +- 'UnresolvedRelation `a`
----
====

=== [[creating-instance]] Creating InsertIntoTable Instance

`InsertIntoTable` takes the following when created:

* [[table]] link:spark-sql-LogicalPlan.adoc[Logical plan] representing a table
* [[partition]] Partitions (as a collection of partition keys and optional partition values for dynamic partition insert)
* [[query]] link:spark-sql-LogicalPlan.adoc[Logical plan] representing the data to be written
* [[overwrite]] `overwrite` flag that indicates whether to overwrite an existing table or partitions
* [[ifPartitionNotExists]] `ifPartitionNotExists` flag
