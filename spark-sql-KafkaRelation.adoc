== [[KafkaRelation]] KafkaRelation

`KafkaRelation` is a <<spark-sql-BaseRelation.adoc#, BaseRelation>> with a <<spark-sql-TableScan.adoc#, TableScan>>.

`KafkaRelation` is <<creating-instance, created>> exclusively when `KafkaSourceProvider` is requested to <<spark-sql-KafkaSourceProvider.adoc#createRelation-RelationProvider, create a BaseRelation>> (as a <<spark-sql-RelationProvider.adoc#createRelation, RelationProvider>>).

[[schema]]
`KafkaRelation` uses the fixed <<spark-sql-BaseRelation.adoc#schema, schema>>.

[[kafkaSchema]]
.KafkaRelation's Schema (in the positional order)
[cols="1m,2",options="header",width="100%"]
|===
| Field Name
| Data Type

| `key`
| `BinaryType`

| `value`
| `BinaryType`

| `topic`
| `StringType`

| `partition`
| `IntegerType`

| `offset`
| `LongType`

| `timestamp`
| `TimestampType`

| `timestampType`
| `IntegerType`
|===

[[toString]]
`KafkaRelation` uses the following human-readable text representation:

```
KafkaRelation(strategy=[strategy], start=[startingOffsets], end=[endingOffsets])
```

[[internal-registries]]
.KafkaRelation's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,2",options="header",width="100%"]
|===
| Name
| Description

| pollTimeoutMs
| [[pollTimeoutMs]] FIXME

Used when...FIXME
|===

[[logging]]
[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.sql.kafka010.KafkaRelation` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.kafka010.KafkaRelation=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating KafkaRelation Instance

`KafkaRelation` takes the following when created:

* [[sqlContext]] `SQLContext`
* [[strategy]] `ConsumerStrategy`
* [[sourceOptions]] Source options (as `Map[String, String]`)
* [[specifiedKafkaParams]] User-defined Kafka parameters (as `Map[String, String]`)
* [[failOnDataLoss]] `failOnDataLoss` flag
* [[startingOffsets]] Starting offsets (as `KafkaOffsetRangeLimit`)
* [[endingOffsets]] Ending offsets (as `KafkaOffsetRangeLimit`)

`KafkaRelation` initializes the <<internal-registries, internal registries and counters>>.

=== [[buildScan]] Building Distributed Data Scan with Column Pruning -- `buildScan` Method

[source, scala]
----
buildScan(): RDD[Row]
----

NOTE: `buildScan` is part of <<spark-sql-TableScan.adoc#buildScan, TableScan Contract>> to build a distributed data scan with column pruning.

`buildScan` <<spark-sql-KafkaSourceProvider.adoc#kafkaParamsForDriver, kafkaParamsForDriver>> from the <<specifiedKafkaParams, user-defined Kafka parameters>> and uses it to create a <<spark-sql-KafkaOffsetReader.adoc#creating-instance, KafkaOffsetReader>> (together with the <<strategy, ConsumerStrategy>>, the <<sourceOptions, source options>> and a unique group ID of the format `spark-kafka-relation-[randomUUID]-driver`).

`buildScan` then uses the `KafkaOffsetReader` to <<getPartitionOffsets, getPartitionOffsets>> for the starting and ending offsets and <<spark-sql-KafkaOffsetReader.adoc#close, closes>> it right after.

`buildScan` creates a `KafkaSourceRDDOffsetRange` for every pair of the starting and ending offsets.

`buildScan` prints out the following INFO message to the logs:

```
GetBatch generating RDD of offset range: [comma-separated offsetRanges]
```

`buildScan` then <<spark-sql-KafkaSourceProvider.adoc#kafkaParamsForExecutors, kafkaParamsForExecutors>> and uses it to create a `KafkaSourceRDD` (with the <<pollTimeoutMs, pollTimeoutMs>>) and maps over all the elements (using `RDD.map` operator that creates a `MapPartitionsRDD`).

TIP: Use `RDD.toDebugString` to see the two RDDs, i.e. `KafkaSourceRDD` and `MapPartitionsRDD`, in the RDD lineage.

In the end, `buildScan` requests the <<sqlContext, SQLContext>> to <<spark-sql-SparkSession.adoc#internalCreateDataFrame, create a DataFrame>> from the `KafkaSourceRDD` and the <<schema, schema>>.

`buildScan` throws an `IllegalStateException` when the topic partitions for starting offsets are different from the ending offsets topics:

```
different topic partitions for starting offsets topics[[fromTopics]] and ending offsets topics[[untilTopics]]
```

=== [[getPartitionOffsets]] `getPartitionOffsets` Internal Method

[source, scala]
----
getPartitionOffsets(
  kafkaReader: KafkaOffsetReader,
  kafkaOffsets: KafkaOffsetRangeLimit): Map[TopicPartition, Long]
----

`getPartitionOffsets`...FIXME

NOTE: `getPartitionOffsets` is used exclusively when `KafkaRelation` is requested to <<buildScan, buildScan>>.
