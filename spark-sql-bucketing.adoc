== Bucketing

*Bucketing* is an optimization technique in Spark SQL that uses *buckets* and *bucketing columns* to determine data partitioning.

Bucketing is used exclusively in link:spark-sql-SparkPlan-FileSourceScanExec.adoc[FileSourceScanExec] physical operator (when it is requested for the link:spark-sql-SparkPlan-FileSourceScanExec.adoc#inputRDD[input RDD] and to determine link:spark-sql-SparkPlan-FileSourceScanExec.adoc#outputPartitioning[output partitioning] and link:spark-sql-SparkPlan-FileSourceScanExec.adoc#outputOrdering[output ordering]).

The motivation is to optimize performance of a join query by avoiding shuffles (aka _exchanges_) of tables participating in the join. Bucketing results in fewer exchanges (and so stages).

NOTE: Bucketing can show the biggest benefit when *pre-shuffled bucketed tables* are used more than once as bucketing itself takes time (that you will offset executing multiple join queries later).

Bucketing is enabled by default. Spark SQL uses <<spark.sql.sources.bucketing.enabled, spark.sql.sources.bucketing.enabled>> configuration property to control whether the bucketing should be enabled and used for query optimization or not.

[source, scala]
----
// Example: SortMergeJoin of two FileScans

// Make sure that bucketing is on
assert(spark.sessionState.conf.bucketingEnabled, "Bucketing disabled?!")

// Make sure that you don't end up with a BroadcastHashJoin and a BroadcastExchange
// Disable auto broadcasting
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)

scala> spark.catalog.listTables.show
+-----+--------+-----------+---------+-----------+
| name|database|description|tableType|isTemporary|
+-----+--------+-----------+---------+-----------+
|a10e6| default|       null|  MANAGED|      false|
|a10e8| default|       null|  MANAGED|      false|
+-----+--------+-----------+---------+-----------+

val t1 = spark.table("a10e6")
val t2 = spark.table("a10e8")

// trigger execution of the join query
t1.join(t2, "id").foreach(_ => ())
----

The above join query is a fine example of a link:spark-sql-SparkPlan-SortMergeJoinExec.adoc[SortMergeJoinExec] (aka _SortMergeJoin_) of two link:spark-sql-SparkPlan-FileSourceScanExec.adoc[FileSourceScanExecs] (aka _Scan_). The join query uses link:spark-sql-SparkPlan-ShuffleExchangeExec.adoc[ShuffleExchangeExec] physical operators (aka _Exchange_) to shuffle the table datasets for the SortMergeJoin.

.SortMergeJoin of FileScans (Details for Query)
image::images/spark-sql-bucketing-sortmergejoin-filescans.png[align="center"]

One way to avoid the exchanges and so optimize the join query is to use table bucketing that is applicable for all file-based data sources, e.g. Parquet, ORC, JSON, CSV, that are saved as a table using link:spark-sql-DataFrameWriter.adoc#saveAsTable[DataFrameWrite.saveAsTable] or simply available in a link:spark-sql-Catalog.adoc[catalog] by link:spark-sql-SparkSession.adoc#table[SparkSession.table].

NOTE: Bucketing link:spark-sql-DataFrameWriter.adoc#assertNotBucketed[is not supported] for link:spark-sql-DataFrameWriter.adoc#save[DataFrameWriter.save], link:spark-sql-DataFrameWriter.adoc#insertInto[DataFrameWriter.insertInto] and link:spark-sql-DataFrameWriter.adoc#jdbc[DataFrameWriter.jdbc] methods.

You use link:spark-sql-DataFrameWriter.adoc#bucketBy[DataFrameWriter.bucketBy] method to specify the number of buckets and the bucketing columns.

You can optionally sort the output rows in buckets using link:spark-sql-DataFrameWriter.adoc#sortBy[DataFrameWriter.sortBy] method.

[source, scala]
----
people.write
  .bucketBy(42, "name")
  .sortBy("age")
  .saveAsTable("people_bucketed")
----

NOTE: link:spark-sql-DataFrameWriter.adoc#bucketBy[DataFrameWriter.bucketBy] and link:spark-sql-DataFrameWriter.adoc#sortBy[DataFrameWriter.sortBy] simply set respective internal properties that eventually become a link:spark-sql-BucketSpec.adoc[bucketing specification].

Unlike bucketing in Apache Hive, Spark SQL creates the bucket files per the number of buckets and partitions. In other words, the number of bucketing files is the number of buckets multiplied by the number of task writers (one per partition).

```
val large = spark.range(100000000)

large.write
  .bucketBy(4, "id")
  .sortBy("id")
  .mode("overwrite")
  .saveAsTable("bucketed_4_id")

scala> println(large.queryExecution.toRdd.getNumPartitions)
8

// That gives 8 (partitions/task writers) x 8 (buckets) = 64 files
// With _SUCCESS extra file and the ls -l header "total 794624" that gives 66 files
$ ls -tlr spark-warehouse/buckets_8_10e8 | wc -l
      66
```

After you bucketed the table data sets, you should notice that the Exchanges are no longer needed (as the tables are already pre-shuffled).

[source, scala]
----
// Note that the example uses the bucketed tables
val t1 = spark.table("buckets_8_10e6")
val t2 = spark.table("buckets_8_10e8")

// trigger execution of the join query
t1.join(t2, "id").foreach(_ => ())
----

The above join query of the bucketed tables shows no link:spark-sql-SparkPlan-ShuffleExchangeExec.adoc[ShuffleExchangeExec] physical operators (aka _Exchange_) as the shuffling has already been executed (before the query was run).

.SortMergeJoin of Bucketed Tables (Details for Query)
image::images/spark-sql-bucketing-sortmergejoin-bucketed-tables-no-exchanges.png[align="center"]

The number of partitions of a bucketed table is exactly the number of buckets.

[source, scala]
----
// FIXME Show the number of buckets
val table = spark.table("bucketed_table")
val rdd = table.queryExecution.toRdd
println(rdd.partitions.length)
----

Use link:spark-sql-SessionCatalog.adoc#getTableMetadata[SessionCatalog] or `DESCRIBE EXTENDED` SQL command to find the bucket information.

[source, scala]
----
scala> spark.catalog.listTables.show(false)
+---------------------+--------+-----------+---------+-----------+
|name                 |database|description|tableType|isTemporary|
+---------------------+--------+-----------+---------+-----------+
|buckets_8_10e6_sorted|default |null       |MANAGED  |false      |
+---------------------+--------+-----------+---------+-----------+

// DESC EXTENDED or DESC FORMATTED would also work
scala> sql("DESCRIBE EXTENDED buckets_8_10e6_sorted").show(numRows = 21, truncate = false)
+----------------------------+---------------------------------------------------------------------+-------+
|col_name                    |data_type                                                            |comment|
+----------------------------+---------------------------------------------------------------------+-------+
|id                          |bigint                                                               |null   |
|                            |                                                                     |       |
|# Detailed Table Information|                                                                     |       |
|Database                    |default                                                              |       |
|Table                       |buckets_8_10e6_sorted                                                |       |
|Owner                       |jacek                                                                |       |
|Created Time                |Tue Apr 17 18:42:37 CEST 2018                                        |       |
|Last Access                 |Thu Jan 01 01:00:00 CET 1970                                         |       |
|Created By                  |Spark 2.3.1-SNAPSHOT                                                 |       |
|Type                        |MANAGED                                                              |       |
|Provider                    |parquet                                                              |       |
|Num Buckets                 |8                                                                    |       |
|Bucket Columns              |[`id`]                                                               |       |
|Sort Columns                |[`id`]                                                               |       |
|Table Properties            |[transient_lastDdlTime=1523983357]                                   |       |
|Statistics                  |4055953 bytes                                                        |       |
|Location                    |file:/Users/jacek/dev/oss/spark/spark-warehouse/buckets_8_10e6_sorted|       |
|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                   |       |
|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                     |       |
|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat            |       |
|Storage Properties          |[serialization.format=1]                                             |       |
+----------------------------+---------------------------------------------------------------------+-------+

import org.apache.spark.sql.catalyst.TableIdentifier
val metadata = spark.sessionState.catalog.getTableMetadata(TableIdentifier("buckets_8_10e6_sorted"))
scala> metadata.bucketSpec.foreach(println)
8 buckets, bucket columns: [id], sort columns: [id]
----

The link:spark-sql-BucketSpec.adoc#numBuckets[number of buckets] has to be between `0` and `100000` exclusive or Spark SQL reports an `AnalysisException`:

```
Number of buckets should be greater than 0 but less than 100000. Got `[numBuckets]`
```

There are however requirements that have to be met before link:spark-sql-SparkOptimizer.adoc[Spark Optimizer] gives a no-Exchange query plan:

. The number of partitions on both sides of a join has to be exactly the same.

. Both join operators have to use link:spark-sql-SparkPlan-Partitioning.adoc#HashPartitioning[HashPartitioning] partitioning scheme.

It is acceptable to use bucketing for one side of a join.

[source, scala]
----
// Make sure that you don't end up with a BroadcastHashJoin and a BroadcastExchange
// Disable auto broadcasting
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)

val bucketedTableName = "bucketed_4_id"
val large = spark.range(1000000)
large.write
  .bucketBy(4, "id")
  .sortBy("id")
  .mode("overwrite")
  .saveAsTable(bucketedTableName)
val bucketedTable = spark.table(bucketedTableName)

val t1 = spark
  .range(4)
  .repartition(4, $"id")  // Make sure that the number of partitions matches the other side

val q = t1.join(bucketedTable, "id")
scala> q.explain
== Physical Plan ==
*(4) Project [id#104L]
+- *(4) SortMergeJoin [id#104L], [id#102L], Inner
   :- *(2) Sort [id#104L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(id#104L, 4)
   :     +- *(1) Range (0, 4, step=1, splits=8)
   +- *(3) Sort [id#102L ASC NULLS FIRST], false, 0
      +- *(3) Project [id#102L]
         +- *(3) Filter isnotnull(id#102L)
            +- *(3) FileScan parquet default.bucketed_4_id[id#102L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>

q.foreach(_ => ())
----

.SortMergeJoin of One Bucketed Table (Details for Query)
image::images/spark-sql-bucketing-sortmergejoin-one-bucketed-table.png[align="center"]

=== Sorting

[source, scala]
----
// Make sure that you don't end up with a BroadcastHashJoin and a BroadcastExchange
// Disable auto broadcasting
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)

val bucketedTableName = "bucketed_4_id"
val large = spark.range(1000000)
large.write
  .bucketBy(4, "id")
  .sortBy("id")
  .mode("overwrite")
  .saveAsTable(bucketedTableName)

// Describe the table and include bucketing spec only
val descSQL = sql(s"DESC FORMATTED $bucketedTableName").filter($"col_name".contains("Bucket") || $"col_name" === "Sort Columns")
scala> descSQL.show(truncate = false)
+--------------+---------+-------+
|col_name      |data_type|comment|
+--------------+---------+-------+
|Num Buckets   |4        |       |
|Bucket Columns|[`id`]   |       |
|Sort Columns  |[`id`]   |       |
+--------------+---------+-------+

val bucketedTable = spark.table(bucketedTableName)
val t1 = spark.range(4)
  .repartition(2, $"id")  // Use just 2 partitions
  .sortWithinPartitions("id") // sort partitions

val q = t1.join(bucketedTable, "id")
// Note two exchanges and sorts
scala> q.explain
== Physical Plan ==
*(5) Project [id#79L]
+- *(5) SortMergeJoin [id#79L], [id#77L], Inner
   :- *(3) Sort [id#79L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(id#79L, 4)
   :     +- *(2) Sort [id#79L ASC NULLS FIRST], false, 0
   :        +- Exchange hashpartitioning(id#79L, 2)
   :           +- *(1) Range (0, 4, step=1, splits=8)
   +- *(4) Sort [id#77L ASC NULLS FIRST], false, 0
      +- *(4) Project [id#77L]
         +- *(4) Filter isnotnull(id#77L)
            +- *(4) FileScan parquet default.bucketed_4_id[id#77L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/jacek/dev/oss/spark/spark-warehouse/bucketed_4_id], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>

q.foreach(_ => ())
----

WARNING: There are two exchanges and sorts which makes the above use case almost unusable. I filed an issue at https://issues.apache.org/jira/browse/SPARK-24025[SPARK-24025 Join of bucketed and non-bucketed tables can give two exchanges and sorts for non-bucketed side].

.SortMergeJoin of Sorted Dataset and Bucketed Table (Details for Query)
image::images/spark-sql-bucketing-sortmergejoin-sorted-dataset-and-bucketed-table.png[align="center"]

=== [[spark.sql.sources.bucketing.enabled]] spark.sql.sources.bucketing.enabled Spark SQL Configuration Property

Bucketing is enabled when link:spark-sql-properties.adoc#spark.sql.sources.bucketing.enabled[spark.sql.sources.bucketing.enabled] configuration property is turned on, i.e. `true`.

NOTE: link:spark-sql-properties.adoc#spark.sql.sources.bucketing.enabled[spark.sql.sources.bucketing.enabled] configuration property is turned on, i.e. `true`, by default.

TIP: Use link:spark-sql-SQLConf.adoc#bucketingEnabled[SQLConf.bucketingEnabled] to access the current value of `spark.sql.sources.bucketing.enabled` property.

[source, scala]
----
// Bucketing is on by default
val sqlConf = spark.sessionState.conf
scala> println(sqlConf.bucketingEnabled)
true
----
