== [[AstBuilder]] AstBuilder -- ANTLR-based SQL Parser

`AstBuilder` converts a SQL statement into Spark SQL's corresponding entity (i.e. link:spark-sql-DataType.adoc[DataType], link:spark-sql-Expression.adoc[Expression], link:spark-sql-LogicalPlan.adoc[LogicalPlan] or `TableIdentifier`) using <<visit-callbacks, visit callback methods>>.

`AstBuilder` is the link:spark-sql-AbstractSqlParser.adoc#astBuilder[AST builder] of `AbstractSqlParser` (i.e. the base SQL parsing infrastructure in Spark SQL).

[TIP]
====
Spark SQL supports SQL statements as described in https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4[SqlBase.g4]. Using the file can tell you (almost) exactly what Spark SQL supports at any given time.

_"Almost"_ being that although the grammar accepts a SQL statement it can be reported as not allowed by `AstBuilder`, e.g.

```
scala> sql("EXPLAIN FORMATTED SELECT * FROM myTable").show
org.apache.spark.sql.catalyst.parser.ParseException:
Operation not allowed: EXPLAIN FORMATTED(line 1, pos 0)

== SQL ==
EXPLAIN FORMATTED SELECT * FROM myTable
^^^

  at org.apache.spark.sql.catalyst.parser.ParserUtils$.operationNotAllowed(ParserUtils.scala:39)
  at org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitExplain$1.apply(SparkSqlParser.scala:275)
  at org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitExplain$1.apply(SparkSqlParser.scala:273)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:93)
  at org.apache.spark.sql.execution.SparkSqlAstBuilder.visitExplain(SparkSqlParser.scala:273)
  at org.apache.spark.sql.execution.SparkSqlAstBuilder.visitExplain(SparkSqlParser.scala:53)
  at org.apache.spark.sql.catalyst.parser.SqlBaseParser$ExplainContext.accept(SqlBaseParser.java:480)
  at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:42)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:66)
  at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:66)
  at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:93)
  at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:65)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:62)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:61)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:90)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:46)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:61)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  ... 48 elided
```
====

`AstBuilder` is a ANTLR `AbstractParseTreeVisitor` (as `SqlBaseBaseVisitor`) that is generated from https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4[SqlBase.g4] ANTLR grammar for Spark SQL.

[NOTE]
====
`SqlBaseBaseVisitor` is a ANTLR-specific base class that is auto-generated at build time from a ANTLR grammar in `SqlBase.g4`.

`SqlBaseBaseVisitor` is an ANTLR http://www.antlr.org/api/Java/org/antlr/v4/runtime/tree/AbstractParseTreeVisitor.html[AbstractParseTreeVisitor].
====

[[visit-callbacks]]
.AstBuilder's Visit Callback Methods
[cols="1,1,3",options="header",width="100%"]
|===
| Callback Method
| ANTLR rule / labeled alternative
| Spark SQL Entity

| [[visitDereference]] `visitDereference`
|
|

| [[visitExists]] `visitExists`
| `#exists` labeled alternative
| link:spark-sql-Expression-Exists.adoc[Exists] expression

| [[visitExplain]] `visitExplain`
| `explain`
a| link:spark-sql-LogicalPlan-ExplainCommand.adoc[ExplainCommand]

[NOTE]
====
Can be a `OneRowRelation` for an `EXPLAIN` for an unexplainable link:spark-sql-LogicalPlan-DescribeTableCommand.adoc[DescribeTableCommand] logical command as created from <<visitDescribeTable, DESCRIBE TABLE>> SQL statement.

```
val q = sql("EXPLAIN DESCRIBE TABLE t")
scala> println(q.queryExecution.logical.numberedTreeString)
scala> println(q.queryExecution.logical.numberedTreeString)
00 ExplainCommand OneRowRelation$, false, false, false
```
====

| [[visitFromClause]] `visitFromClause`
| `fromClause`
a| link:spark-sql-LogicalPlan.adoc[LogicalPlan]

Supports multiple comma-separated relations (that all together build a condition-less INNER JOIN) with optional link:spark-sql-Expression-Generator.adoc#lateral-view[LATERAL VIEW].

A relation can be one of the following or a combination thereof:

* Table identifier
* Inline table using `VALUES exprs AS tableIdent`
* Table-valued function (currently only `range` is supported)

| [[visitFunctionCall]] `visitFunctionCall`
| `functionCall` labeled alternative
a|

* link:spark-sql-Expression-UnresolvedFunction.adoc[UnresolvedFunction] for a bare function (with no window specification)
* [[visitFunctionCall-UnresolvedWindowExpression]] link:spark-sql-Expression-WindowExpression.adoc#UnresolvedWindowExpression[UnresolvedWindowExpression] for a function evaluated in a windowed context with a `WindowSpecReference`
* link:spark-sql-Expression-WindowExpression.adoc[WindowExpression] for a function over a window

TIP: See the <<function-examples, function examples>> below.

| [[visitMultiInsertQuery]] `visitMultiInsertQuery`
| `multiInsertQueryBody`
a| A logical operator with a link:spark-sql-LogicalPlan-InsertIntoTable.adoc[InsertIntoTable] (and link:spark-sql-LogicalPlan-UnresolvedRelation.adoc[UnresolvedRelation] leaf operator)

```
FROM relation (',' relation)* lateralView*
INSERT OVERWRITE TABLE ...

FROM relation (',' relation)* lateralView*
INSERT INTO TABLE? ...
```

| [[visitNamedExpression]] `visitNamedExpression`
| `namedExpression`
a|

* `Alias` (for a single alias)
* `MultiAlias` (for a parenthesis enclosed alias list
* a bare link:spark-sql-Expression.adoc[Expression]

| [[visitQuerySpecification]] `visitQuerySpecification`
| `querySpecification`
a| `OneRowRelation` or link:spark-sql-LogicalPlan.adoc[LogicalPlan]

[NOTE]
====
`visitQuerySpecification` creates a `OneRowRelation` for a `SELECT` without a `FROM` clause.

```
val q = sql("select 1")
scala> println(q.queryExecution.logical.numberedTreeString)
00 'Project [unresolvedalias(1, None)]
01 +- OneRowRelation$
```
====

| [[visitPredicated]] `visitPredicated`
| `predicated`
| link:spark-sql-Expression.adoc[Expression]

| [[visitRelation]] `visitRelation`
| `relation`
| link:spark-sql-LogicalPlan.adoc[LogicalPlan] for a `FROM` clause.

| [[visitSingleDataType]] `visitSingleDataType`
| `singleDataType`
| link:spark-sql-DataType.adoc[DataType]

| [[visitSingleExpression]] `visitSingleExpression`
| `singleExpression`
| link:spark-sql-Expression.adoc[Expression]

Takes the named expression and relays to <<visitNamedExpression, visitNamedExpression>>

| [[visitSingleInsertQuery]] `visitSingleInsertQuery`
| `#singleInsertQuery` labeled alternative
a| A logical operator with a link:spark-sql-LogicalPlan-InsertIntoTable.adoc[InsertIntoTable]

```
INSERT INTO TABLE? tableIdentifier partitionSpec? #insertIntoTable

INSERT OVERWRITE TABLE tableIdentifier (partitionSpec (IF NOT EXISTS)?)? #insertOverwriteTable
```

| [[visitSingleStatement]] `visitSingleStatement`
| `singleStatement`
a| link:spark-sql-LogicalPlan.adoc[LogicalPlan] from a single statement

NOTE: A single statement can be quite involved.

| [[visitSingleTableIdentifier]] `visitSingleTableIdentifier`
| `singleTableIdentifier`
| `TableIdentifier`

| [[visitStar]] `visitStar`
| `#star` labeled alternative
| link:spark-sql-Expression-UnresolvedStar.adoc[UnresolvedStar]

| [[visitSubqueryExpression]] `visitSubqueryExpression`
| `#subqueryExpression` labeled alternative
| link:spark-sql-Expression-ScalarSubquery.adoc[ScalarSubquery]

| [[visitWindowDef]] `visitWindowDef`
| `windowDef` labeled alternative
a| link:spark-sql-Expression-WindowSpecDefinition.adoc[WindowSpecDefinition]

```
// CLUSTER BY with window frame
'(' CLUSTER BY partition+=expression (',' partition+=expression)*) windowFrame? ')'

// PARTITION BY and ORDER BY with window frame
'(' ((PARTITION \| DISTRIBUTE) BY partition+=expression (',' partition+=expression)*)?
  ((ORDER \| SORT) BY sortItem (',' sortItem)*)?)
  windowFrame? ')'
```
|===

[[with-methods]]
.AstBuilder's Parsing Handlers
[cols="1,3",options="header",width="100%"]
|===
| Parsing Handler
| LogicalPlan Added

| [[withAggregation]] `withAggregation`
a|

* link:spark-sql-LogicalPlan-GroupingSets.adoc[GroupingSets] for `GROUP BY &hellip; GROUPING SETS (&hellip;)`

* link:spark-sql-LogicalPlan-Aggregate.adoc[Aggregate] for `GROUP BY &hellip; (WITH CUBE \| WITH ROLLUP)?`

| [[withGenerate]] `withGenerate`
| link:spark-sql-Expression-Generator.adoc[Generate] with a link:spark-sql-Expression-UnresolvedGenerator.adoc[UnresolvedGenerator] and link:spark-sql-LogicalPlan-Generate.adoc#join[join] flag turned on for `LATERAL VIEW` (in `SELECT` or `FROM` clauses).

| [[withHints]] `withHints`
a| link:spark-sql-LogicalPlan-Hint.adoc[Hint] for `/*+ hint */` in `SELECT` queries.

TIP: Note `+` (plus) between `/\*` and `*/`

`hint` is of the format `name` or `name (param1, param2, ...)`.

```
/*+ BROADCAST (table) */
```

| [[withInsertInto]] `withInsertInto`
a|

* link:spark-sql-LogicalPlan-InsertIntoTable.adoc[InsertIntoTable] for <<visitSingleInsertQuery, visitSingleInsertQuery>> or <<visitMultiInsertQuery, visitMultiInsertQuery>>

* `InsertIntoDir` for...FIXME

| [[withJoinRelations]] `withJoinRelations`
a| link:spark-sql-LogicalPlan-Join.adoc[Join] for a <<visitFromClause, FROM clause>> and <<visitRelation, relation>> alone.

The following join types are supported:

* `INNER` (default)
* `CROSS`
* `LEFT` (with optional `OUTER`)
* `LEFT SEMI`
* `RIGHT` (with optional `OUTER`)
* `FULL` (with optional `OUTER`)
* `ANTI` (optionally prefixed with `LEFT`)

The following join criteria are supported:

* `ON booleanExpression`
* `USING '(' identifier (',' identifier)* ')'`

Joins can be `NATURAL` (with no join criteria).

| [[withQueryResultClauses]] `withQueryResultClauses`
|

| [[withQuerySpecification]] `withQuerySpecification`
a| Adds a query specification to a logical operator.

For transform `SELECT` (with `TRANSFORM`, `MAP` or `REDUCE` qualifiers), `withQuerySpecification` does...FIXME

---

For regular `SELECT` (no `TRANSFORM`, `MAP` or `REDUCE` qualifiers), `withQuerySpecification` adds (in that order):

1. <<withGenerate, Generate>> unary logical operators (if used in the parsed SQL text)

1. `Filter` unary logical plan (if used in the parsed SQL text)

1. <<withAggregation, GroupingSets or Aggregate>> unary logical operators (if used in the parsed SQL text)

1. `Project` and/or `Filter` unary logical operators

1. <<withWindows, WithWindowDefinition>> unary logical operator (if used in the parsed SQL text)

1. <<withHints, UnresolvedHint>> unary logical operator (if used in the parsed SQL text)

| [[withPredicate]] `withPredicate`
a|
* `NOT? IN '(' query ')'` gives an link:spark-sql-Expression-In.adoc[In] predicate expression with link:spark-sql-Expression-ListQuery.adoc[ListQuery] expression

* `NOT? IN '(' expression (',' expression)* ')'` gives an link:spark-sql-Expression-In.adoc[In] predicate expression

| [[withWindows]] `withWindows`
a| link:spark-sql-LogicalPlan-WithWindowDefinition.adoc[WithWindowDefinition] for link:spark-sql-functions-windows.adoc[window aggregates] (given `WINDOW` definitions).

Used for <<withQueryResultClauses, withQueryResultClauses>> and <<withQuerySpecification, withQuerySpecification>> with `windows` definition.

```
WINDOW identifier AS windowSpec
  (',' identifier AS windowSpec)*
```

TIP: Consult `windows`, `namedWindow`, `windowSpec`, `windowFrame`, and `frameBound` (with `windowRef` and `windowDef`) ANTLR parsing rules for Spark SQL in link:++https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4#L629++[SqlBase.g4].
|===

NOTE: `AstBuilder` belongs to `org.apache.spark.sql.catalyst.parser` package.

=== [[function-examples]] Function Examples

The examples are handled by <<visitFunctionCall, visitFunctionCall>>.

[source, scala]
----
import spark.sessionState.sqlParser

scala> sqlParser.parseExpression("foo()")
res0: org.apache.spark.sql.catalyst.expressions.Expression = 'foo()

scala> sqlParser.parseExpression("foo() OVER windowSpecRef")
res1: org.apache.spark.sql.catalyst.expressions.Expression = unresolvedwindowexpression('foo(), WindowSpecReference(windowSpecRef))

scala> sqlParser.parseExpression("foo() OVER (CLUSTER BY field)")
res2: org.apache.spark.sql.catalyst.expressions.Expression = 'foo() windowspecdefinition('field, UnspecifiedFrame)
----
