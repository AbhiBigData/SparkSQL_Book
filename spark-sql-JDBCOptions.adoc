== [[JDBCOptions]] JDBCOptions -- JDBC Data Source Options

`JDBCOptions` represents the <<options, options>> of the <<spark-sql-jdbc.adoc#, JDBC data source>>.

[[options]]
.Options for JDBC Data Source
[cols="1m,1,2",options="header",width="100%",separator="!"]
|===
! Option / Key
! Default Value
! Description

! batchsize
! `1000`
! [[batchsize]]

The minimum value is `1`

Used exclusively when `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-CreatableRelationProvider, create a BaseRelation>> (as a <<spark-sql-CreatableRelationProvider.adoc#createRelation, CreatableRelationProvider>>) through `JdbcUtils` helper object and its <<spark-sql-JdbcUtils.adoc#saveTable, saveTable>>.

! createTableColumnTypes
!
! [[createTableColumnTypes]]

Used exclusively when `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-CreatableRelationProvider, create a BaseRelation>> (as a <<spark-sql-CreatableRelationProvider.adoc#createRelation, CreatableRelationProvider>>) through `JdbcUtils` helper object and its <<spark-sql-JdbcUtils.adoc#createTable, createTable>>.

! `createTableOptions`
! Empty string
! [[createTableOptions]]

Used exclusively when `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-CreatableRelationProvider, create a BaseRelation>> (as a <<spark-sql-CreatableRelationProvider.adoc#createRelation, CreatableRelationProvider>>) through `JdbcUtils` helper object and its <<spark-sql-JdbcUtils.adoc#createTable, createTable>>.

! `customSchema`
! (undefined)
a! [[customSchema]] Specifies the custom data types of the read schema (that is used at link:spark-sql-DataFrameReader.adoc#jdbc[load time])

`customSchema` is a comma-separated list of field definitions with column names and their link:spark-sql-DataType.adoc[data types] in a canonical SQL representation, e.g. `id DECIMAL(38, 0), name STRING`.

`customSchema` defines the data types of the columns that will override the data types inferred from the table schema and follows the following pattern:

```
colTypeList
    : colType (',' colType)*
    ;

colType
    : identifier dataType (COMMENT STRING)?
    ;

dataType
    : complex=ARRAY '<' dataType '>'                            #complexDataType
    | complex=MAP '<' dataType ',' dataType '>'                 #complexDataType
    | complex=STRUCT ('<' complexColTypeList? '>' | NEQ)        #complexDataType
    | identifier ('(' INTEGER_VALUE (',' INTEGER_VALUE)* ')')?  #primitiveDataType
    ;
```

Used exclusively when `JDBCRelation` is requested for the <<spark-sql-JDBCRelation.adoc#schema, schema>>.

! `dbtable`
!
a! [[dbtable]] (*required*)

Used when:

* `JDBCRDD` is requested to <<spark-sql-JDBCRDD.adoc#resolveTable, resolveTable>> (when `JDBCRelation` is requested for the <<spark-sql-JDBCRelation.adoc#schema, schema>>) and <<spark-sql-JDBCRelation.adoc#compute, compute a partition>>

* `JDBCRelation` is requested to <<spark-sql-JDBCRelation.adoc#insert, insert or overwrite data>> and for the <<spark-sql-JDBCRelation.adoc#toString, human-friendly text representation>>

* `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-CreatableRelationProvider, create a BaseRelation>> (as a <<spark-sql-CreatableRelationProvider.adoc#createRelation, CreatableRelationProvider>>)

* `JdbcUtils` is requested to <<spark-sql-JdbcUtils.adoc#tableExists, tableExists>>, <<spark-sql-JdbcUtils.adoc#truncateTable, truncateTable>>, <<spark-sql-JdbcUtils.adoc#getSchemaOption, getSchemaOption>>, <<spark-sql-JdbcUtils.adoc#saveTable, saveTable>> and <<spark-sql-JdbcUtils.adoc#createTable, createTable>>

* `JDBCOptions` is <<creating-instance, created>> (with the input parameters for the <<url, url>> and <<dbtable, dbtable>> options)

* `DataFrameReader` is requested to <<spark-sql-DataFrameReader.adoc#jdbc, load data from external table using JDBC data source>> (using `DataFrameReader.jdbc` method with the input parameters for the <<url, url>> and <<dbtable, dbtable>> options)

! `driver`
!
! [[driver]][[driverClass]] (*recommended*) JDBC driver's class name.

When defined, the class will get registered with Java's https://docs.oracle.com/javase/8/docs/api/java/sql/DriverManager.html[java.sql.DriverManager]

! `fetchsize`
! `0`
! [[fetchsize]]

! `isolationLevel`
! `READ_UNCOMMITTED`
a! [[isolationLevel]] One of the following:

* NONE
* READ_UNCOMMITTED
* READ_COMMITTED
* REPEATABLE_READ
* SERIALIZABLE

! `lowerBound`
!
! [[lowerBound]] Lower bound of partition column

! `numPartitions`
!
! [[numPartitions]] Number of partitions

! `partitionColumn`
!
! [[partitionColumn]] Name of the column used to partition dataset (using a `JDBCPartitioningInfo`).

Used in `JdbcRelationProvider` to link:spark-sql-JdbcRelationProvider.adoc#createRelation-RelationProvider[create a `JDBCRelation`] (with proper `JDBCPartitions` with `WHERE` clause).

When defined, <<lowerBound, lowerBound>>, <<upperBound, upperBound>> and <<numPartitions, numPartitions>> options are required.

When undefined, <<lowerBound, lowerBound>> and <<upperBound, upperBound>> have to be undefined.

! `truncate`
! `false`
! [[truncate]][[isTruncate]] (used only for writing) Enables table truncation.

! `sessionInitStatement`
!
! [[sessionInitStatement]]

! `upperBound`
!
! [[upperBound]] Upper bound of the partition column

! `url`
!
! [[url]] (*required*)
|===

NOTE: The <<options, options>> are case-insensitive.

`JDBCOptions` is <<creating-instance, created>> when:

* `DataFrameReader` is requested to link:spark-sql-DataFrameReader.adoc#jdbc[load data from an external table using JDBC] (and create a `DataFrame` to represent the process of loading the data)

* `JdbcRelationProvider` is requested to create a `BaseRelation` (as a link:spark-sql-JdbcRelationProvider.adoc#createRelation-RelationProvider[RelationProvider] and link:spark-sql-JdbcRelationProvider.adoc#createRelation-CreatableRelationProvider[CreatableRelationProvider])

=== [[creating-instance]] Creating JDBCOptions Instance

`JDBCOptions` takes the following when created:

* URL
* [[table]] Name of the table
* [[parameters]] Collection of key-value pairs (i.e. `Map[String, String]`)

The input `URL` and <<table, table>> are set as the current <<url, url>> and <<dbtable, dbtable>> options (overriding the values in the input <<parameters, parameters>> if defined).

=== [[asProperties]] `asProperties` Property

[source, scala]
----
asProperties: Properties
----

`asProperties`...FIXME

[NOTE]
====
`asProperties` is used when:

* `JDBCRDD` is requested to link:spark-sql-JDBCRDD.adoc#compute[compute a partition] (that requests a `JdbcDialect` to link:spark-sql-JdbcDialect.adoc#beforeFetch[beforeFetch])

* `JDBCRelation` is requested to link:spark-sql-JDBCRelation.adoc#insert[insert a data (from a DataFrame) to a table]
====

=== [[asConnectionProperties]] `asConnectionProperties` Property

[source, scala]
----
asConnectionProperties: Properties
----

`asConnectionProperties`...FIXME

NOTE: `asConnectionProperties` is used exclusively when `JdbcUtils` is requested to link:spark-sql-JdbcUtils.adoc#createConnectionFactory[createConnectionFactory]
