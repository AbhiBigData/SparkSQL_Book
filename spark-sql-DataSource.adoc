== [[DataSource]] DataSource -- Pluggable Data Provider Framework

`DataSource` is one of the main parts of *Data Source API* in Spark SQL (together with link:spark-sql-DataFrameReader.adoc[DataFrameReader] for loading datasets, link:spark-sql-DataFrameWriter.adoc[DataFrameWriter] for saving datasets and `StreamSourceProvider` for creating streaming sources).

`DataSource` models a **pluggable data provider framework** with <<providers, extension points>> for Spark SQL integrators to expand the list of supported external data sources in Spark SQL.

`DataSource` is <<creating-instance, created>> when...MEFIXME

[[providers]]
.DataSource's Provider (and Format) Contracts
[cols="1,3",options="header",width="100%"]
|===
| Extension Point
| Description

| [[CreatableRelationProvider]] link:spark-sql-CreatableRelationProvider.adoc[CreatableRelationProvider]
| Data source that saves the result of a structured query per save mode and returns the schema

| [[FileFormat]] link:spark-sql-FileFormat.adoc[FileFormat]
a| Used in:

* <<sourceSchema, sourceSchema>> for streamed reading

* <<write, write>> for writing a `DataFrame` to a `DataSource` (as part of creating a table as select)

| [[RelationProvider]] link:spark-sql-RelationProvider.adoc[RelationProvider]
| Data source that supports schema inference and can be accessed using SQL's `USING` clause

| [[SchemaRelationProvider]] link:spark-sql-SchemaRelationProvider.adoc[SchemaRelationProvider]
| Data source that requires a user-defined schema

| `StreamSourceProvider`
a| Used in:

* <<sourceSchema, sourceSchema>> and <<createSource, createSource>> for streamed reading

* <<createSink, createSink>> for streamed writing

* <<resolveRelation, resolveRelation>> for resolved link:spark-sql-BaseRelation.adoc[BaseRelation].
|===

As a user, you interact with `DataSource` by link:spark-sql-DataFrameReader.adoc[DataFrameReader] (when you execute link:spark-sql-SparkSession.adoc#read[spark.read] or link:spark-sql-SparkSession.adoc#readStream[spark.readStream]) or SQL's `CREATE TABLE USING`.

[source, scala]
----
// Batch reading
val people: DataFrame = spark.read
  .format("csv")
  .load("people.csv")

// Streamed reading
val messages: DataFrame = spark.readStream
  .format("kafka")
  .option("subscribe", "topic")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .load
----

`DataSource` uses a link:spark-sql-SparkSession.adoc[SparkSession], a class name, a collection of `paths`, optional user-specified link:spark-sql-schema.adoc[schema], a collection of partition columns, a bucket specification, and configuration options.

NOTE: Data source is also called a *table provider*.

[[internal-registries]]
.DataSource's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| `providingClass`
| [[providingClass]] FIXME

Used when...FIXME

| `sourceInfo`
| [[sourceInfo]] FIXME

Used when...FIXME

| `caseInsensitiveOptions`
| [[caseInsensitiveOptions]] FIXME

Used when...FIXME

| `equality`
| [[equality]] FIXME

Used when...FIXME

| `backwardCompatibilityMap`
| [[backwardCompatibilityMap]] FIXME

Used when...FIXME
|===

=== [[writeAndRead]] Writing DataFrame to Data Source per Save Mode Followed by Reading Rows Back (as BaseRelation) -- `writeAndRead` Method

[source, scala]
----
writeAndRead(mode: SaveMode, data: DataFrame): BaseRelation
----

CAUTION: FIXME

NOTE: `writeAndRead` is used exclusively when link:spark-sql-LogicalPlan-CreateDataSourceTableAsSelectCommand.adoc#run[CreateDataSourceTableAsSelectCommand] logical command is executed.

=== [[write]] Writing DataFrame to Data Source Per Save Mode -- `write` Method

[source, scala]
----
write(mode: SaveMode, data: DataFrame): BaseRelation
----

`write` writes the result of executing a structured query (as link:spark-sql-DataFrame.adoc[DataFrame]) to a data source per save `mode`.

Internally, `write` <<lookupDataSource, looks up the data source>> and branches off per <<providingClass, providingClass>>.

[[write-providingClass-branches]]
.write's Branches per Supported providingClass (in execution order)
[width="100%",cols="1,2",options="header"]
|===
| providingClass
| Description

| link:spark-sql-CreatableRelationProvider.adoc[CreatableRelationProvider]
| Executes link:spark-sql-CreatableRelationProvider.adoc#createRelation[CreatableRelationProvider.createRelation]

| link:spark-sql-FileFormat.adoc[FileFormat]
| <<writeInFileFormat, writeInFileFormat>>

| _others_
| Reports a `RuntimeException`
|===

NOTE: `write` does not support the internal `CalendarIntervalType` in the link:spark-sql-schema.adoc[schema of `data` `DataFrame`] and throws a `AnalysisException` when there is one.

NOTE: `write` is used exclusively when link:spark-sql-LogicalPlan-RunnableCommand.adoc#SaveIntoDataSourceCommand[SaveIntoDataSourceCommand] is executed.

=== [[writeInFileFormat]] `writeInFileFormat` Internal Method

CAUTION: FIXME

For link:spark-sql-FileFormat.adoc[FileFormat] data sources, `write` takes all `paths` and `path` option and makes sure that there is only one.

NOTE: `write` uses Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/Path.html[Path] to access the https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem] and calculate the qualified output path.

`write` requests `PartitioningUtils` to link:spark-sql-PartitioningUtils.adoc#validatePartitionColumn[validatePartitionColumn].

When appending to a table, ...FIXME

In the end, `write` (for a link:spark-sql-FileFormat.adoc[FileFormat] data source) link:spark-sql-SessionState.adoc#executePlan[prepares a `InsertIntoHadoopFsRelationCommand` logical plan] with link:spark-sql-QueryExecution.adoc#toRdd[executes] it.

CAUTION: FIXME Is `toRdd` a job execution?

=== [[createSource]] `createSource` Method

[source, scala]
----
createSource(metadataPath: String): Source
----

CAUTION: FIXME

=== [[createSink]] `createSink` Method

CAUTION: FIXME

==== [[sourceSchema]] `sourceSchema` Internal Method

[source, scala]
----
sourceSchema(): SourceInfo
----

`sourceSchema` returns the name and link:spark-sql-schema.adoc[schema] of the data source for streamed reading.

CAUTION: FIXME Why is the method called? Why does this bother with streamed reading and data sources?!

It supports two class hierarchies, i.e. link:spark-sql-FileFormat.adoc[FileFormat] and Structured Streaming's `StreamSourceProvider` data sources.

Internally, `sourceSchema` first creates an instance of the data source and...

CAUTION: FIXME Finish...

For Structured Streaming's `StreamSourceProvider` data sources, `sourceSchema` relays calls to `StreamSourceProvider.sourceSchema`.

For link:spark-sql-FileFormat.adoc[FileFormat] data sources, `sourceSchema` makes sure that `path` option was specified.

TIP: `path` is looked up in a case-insensitive way so `paTh` and `PATH` and `pAtH` are all acceptable. Use the lower-case version of `path`, though.

NOTE: `path` can use https://en.wikipedia.org/wiki/Glob_%28programming%29[glob pattern] (not regex syntax), i.e. contain any of `{}[]*?\` characters.

It checks whether the path exists if a glob pattern is not used. In case it did not exist you will see the following `AnalysisException` exception in the logs:

```
scala> spark.read.load("the.file.does.not.exist.parquet")
org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/jacek/dev/oss/spark/the.file.does.not.exist.parquet;
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:375)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:364)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
  at scala.collection.immutable.List.flatMap(List.scala:344)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:364)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:132)
  ... 48 elided
```

If link:spark-sql-properties.adoc#spark.sql.streaming.schemaInference[spark.sql.streaming.schemaInference] is disabled and the data source is different than link:spark-sql-TextFileFormat.adoc[TextFileFormat], and the input `userSpecifiedSchema` is not specified, the following `IllegalArgumentException` exception is thrown:

[options="wrap"]
----
Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.
----

CAUTION: FIXME I don't think the exception will ever happen for non-streaming sources since the schema is going to be defined earlier. When?

Eventually, it returns a `SourceInfo` with `FileSource[path]` and the schema (as calculated using the <<inferFileFormatSchema, inferFileFormatSchema>> internal method).

For any other data source, it throws `UnsupportedOperationException` exception:

```
Data source [className] does not support streamed reading
```

==== [[inferFileFormatSchema]] `inferFileFormatSchema` Internal Method

[source, scala]
----
inferFileFormatSchema(format: FileFormat): StructType
----

`inferFileFormatSchema` private method computes (aka _infers_) schema (as link:spark-sql-StructType.adoc[StructType]). It returns `userSpecifiedSchema` if specified or uses `FileFormat.inferSchema`. It throws a `AnalysisException` when is unable to infer schema.

It uses `path` option for the list of directory paths.

NOTE: It is used by <<sourceSchema, DataSource.sourceSchema>> and <<createSource, DataSource.createSource>> when link:spark-sql-FileFormat.adoc[FileFormat] is processed.

=== [[resolveRelation]] Resolving Relation (Creating BaseRelation) -- `resolveRelation` Method

[source, scala]
----
resolveRelation(checkFilesExist: Boolean = true): BaseRelation
----

`resolveRelation` resolves (i.e. creates) a link:spark-sql-BaseRelation.adoc[BaseRelation].

Internally, `resolveRelation` tries to create an instance of the <<providingClass, providingClass>> and branches off per its type and whether the optional <<userSpecifiedSchema, user-specified schema>> was specified or not.

.Resolving BaseRelation per Provider and User-Specified Schema
[cols="1,3",options="header",width="100%"]
|===
| Provider
| Behaviour

| link:spark-sql-SchemaRelationProvider.adoc[SchemaRelationProvider]
| Executes link:spark-sql-SchemaRelationProvider.adoc#createRelation[SchemaRelationProvider.createRelation] with the provided schema

| link:spark-sql-RelationProvider.adoc[RelationProvider]
| Executes link:spark-sql-RelationProvider.adoc#createRelation[RelationProvider.createRelation]

| link:spark-sql-FileFormat.adoc[FileFormat]
| Creates a link:spark-sql-BaseRelation.adoc#HadoopFsRelation[HadoopFsRelation]
|===

[NOTE]
====
`resolveRelation` is used when:

* `DataSource` <<writeAndRead, writes and reads>> the result of a link:spark-sql-DataFrame.adoc[structured query] (when <<providingClass, providingClass>> is a link:spark-sql-FileFormat.adoc[FileFormat])
* `DataFrameReader` link:spark-sql-DataFrameReader.adoc#load[loads data from a data source that supports multiple paths]
* `TextInputCSVDataSource` and `TextInputJsonDataSource` are requested to infer schema
* `CreateDataSourceTableCommand` runnable command is link:spark-sql-LogicalPlan-CreateDataSourceTableCommand.adoc#run[executed]
* `CreateTempViewUsing` runnable command is executed
* `FindDataSourceTable` does link:spark-sql-FindDataSourceTable.adoc#readDataSourceTable[readDataSourceTable]
* `ResolveSQLOnFile` converts a logical plan (when <<providingClass, providingClass>> is a link:spark-sql-FileFormat.adoc[FileFormat])
* `HiveMetastoreCatalog` is requested for link:spark-sql-HiveMetastoreCatalog.adoc#convertToLogicalRelation[]
* Structured Streaming's `FileStreamSource` creates batches of records
====

=== [[buildStorageFormatFromOptions]] `buildStorageFormatFromOptions` Method

[source, scala]
----
buildStorageFormatFromOptions(options: Map[String, String]): CatalogStorageFormat
----

`buildStorageFormatFromOptions`...FIXME

NOTE: `buildStorageFormatFromOptions` is used when...FIXME

=== [[creating-instance]][[apply]] Creating DataSource Instance

`DataSource` takes the following when created:

* [[sparkSession]] link:spark-sql-SparkSession.adoc[SparkSession]
* [[className]] Name of the provider class (aka _input data source format_)
* [[paths]] Paths to load (default: empty)
* [[userSpecifiedSchema]] (optional) User-specified link:spark-sql-StructType.adoc[schema] (default: undefined)
* [[partitionColumns]] (optional) Names of the partition columns (default: empty)
* [[bucketSpec]] Optional link:spark-sql-BucketSpec.adoc[bucketing specification] (default: undefined)
* [[options]] Options (default: empty)
* [[catalogTable]] (optional) link:spark-sql-CatalogTable.adoc[CatalogTable] (default: undefined)

`DataSource` initializes the <<internal-registries, internal registries and counters>>.

==== [[lookupDataSource]] Looking Up Class By Name Of Data Source Provider -- `lookupDataSource` Method

[source, scala]
----
lookupDataSource(provider: String, conf: SQLConf): Class[_]
----

`lookupDataSource` looks up the class name in the <<backwardCompatibilityMap, backwardCompatibilityMap>> and then replaces the class name exclusively for the `orc` provider per link:spark-sql-properties.adoc#spark.sql.orc.impl[spark.sql.orc.impl] internal configuration property:

* For `hive` (default), `lookupDataSource` uses `org.apache.spark.sql.hive.orc.OrcFileFormat`

* For `native`, `lookupDataSource` uses the canonical class name of link:spark-sql-OrcFileFormat.adoc[OrcFileFormat], i.e. `org.apache.spark.sql.execution.datasources.orc.OrcFileFormat`

With the provider's class name (aka _provider1_ internally) `lookupDataSource` assumes another name variant of format `[provider1].DefaultSource` (aka _provider2_ internally).

`lookupDataSource` then uses Java's link:++https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader] to find all link:spark-sql-DataSourceRegister.adoc[DataSourceRegister] provider classes on the CLASSPATH.

`lookupDataSource` filters out the `DataSourceRegister` provider classes (by their link:spark-sql-DataSourceRegister.adoc#shortName[alias]) that match the _provider1_ (case-insensitive), e.g. `parquet` or `kafka`.

If a single provider class was found for the alias, `lookupDataSource` simply returns the provider class.

If no `DataSourceRegister` could be found by the short name (alias), `lookupDataSource` considers the names of the format provider as the fully-qualified class names and tries to load them instead (using Java's link:++https://docs.oracle.com/javase/8/docs/api/java/lang/ClassLoader.html#loadClass-java.lang.String-++[ClassLoader.loadClass]).

NOTE: You can reference your own custom `DataSource` in your code by link:spark-sql-DataFrameWriter.adoc#format[DataFrameWriter.format] method which is the alias or a fully-qualified class name.

CAUTION: FIXME Describe the other cases (orc and avro)

If no provider class could be found, `lookupDataSource` throws a `RuntimeException`:

[options="wrap"]
----
java.lang.ClassNotFoundException: Failed to find data source: [provider1]. Please find packages at http://spark.apache.org/third-party-projects.html
----

If however, `lookupDataSource` found multiple registered aliases for the provider name...FIXME
