== [[Builder]] Builder -- Building SparkSession using Fluent API

`Builder` is the <<methods, fluent API>> to create a <<spark-sql-SparkSession.adoc#, SparkSession>>.

[[methods]]
.Builder API
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| <<enableHiveSupport, enableHiveSupport>>
a| Enables Hive support

[source, scala]
----
enableHiveSupport(): Builder
----

| <<getOrCreate, getOrCreate>>
a| Gets the current link:spark-sql-SparkSession.adoc[SparkSession] or creates a new one.

[source, scala]
----
getOrCreate(): SparkSession
----

| <<withExtensions, withExtensions>>
a| Access to the <<spark-sql-SparkSessionExtensions.adoc#, SparkSessionExtensions>>

[source, scala]
----
withExtensions(f: SparkSessionExtensions => Unit): Builder
----
|===

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // avoid hardcoding the deployment environment
  .enableHiveSupport()              // self-explanatory, isn't it?
  .getOrCreate
----

You can use the fluent design pattern to set the various properties of a `SparkSession` that opens a session to Spark SQL.

NOTE: You can have multiple ``SparkSession``s in a single Spark application for different link:spark-sql-SparkSession.adoc#catalog[data catalogs] (through relational entities).

[[internal-registries]]
.Builder's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,2",options="header",width="100%"]
|===
| Name
| Description

| extensions
| [[extensions]] <<spark-sql-SparkSessionExtensions.adoc#, SparkSessionExtensions>>

Used when...FIXME
|===

=== [[getOrCreate]] `getOrCreate` Method

CAUTION: FIXME

=== [[config]] `config` Method

CAUTION: FIXME

=== [[enableHiveSupport]] Enabling Hive Support -- `enableHiveSupport` Method

[source, scala]
----
enableHiveSupport(): Builder
----

`enableHiveSupport` enables Hive support, i.e. running structured queries on Hive tables (and a persistent Hive metastore, support for Hive serdes and Hive user-defined functions).

[NOTE]
====
You do *not* need any existing Hive installation to use Spark's Hive support. `SparkSession` context will automatically create `metastore_db` in the current directory of a Spark application and a directory configured by link:spark-sql-StaticSQLConf.adoc#spark.sql.warehouse.dir[spark.sql.warehouse.dir].

Refer to link:spark-sql-SharedState.adoc[SharedState].
====

Internally, `enableHiveSupport` makes sure that the Hive classes are on CLASSPATH, i.e. Spark SQL's `org.apache.hadoop.hive.conf.HiveConf`, and sets link:spark-sql-StaticSQLConf.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] internal configuration property to `hive`.

=== [[withExtensions]] `withExtensions` Method

[source, scala]
----
withExtensions(f: SparkSessionExtensions => Unit): Builder
----

`withExtensions` simply executes the input `f` function with the <<extensions, SparkSessionExtensions>>.
