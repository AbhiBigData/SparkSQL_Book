== [[SparkOptimizer]] SparkOptimizer -- Default Spark Optimizer

`SparkOptimizer` is the concrete link:spark-sql-Optimizer.adoc[logical optimizer] in Spark SQL that uses the <<batches, regular optimizations>>.

`SparkOptimizer` is available as link:spark-sql-SessionState.adoc#optimizer[optimizer] property of `SessionState`.

[source, scala]
----
sparkSession.sessionState.optimizer
----

Executing the <<batches, optimization rules>> on a link:spark-sql-LogicalPlan.adoc[logical query plan] gives an *optimized logical plan* of a structured query.

You can access the optimization logical plan of a structured query through the `QueryExecution` as link:spark-sql-QueryExecution.adoc#optimizedPlan[optimizedPlan].

[source, scala]
----
// Applying two filter in sequence on purpose
// We want to kick CombineTypedFilters optimizer in
val dataset = spark.range(10).filter(_ % 2 == 0).filter(_ == 0)

// optimizedPlan is a lazy value
// Only at the first time you call it you will trigger optimizations
// Next calls end up with the cached already-optimized result
// Use explain to trigger optimizations again
scala> dataset.queryExecution.optimizedPlan
res0: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
TypedFilter <function1>, class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)
+- Range (0, 10, step=1, splits=Some(8))
----

A Spark developer can extend the <<batches, available logical optimizations>> and register <<User-Provided-Optimizers, user-provided optimizations>> using <<experimentalMethods, ExperimentalMethods>>.

`SparkOptimizer` comes however with three extension points for additional optimization batches (two of which are for custom `SparkOptimizers`):

. <<preOptimizationBatches, preOptimizationBatches>>
. <<postHocOptimizationBatches, postHocOptimizationBatches>>
. <<User-Provided-Optimizers, User Provided Optimizers>>

[[batches]]
.SparkOptimizer's Regular Optimization Batches (in the order of execution)
[cols="2,1,3,3",options="header",width="100%"]
|===
^.^| Batch Name
^.^| Strategy
| Rules
| Description

|
|
| <<preOptimizationBatches, preOptimizationBatches>>
|

|
|
| link:spark-sql-Optimizer.adoc#batches[Default Optimization Batches]
|

^.^| Optimize Metadata Only Query
^.^| `Once`
| link:spark-sql-SparkOptimizer-OptimizeMetadataOnlyQuery.adoc[OptimizeMetadataOnlyQuery]
|

^.^| Extract Python UDF from Aggregate
^.^| `Once`
| link:spark-sql-SparkOptimizer-ExtractPythonUDFFromAggregate.adoc[ExtractPythonUDFFromAggregate]
|

^.^| Prune File Source Table Partitions
^.^| `Once`
| link:spark-sql-SparkOptimizer-PruneFileSourcePartitions.adoc[PruneFileSourcePartitions]
|

^.^| Push down operators to data source scan
^.^| `Once`
| link:spark-sql-SparkOptimizer-PushDownOperatorsToDataSource.adoc[PushDownOperatorsToDataSource]
|

^.^|
^.^|
| <<postHocOptimizationBatches, postHocOptimizationBatches>>
|

^.^| [[User-Provided-Optimizers]] User Provided Optimizers
^.^| link:spark-sql-Optimizer.adoc#fixedPoint[FixedPoint]
| link:spark-sql-ExperimentalMethods.adoc#extraOptimizations[extraOptimizations]
|
|===

[TIP]
====
Enable `DEBUG` or `TRACE` logging levels for `org.apache.spark.sql.execution.SparkOptimizer` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.SparkOptimizer=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating SparkOptimizer Instance

`SparkOptimizer` takes the following when created:

* [[catalog]] link:spark-sql-SessionCatalog.adoc[SessionCatalog]
* [[conf]] link:spark-sql-SQLConf.adoc[SQLConf]
* [[experimentalMethods]] link:spark-sql-ExperimentalMethods.adoc[ExperimentalMethods]

NOTE: `SparkOptimizer` is created when `SessionState` link:spark-sql-SessionState.adoc#creating-instance[is created] (that initializes link:spark-sql-SessionState.adoc#optimizer[optimizer] property).

=== [[preOptimizationBatches]] Extension Point for Additional Pre-Optimization Batches -- `preOptimizationBatches` Method

[source, scala]
----
preOptimizationBatches: Seq[Batch]
----

`preOptimizationBatches` are the additional *pre-optimization batches* that are executed right before the <<batches, regular optimization batches>>.

=== [[postHocOptimizationBatches]] Extension Point for Additional Post-Hoc Optimization Batches -- `postHocOptimizationBatches` Method

[source, scala]
----
postHocOptimizationBatches: Seq[Batch] = Nil
----

`postHocOptimizationBatches` are the additional *post-optimization batches* that are executed right after the <<batches, regular optimization batches>> (before <<User-Provided-Optimizers, User Provided Optimizers>>).

=== [[i-want-more]] Further Reading and Watching

1. https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html[Deep Dive into Spark SQLâ€™s Catalyst Optimizer]

2. (video) https://youtu.be/_1byVWTEK1s?t=19m7s[Modern Spark DataFrame and Dataset (Intermediate Tutorial)] by https://twitter.com/adbreind[Adam Breindel] from Databricks.
