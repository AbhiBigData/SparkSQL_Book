== [[KafkaOffsetReader]] KafkaOffsetReader

[[options]]
.KafkaOffsetReader's Options
[cols="1,1,2",options="header",width="100%"]
|===
| Name
| Default Value
| Description

| [[fetchOffset.numRetries]] `fetchOffset.numRetries`
| `3`
|

| [[fetchOffset.retryIntervalMs]] `fetchOffset.retryIntervalMs`
| `1000`
| How long to wait before retries.
|===

[[internal-registries]]
.KafkaOffsetReader's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[consumer]] `consumer`
a| Kafka's https://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/Consumer.html[Consumer] (with keys and values of `Array[Byte]` type)

<<createConsumer, Initialized>> when `KafkaOffsetReader` is <<creating-instance, created>>.

Used when `KafkaOffsetReader`:

* <<fetchTopicPartitions, fetchTopicPartitions>>
* <<fetchSpecificOffsets, fetches offsets for selected TopicPartitions>>
* <<fetchEarliestOffsets, fetchEarliestOffsets>>
* <<fetchLatestOffsets, fetchLatestOffsets>>
* <<resetConsumer, resetConsumer>>
* <<close, is closed>>

| [[execContext]] `execContext`
|

| [[groupId]] `groupId`
|

| [[kafkaReaderThread]] `kafkaReaderThread`
|

| [[maxOffsetFetchAttempts]] `maxOffsetFetchAttempts`
|

| [[nextId]] `nextId`
|

| [[offsetFetchAttemptIntervalMs]] `offsetFetchAttemptIntervalMs`
|
|===

[TIP]
====
Enable `INFO` or `DEBUG` logging levels for `org.apache.spark.sql.kafka010.KafkaOffsetReader` to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.kafka010.KafkaOffsetReader=DEBUG
```

Refer to link:spark-sql-streaming-logging.adoc[Logging].
====

=== [[fetchSpecificOffsets]] Fetching Offsets for Selected TopicPartitions -- `fetchSpecificOffsets` Method

[source, scala]
----
fetchSpecificOffsets(partitionOffsets: Map[TopicPartition, Long]): Map[TopicPartition, Long]
----

.KafkaOffsetReader's fetchSpecificOffsets
image::images/KafkaOffsetReader-fetchSpecificOffsets.png[align="center"]

`fetchSpecificOffsets` requests the <<consumer, Kafka Consumer>> to `poll(0)`.

`fetchSpecificOffsets` requests the <<consumer, Kafka Consumer>> for assigned partitions (using `Consumer.assignment()`).

`fetchSpecificOffsets` requests the <<consumer, Kafka Consumer>> to `pause(partitions)`.

You should see the following DEBUG message in the logs:

```
DEBUG KafkaOffsetReader: Partitions assigned to consumer: [partitions]. Seeking to [partitionOffsets]
```

For every partition offset in the input `partitionOffsets`, `fetchSpecificOffsets` requests the <<consumer, Kafka Consumer>> to:

* `seekToEnd` for the latest (aka `-1`)
* `seekToBeginning` for the earliest (aka `-2`)
* `seek` for other offsets

In the end, `fetchSpecificOffsets` creates a collection of Kafka's `TopicPartition` and `position` (using the <<consumer, Kafka Consumer>>).

NOTE: `fetchSpecificOffsets` is used when `KafkaSource` is requested to <<spark-sql-streaming-KafkaSource.adoc#fetchAndVerify, fetch and verify initial partition offsets>>.

=== [[createConsumer]] Creating Kafka Consumer -- `createConsumer` Internal Method

[source, scala]
----
createConsumer(): Consumer[Array[Byte], Array[Byte]]
----

`createConsumer` requests <<consumerStrategy, ConsumerStrategy>> to link:spark-sql-streaming-ConsumerStrategy.adoc#createConsumer[create a Kafka Consumer] with <<driverKafkaParams, driverKafkaParams>> and <<nextGroupId, new generated group.id Kafka property>>.

NOTE: `createConsumer` is used when `KafkaOffsetReader` is <<creating-instance, created>> (and initializes <<consumer, consumer>>) and <<resetConsumer, resetConsumer>>

=== [[creating-instance]] Creating KafkaOffsetReader Instance

`KafkaOffsetReader` takes the following when created:

* [[consumerStrategy]] `ConsumerStrategy`,
* [[driverKafkaParams]] Kafka parameters (as `Map[String, Object]`)
* [[readerOptions]] Reader options (as `Map[String, String]`)
* [[driverGroupIdPrefix]] Prefix for the group id

`KafkaOffsetReader` initializes the <<internal-registries, internal registries and counters>>.

=== [[close]] `close` Method

[source, scala]
----
close(): Unit
----

`close`...FIXME

NOTE: `close` is used when...FIXME
