== [[DataSourceV2ScanExec]] DataSourceV2ScanExec Leaf Physical Operator

`DataSourceV2ScanExec` is a link:spark-sql-SparkPlan.adoc#LeafExecNode[leaf physical operator] to represent link:spark-sql-LogicalPlan-DataSourceV2Relation.adoc[DataSourceV2Relation] logical operators at execution time.

NOTE: A link:spark-sql-LogicalPlan-DataSourceV2Relation.adoc[DataSourceV2Relation] logical operator is created when...FIXME

`DataSourceV2ScanExec` is a link:spark-sql-ColumnarBatchScan.adoc[ColumnarBatchScan] that <<supportsBatch, supports vectorized batch decoding>> (when <<creating-instance, created>> for a <<reader, DataSourceReader>> that supports it, i.e. the `DataSourceReader` is a link:spark-sql-SupportsScanColumnarBatch.adoc[SupportsScanColumnarBatch] with the link:spark-sql-SupportsScanColumnarBatch.adoc#enableBatchRead[enableBatchRead] flag enabled).

`DataSourceV2ScanExec` is also a `DataSourceReaderHolder`.

`DataSourceV2ScanExec` is <<creating-instance, created>> exclusively when `DataSourceV2Strategy` execution planning strategy is link:spark-sql-SparkStrategy-DataSourceV2Strategy.adoc#apply[executed] (for a logical query plan with link:spark-sql-LogicalPlan-DataSourceV2Relation.adoc[DataSourceV2Relation] logical operators).

[[internal-registries]]
.DataSourceV2ScanExec's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[readerFactories]] `readerFactories`
| Collection of link:spark-sql-DataReaderFactory.adoc[DataReaderFactory] objects of link:spark-sql-UnsafeRow.adoc[UnsafeRows]

Used when...FIXME
|===

=== [[doExecute]] Executing DataSourceV2ScanExec -- `doExecute` Method

[source, scala]
----
doExecute(): RDD[InternalRow]
----

NOTE: `doExecute` is part of link:spark-sql-SparkPlan.adoc#doExecute[SparkPlan Contract] to describe a distributed computation as an `RDD` of link:spark-sql-InternalRow.adoc[internal rows] that is the runtime representation of a structured query (aka _execute_).

`doExecute`...FIXME

=== [[supportsBatch]] `supportsBatch` Property

[source, scala]
----
supportsBatch: Boolean
----

NOTE: `supportsBatch` is part of link:spark-sql-ColumnarBatchScan.adoc#supportsBatch[ColumnarBatchScan Contract] to control whether the physical operator supports link:spark-sql-vectorized-parquet-reader.adoc[vectorized decoding] or not.

`supportsBatch` is enabled (i.e. `true`) only when the <<reader, DataSourceReader>> is a link:spark-sql-SupportsScanColumnarBatch.adoc[SupportsScanColumnarBatch] with the link:spark-sql-SupportsScanColumnarBatch.adoc#enableBatchRead[enableBatchRead] flag enabled.

NOTE: link:spark-sql-SupportsScanColumnarBatch.adoc#enableBatchRead[enableBatchRead] flag is enabled by default.

`supportsBatch` is disabled (i.e. `false`) otherwise.

=== [[creating-instance]] Creating DataSourceV2ScanExec Instance

`DataSourceV2ScanExec` takes the following when created:

* [[output]] Output schema (as a collection of `AttributeReferences`)
* [[reader]] `DataSourceReader`

`DataSourceV2ScanExec` initializes the <<internal-registries, internal registries and counters>>.
