== Dataset API -- Dataset Operators

Dataset API is a <<methods, set of operators>> with typed and untyped transformations, and actions.

Dataset API can be grouped per target, i.e. the part of a structured query (a <<spark-sql-Dataset.adoc#, Dataset>>) they are applied to and work with.

. link:spark-sql-Column.adoc[Column Operators]
. link:spark-sql-functions.adoc[Standard Functions] (from `functions` object)
. link:spark-sql-udfs.adoc[User-Defined Functions (UDFs)]
. link:spark-sql-basic-aggregation.adoc[Basic Aggregation -- Typed and Untyped Grouping Operators]
. link:spark-sql-functions-windows.adoc[Window Aggregate Functions]
. link:spark-sql-UserDefinedAggregateFunction.adoc[User-Defined Aggregate Functions (UDAFs)]
. link:spark-sql-joins.adoc[Dataset Join Operators]
. link:spark-sql-caching.adoc[Dataset Caching and Persistence]
. link:spark-sql-checkpointing.adoc[Dataset Checkpointing]

Beside the above operators, there are the following ones working with a `Dataset` as a whole.

[[methods]]
[[operators]]
.Dataset Operators (Transformations and Actions)
[cols="1,3",options="header",width="100%"]
|===
| Operator
| Description

| <<as, as>>
| Enforcing types, i.e. marking the records in the `Dataset` as of a given data type (_data type conversion_)

| <<spark-sql-Dataset-typed-transformations.adoc#coalesce, coalesce>>
| Repartitioning a `Dataset` with shuffle disabled.

| <<spark-sql-Dataset-actions.adoc#collect, collect>>
| Action to...FIXME

| <<spark-sql-Dataset-actions.adoc#count, count>>
| Action to count the number of rows

| <<spark-sql-Dataset-basic-actions.adoc#createGlobalTempView, createGlobalTempView>>
|

| <<spark-sql-Dataset-basic-actions.adoc#createOrReplaceTempView, createOrReplaceTempView>>
|

| <<spark-sql-Dataset-basic-actions.adoc#createTempView, createTempView>>
|

| <<spark-sql-Dataset-actions.adoc#describe, describe>>
| Action to...FIXME

| <<spark-sql-Dataset-untyped-transformations.adoc#drop, drop>>
|

| <<explain, explain>>
| Explain logical and physical plans of a `Dataset`

| <<spark-sql-Dataset-typed-transformations.adoc#filter, filter>>
|

| <<spark-sql-Dataset-actions.adoc#first, first>>
| Action to...FIXME

| <<spark-sql-Dataset-typed-transformations.adoc#flatMap, flatMap>>
|

| <<spark-sql-Dataset-actions.adoc#foreach, foreach>>
| Action to...FIXME

| <<spark-sql-Dataset-actions.adoc#foreachPartition, foreachPartition>>
| Action to...FIXME

Internally, `foreachPartition` executes `foreachPartition` action on the Dataset's link:spark-sql-Dataset.adoc#rdd[RDD].

| <<spark-sql-Dataset-actions.adoc#head, head>>
| Action to...FIXME

| <<spark-sql-Dataset-typed-transformations.adoc#hint, hint>>
| Specifying a hint (and optional parameters)

| <<spark-sql-Dataset-typed-transformations.adoc#mapPartitions, mapPartitions>>
|

| <<spark-sql-Dataset-typed-transformations.adoc#randomSplit, randomSplit>>
| Randomly split a `Dataset` into two ``Dataset``s

| <<rdd, rdd>>
|

| <<spark-sql-Dataset-actions.adoc#reduce, reduce>>
|  Action to reduce the records of the `Dataset` using the specified binary function.

| <<spark-sql-Dataset-typed-transformations.adoc#repartition, repartition>>
| Repartitioning a `Dataset` with shuffle enabled.

| <<schema, schema>>
|

| <<spark-sql-Dataset-untyped-transformations.adoc#select, select>>
|

| <<spark-sql-Dataset-untyped-transformations.adoc#selectExpr, selectExpr>>
|

| <<spark-sql-Dataset-actions.adoc#show, show>>
| Action to...FIXME

| <<spark-sql-Dataset-actions.adoc#summary, summary>>
| [[summary]] Action to calculate statistics (e.g. `count`, `mean`, `stddev`, `min`, `max` and `25%`, `50%`, `75%` percentiles)

| <<spark-sql-Dataset-actions.adoc#take, take>>
| Action to take n first records of the `Dataset`

| <<spark-sql-Dataset-untyped-transformations.adoc#toDF, toDF>>
| Converts a `Dataset` to a `DataFrame`

| <<spark-sql-Dataset-typed-transformations.adoc#toJSON, toJSON>>
|

| <<spark-sql-Dataset-actions.adoc#toLocalIterator, toLocalIterator>>
| Action to...FIXME

| <<spark-sql-Dataset-typed-transformations.adoc#transform, transform>>
| Transforms a `Dataset`

| <<spark-sql-Dataset-typed-transformations.adoc#where, where>>
|

| <<spark-sql-Dataset-typed-transformations.adoc#withWatermark, withWatermark>>
| (*Spark Structured Streaming*) Creates a streaming `Dataset` with `EventTimeWatermark` logical operator

| <<write, write>>
|

| <<writeStream, writeStream>>
| (*Spark Structured Streaming*)
|===

=== [[write]] Accessing DataFrameWriter (to Describe Writing Dataset) -- `write` Operator

[source, scala]
----
write: DataFrameWriter[T]
----

`write` gives link:spark-sql-DataFrameWriter.adoc[DataFrameWriter] for records of type `T`.

[source, scala]
----
import org.apache.spark.sql.{DataFrameWriter, Dataset}
val ints: Dataset[Int] = (0 to 5).toDS
val writer: DataFrameWriter[Int] = ints.write
----

=== [[explain]] Displaying Logical and Physical Plans, Their Cost and Codegen -- `explain` Operator

[source, scala]
----
explain(): Unit
explain(extended: Boolean): Unit
----

`explain` prints the link:spark-sql-LogicalPlan.adoc[logical] and (with `extended` flag enabled) link:spark-sql-SparkPlan.adoc[physical] plans, their cost and codegen to the console.

TIP: Use `explain` to review the structured queries and optimizations applied.

Internally, `explain` creates a link:spark-sql-LogicalPlan-ExplainCommand.adoc[ExplainCommand] logical command and requests `SessionState` to link:spark-sql-SessionState.adoc#executePlan[execute it] (to get a link:spark-sql-QueryExecution.adoc[QueryExecution] back).

NOTE: `explain` uses link:spark-sql-LogicalPlan-ExplainCommand.adoc[ExplainCommand] logical command that, when link:spark-sql-LogicalPlan-ExplainCommand.adoc#run[executed], gives different text representations of link:spark-sql-QueryExecution.adoc[QueryExecution] (for the Dataset's link:spark-sql-LogicalPlan.adoc[LogicalPlan]) depending on the flags (e.g. extended, codegen, and cost which are disabled by default).

`explain` then requests `QueryExecution` for the link:spark-sql-QueryExecution.adoc#executedPlan[optimized physical query plan] and link:spark-sql-SparkPlan.adoc#executeCollect[collects the records] (as link:spark-sql-InternalRow.adoc[InternalRow] objects).

[NOTE]
====
`explain` uses Dataset's link:spark-sql-Dataset.adoc#sparkSession[SparkSession] to link:spark-sql-SparkSession.adoc#sessionState[access the current `SessionState`].
====

In the end, `explain` goes over the `InternalRow` records and converts them to lines to display to console.

NOTE: `explain` "converts" an `InternalRow` record to a line using link:spark-sql-InternalRow.adoc#getString[getString] at position `0`.

TIP: If you are serious about query debugging you could also use the link:spark-sql-debugging-execution.adoc[Debugging Query Execution facility].

[source, scala]
----
scala> spark.range(10).explain(extended = true)
== Parsed Logical Plan ==
Range (0, 10, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint
Range (0, 10, step=1, splits=Some(8))

== Optimized Logical Plan ==
Range (0, 10, step=1, splits=Some(8))

== Physical Plan ==
*Range (0, 10, step=1, splits=Some(8))
----

=== [[schema]] Accessing Schema -- `schema` Method

A `Dataset` has a *schema*.

[source, scala]
----
schema: StructType
----

[TIP]
====
You may also use the following methods to learn about the schema:

* `printSchema(): Unit`
* <<explain, explain>>
====

=== [[rdd]] Generating RDD of Internal Binary Rows -- `rdd` Attribute

[source, scala]
----
rdd: RDD[T]
----

Whenever you are in need to convert a `Dataset` into a `RDD`, executing `rdd` method gives you the RDD of the proper input object type (not link:spark-sql-DataFrame.adoc#features[Row as in DataFrames]) that sits behind the `Dataset`.

[source, scala]
----
scala> val rdd = tokens.rdd
rdd: org.apache.spark.rdd.RDD[Token] = MapPartitionsRDD[11] at rdd at <console>:30
----

Internally, it looks link:spark-sql-ExpressionEncoder.adoc[ExpressionEncoder] (for the `Dataset`) up and accesses the `deserializer` expression. That gives the link:spark-sql-DataType.adoc[DataType] of the result of evaluating the expression.

NOTE: A deserializer expression is used to decode an link:spark-sql-InternalRow.adoc[InternalRow] to an object of type `T`. See link:spark-sql-ExpressionEncoder.adoc[ExpressionEncoder].

It then executes a link:spark-sql-LogicalPlan-DeserializeToObject.adoc[`DeserializeToObject` logical operator] that will produce a `RDD[InternalRow]` that is converted into the proper `RDD[T]` using the `DataType` and `T`.

NOTE: It is a lazy operation that "produces" a `RDD[T]`.
