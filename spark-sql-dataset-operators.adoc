== Dataset API -- Dataset Operators

Dataset API is a <<methods, set of operators>> with typed and untyped transformations, and actions to work with a structured query (as a <<spark-sql-Dataset.adoc#, Dataset>>) as a whole.

[[methods]]
[[operators]]
.Dataset Operators (Transformations and Actions)
[cols="1,3",options="header",width="100%"]
|===
| Operator
| Description

| <<spark-sql-Dataset-untyped-transformations.adoc#agg, agg>>
a| [[agg]] Untyped transformation to...FIXME

[source, scala]
----
agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame
agg(expr: Column, exprs: Column*): DataFrame
agg(exprs: Map[String, String]): DataFrame
----

| <<spark-sql-Dataset-typed-transformations.adoc#alias, alias>>
a| [[alias]] Typed transformation to...FIXME

[source, scala]
----
alias(alias: String): Dataset[T]
alias(alias: Symbol): Dataset[T]
----

| <<spark-sql-Dataset-untyped-transformations.adoc#apply, apply>>
a| [[apply]] Untyped transformation to select a column based on the column name (i.e. maps a `Dataset` onto a `Column`)

[source, scala]
----
apply(colName: String): Column
----

| <<spark-sql-Dataset-typed-transformations.adoc#as-alias, as>>
a| [[as-alias]] Typed transformation to...FIXME

[source, scala]
----
as(alias: String): Dataset[T]
as(alias: Symbol): Dataset[T]
----

| <<spark-sql-Dataset-typed-transformations.adoc#as-type, as>>
a| [[as-type]] Typed transformation to enforce a type, i.e. marking the records in the `Dataset` as of a given data type (_data type conversion_)

[source, scala]
----
as[U : Encoder]: Dataset[U]
----

| <<spark-sql-Dataset-basic-actions.adoc#cache, cache>>
a| [[cache]] Basic action to cache a Dataset

[source, scala]
----
cache(): this.type
----

| <<spark-sql-Dataset-basic-actions.adoc#checkpoint, checkpoint>>
a| [[checkpoint]] Basic action to checkpoint a Dataset in a reliable way

[source, scala]
----
checkpoint(): Dataset[T]
checkpoint(eager: Boolean): Dataset[T]
----

| <<spark-sql-Dataset-typed-transformations.adoc#coalesce, coalesce>>
a| [[coalesce]] Typed transformation to repartition a Dataset

[source, scala]
----
coalesce(numPartitions: Int): Dataset[T]
----

| <<spark-sql-Dataset-untyped-transformations.adoc#col, col>>
a| [[col]] Untyped transformation to select a column based on the column name (i.e. maps a `Dataset` onto a `Column`)

[source, scala]
----
col(colName: String): Column
----

| <<spark-sql-Dataset-actions.adoc#collect, collect>>
| [[collect]] Action to...FIXME

| <<spark-sql-Dataset-untyped-transformations.adoc#colRegex, colRegex>>
a| [[colRegex]] (*New in 2.3.0*) Untyped transformation to select a column based on the column name specified as a regex (i.e. maps a `Dataset` onto a `Column`)

[source, scala]
----
colRegex(colName: String): Column
----

| <<spark-sql-Dataset-basic-actions.adoc#columns, columns>>
a| [[columns]] Basic action to...FIXME

[source, scala]
----
columns: Array[String]
----

| <<spark-sql-Dataset-actions.adoc#count, count>>
a| [[count]] Action to count the number of rows

[source, scala]
----
count(): Long
----

| <<spark-sql-Dataset-basic-actions.adoc#createGlobalTempView, createGlobalTempView>>
a| [[createGlobalTempView]] Basic action to...FIXME

[source, scala]
----
createGlobalTempView(viewName: String): Unit
----

| <<spark-sql-Dataset-basic-actions.adoc#createOrReplaceGlobalTempView, createOrReplaceGlobalTempView>>
a| [[createOrReplaceGlobalTempView]] Basic action to...FIXME

[source, scala]
----
createOrReplaceGlobalTempView(viewName: String): Unit
----

| <<spark-sql-Dataset-basic-actions.adoc#createOrReplaceTempView, createOrReplaceTempView>>
a| [[createOrReplaceTempView]] Basic action to...FIXME

[source, scala]
----
createOrReplaceTempView(viewName: String): Unit
----

| <<spark-sql-Dataset-basic-actions.adoc#createTempView, createTempView>>
a| [[createTempView]] Basic action to...FIXME

[source, scala]
----
createTempView(viewName: String): Unit
----

| <<spark-sql-Dataset-untyped-transformations.adoc#crossJoin, crossJoin>>
a| [[crossJoin]] Untyped transformation to...FIXME

[source, scala]
----
crossJoin(right: Dataset[_]): DataFrame
----

| <<spark-sql-Dataset-untyped-transformations.adoc#cube, cube>>
a| [[cube]] Untyped transformation to...FIXME

[source, scala]
----
cube(cols: Column*): RelationalGroupedDataset
cube(col1: String, cols: String*): RelationalGroupedDataset
----

| <<spark-sql-Dataset-actions.adoc#describe, describe>>
a| [[describe]] Action to...FIXME

[source, scala]
----
describe(cols: String*): DataFrame
----

| <<spark-sql-Dataset-typed-transformations.adoc#distinct, distinct>>
a| [[distinct]] Typed transformation to...FIXME

[source, scala]
----
distinct(): Dataset[T]
----

| <<spark-sql-Dataset-untyped-transformations.adoc#drop, drop>>
a| [[drop]] Untyped transformation to...FIXME

[source, scala]
----
drop(colName: String): DataFrame
drop(colNames: String*): DataFrame
drop(col: Column): DataFrame
----

| <<spark-sql-Dataset-typed-transformations.adoc#dropDuplicates, dropDuplicates>>
a| [[dropDuplicates]] Typed transformation to...FIXME

[source, scala]
----
dropDuplicates(): Dataset[T]
dropDuplicates(colNames: Array[String]): Dataset[T]
dropDuplicates(colNames: Seq[String]): Dataset[T]
dropDuplicates(col1: String, cols: String*): Dataset[T]
----

| <<spark-sql-Dataset-basic-actions.adoc#dtypes, dtypes>>
a| [[dtypes]] Basic action to...FIXME

[source, scala]
----
dtypes: Array[(String, String)]
----

| <<spark-sql-Dataset-typed-transformations.adoc#except, except>>
a| [[except]] Typed transformation to...FIXME

[source, scala]
----
except(other: Dataset[T]): Dataset[T]
----

| <<spark-sql-Dataset-basic-actions.adoc#explain, explain>>
a| [[explain]] Basic action to explain the logical and physical plans of the `Dataset`, i.e. displays the logical and physical plans (with optional cost and codegen summaries) to the standard output

[source, scala]
----
explain(): Unit
explain(extended: Boolean): Unit
----

| <<spark-sql-Dataset-typed-transformations.adoc#filter, filter>>
a| [[filter]] Typed transformation to...FIXME

[source, scala]
----
filter(condition: Column): Dataset[T]
filter(conditionExpr: String): Dataset[T]
filter(func: T => Boolean): Dataset[T]
----

| <<spark-sql-Dataset-actions.adoc#first, first>>
a| [[first]] Action to...FIXME

[source, scala]
----
first(): T
----

| <<spark-sql-Dataset-typed-transformations.adoc#flatMap, flatMap>>
a| [[flatMap]] Typed transformation to...FIXME

[source, scala]
----
flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U]
----

| <<spark-sql-Dataset-actions.adoc#foreach, foreach>>
a| [[foreach]] Action to...FIXME

[source, scala]
----
foreach(f: T => Unit): Unit
----

| <<spark-sql-Dataset-actions.adoc#foreachPartition, foreachPartition>>
a| [[foreachPartition]] Action to...FIXME

[source, scala]
----
foreachPartition(f: Iterator[T] => Unit): Unit
----

| <<spark-sql-Dataset-untyped-transformations.adoc#groupBy, groupBy>>
a| [[groupBy]] Untyped transformation to...FIXME

[source, scala]
----
groupBy(cols: Column*): RelationalGroupedDataset
groupBy(col1: String, cols: String*): RelationalGroupedDataset
----

| <<spark-sql-Dataset-typed-transformations.adoc#groupByKey, groupByKey>>
a| [[groupByKey]] Typed transformation to...FIXME

[source, scala]
----
groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T]
----

| <<spark-sql-Dataset-actions.adoc#head, head>>
a| [[head]] Action to...FIXME

[source, scala]
----
head(): T
head(n: Int): Array[T]
----

| <<spark-sql-Dataset-basic-actions.adoc#hint, hint>>
a| [[hint]] Basic action to specify a hint (and optional parameters)

[source, scala]
----
hint(name: String, parameters: Any*): Dataset[T]
----

| <<spark-sql-Dataset-basic-actions.adoc#inputFiles, inputFiles>>
a| [[inputFiles]] Basic action to...FIXME

[source, scala]
----
inputFiles: Array[String]
----

| <<spark-sql-Dataset-typed-transformations.adoc#intersect, intersect>>
a| [[intersect]] Typed transformation to...FIXME

[source, scala]
----
intersect(other: Dataset[T]): Dataset[T]
----

| <<spark-sql-Dataset-basic-actions.adoc#isLocal, isLocal>>
a| [[isLocal]] Basic action to...FIXME

[source, scala]
----
isLocal: Boolean
----

| <<spark-sql-Dataset-untyped-transformations.adoc#join, join>>
a| [[join]] Untyped transformation to...FIXME

[source, scala]
----
join(right: Dataset[_]): DataFrame
join(right: Dataset[_], usingColumn: String): DataFrame
join(right: Dataset[_], usingColumns: Seq[String]): DataFrame
join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame
join(right: Dataset[_], joinExprs: Column): DataFrame
join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame
----

| <<spark-sql-Dataset-typed-transformations.adoc#joinWith, joinWith>>
a| [[joinWith]] Typed transformation to...FIXME

[source, scala]
----
joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)]
joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)]
----

| <<spark-sql-Dataset-typed-transformations.adoc#limit, limit>>
a| [[limit]] Typed transformation to...FIXME

[source, scala]
----
limit(n: Int): Dataset[T]
----

| <<spark-sql-Dataset-basic-actions.adoc#localCheckpoint, localCheckpoint>>
a| [[localCheckpoint]] (*New in 2.3.0*) Basic action to checkpoint the Dataset locally on executors (and therefore unreliably)

[source, scala]
----
localCheckpoint(): Dataset[T]
localCheckpoint(eager: Boolean): Dataset[T]
----

| <<spark-sql-Dataset-typed-transformations.adoc#map, map>>
a| [[map]] Typed transformation to...FIXME

[source, scala]
----
map[U: Encoder](func: T => U): Dataset[U]
----

| <<spark-sql-Dataset-typed-transformations.adoc#mapPartitions, mapPartitions>>
a| [[mapPartitions]] Typed transformation to...FIXME

[source, scala]
----
mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U]
----

| <<spark-sql-Dataset-untyped-transformations.adoc#na, na>>
a| [[na]] Untyped transformation to...FIXME

[source, scala]
----
na: DataFrameNaFunctions
----

| <<spark-sql-Dataset-typed-transformations.adoc#orderBy, orderBy>>
a| [[orderBy]] Typed transformation to...FIXME

[source, scala]
----
orderBy(sortExprs: Column*): Dataset[T]
orderBy(sortCol: String, sortCols: String*): Dataset[T]
----

| <<spark-sql-Dataset-basic-actions.adoc#persist, persist>>
a| [[persist]] Basic action to persist a Dataset

[source, scala]
----
persist(): this.type
persist(newLevel: StorageLevel): this.type
----

| <<spark-sql-Dataset-basic-actions.adoc#printSchema, printSchema>>
a| [[printSchema]] Basic action to...FIXME

[source, scala]
----
printSchema(): Unit
----

| <<spark-sql-Dataset-typed-transformations.adoc#randomSplit, randomSplit>>
a| [[randomSplit]] Typed transformation to split a Dataset randomly into two Datasets

[source, scala]
----
randomSplit(weights: Array[Double]): Array[Dataset[T]]
randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]]
----

| <<rdd, rdd>>
|

| <<spark-sql-Dataset-actions.adoc#reduce, reduce>>
a| [[reduce]] Action to reduce the records of the `Dataset` using the specified binary function.

[source, scala]
----
reduce(func: (T, T) => T): T
----

| <<spark-sql-Dataset-typed-transformations.adoc#repartition, repartition>>
a| [[repartition]] Typed transformation to repartition a Dataset

[source, scala]
----
repartition(partitionExprs: Column*): Dataset[T]
repartition(numPartitions: Int): Dataset[T]
repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T]
----

| <<spark-sql-Dataset-typed-transformations.adoc#repartitionByRange, repartitionByRange>>
a| [[repartitionByRange]] (*New in 2.3.0*) Typed transformation to...FIXME

[source, scala]
----
repartitionByRange(partitionExprs: Column*): Dataset[T]
repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T]
----

| <<spark-sql-Dataset-untyped-transformations.adoc#rollup, rollup>>
a| [[rollup]] Untyped transformation to...FIXME

[source, scala]
----
rollup(cols: Column*): RelationalGroupedDataset
rollup(col1: String, cols: String*): RelationalGroupedDataset
----

| <<spark-sql-Dataset-typed-transformations.adoc#sample, sample>>
a| [[sample]] Typed transformation to...FIXME

[source, scala]
----
sample(withReplacement: Boolean, fraction: Double): Dataset[T]
sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T]
sample(fraction: Double): Dataset[T]  // <1>
sample(fraction: Double, seed: Long): Dataset[T]  // <2>
----
<1> *New in 2.3.0*
<2> *New in 2.3.0*

| <<spark-sql-Dataset-basic-actions.adoc#schema, schema>>
a| [[schema]] Basic action to...FIXME

[source, scala]
----
schema: StructType
----

| <<spark-sql-Dataset-untyped-transformations.adoc#select, select>>
a| [[select]] Transformation to...FIXME

[source, scala]
----
// Untyped transformations
select(cols: Column*): DataFrame
select(col: String, cols: String*): DataFrame

// Typed transformations
select[U1](c1: TypedColumn[T, U1]): Dataset[U1]
select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)]
select[U1, U2, U3](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)]
select[U1, U2, U3, U4](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3],
  c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)]
select[U1, U2, U3, U4, U5](
  c1: TypedColumn[T, U1],
  c2: TypedColumn[T, U2],
  c3: TypedColumn[T, U3],
  c4: TypedColumn[T, U4],
  c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)]
----

| <<spark-sql-Dataset-untyped-transformations.adoc#selectExpr, selectExpr>>
a| [[selectExpr]] Untyped transformation to...FIXME

[source, scala]
----
selectExpr(exprs: String*): DataFrame
----

| <<spark-sql-Dataset-actions.adoc#show, show>>
a| [[show]] Action to...FIXME

[source, scala]
----
show(): Unit
show(truncate: Boolean): Unit
show(numRows: Int): Unit
show(numRows: Int, truncate: Boolean): Unit
show(numRows: Int, truncate: Int): Unit
show(numRows: Int, truncate: Int, vertical: Boolean): Unit // <1>
----
<1> *New in 2.3.0*

| <<spark-sql-Dataset-actions.adoc#summary, summary>>
| [[summary]] Action to calculate statistics (e.g. `count`, `mean`, `stddev`, `min`, `max` and `25%`, `50%`, `75%` percentiles)

| <<spark-sql-Dataset-actions.adoc#take, take>>
| Action to take n first records of the `Dataset`

| <<spark-sql-Dataset-untyped-transformations.adoc#toDF, toDF>>
| Converts a `Dataset` to a `DataFrame`

| <<spark-sql-Dataset-typed-transformations.adoc#toJSON, toJSON>>
|

| <<spark-sql-Dataset-actions.adoc#toLocalIterator, toLocalIterator>>
| [[toLocalIterator]] Action to...FIXME

| <<spark-sql-Dataset-typed-transformations.adoc#transform, transform>>
| Transforms a `Dataset`

| <<spark-sql-Dataset-typed-transformations.adoc#where, where>>
|

| <<spark-sql-Dataset-typed-transformations.adoc#withWatermark, withWatermark>>
| (*Spark Structured Streaming*) Creates a streaming `Dataset` with `EventTimeWatermark` logical operator

| <<write, write>>
|

| <<writeStream, writeStream>>
| (*Spark Structured Streaming*)
|===

=== [[write]] Accessing DataFrameWriter (to Describe Writing Dataset) -- `write` Operator

[source, scala]
----
write: DataFrameWriter[T]
----

`write` gives link:spark-sql-DataFrameWriter.adoc[DataFrameWriter] for records of type `T`.

[source, scala]
----
import org.apache.spark.sql.{DataFrameWriter, Dataset}
val ints: Dataset[Int] = (0 to 5).toDS
val writer: DataFrameWriter[Int] = ints.write
----

=== [[schema]] Accessing Schema -- `schema` Method

A `Dataset` has a *schema*.

[source, scala]
----
schema: StructType
----

[TIP]
====
You may also use the following methods to learn about the schema:

* `printSchema(): Unit`
* <<spark-sql-Dataset-basic-actions.adoc#explain, explain>>
====

=== [[rdd]] Generating RDD of Internal Binary Rows -- `rdd` Attribute

[source, scala]
----
rdd: RDD[T]
----

Whenever you are in need to convert a `Dataset` into a `RDD`, executing `rdd` method gives you the RDD of the proper input object type (not link:spark-sql-DataFrame.adoc#features[Row as in DataFrames]) that sits behind the `Dataset`.

[source, scala]
----
scala> val rdd = tokens.rdd
rdd: org.apache.spark.rdd.RDD[Token] = MapPartitionsRDD[11] at rdd at <console>:30
----

Internally, it looks link:spark-sql-ExpressionEncoder.adoc[ExpressionEncoder] (for the `Dataset`) up and accesses the `deserializer` expression. That gives the link:spark-sql-DataType.adoc[DataType] of the result of evaluating the expression.

NOTE: A deserializer expression is used to decode an link:spark-sql-InternalRow.adoc[InternalRow] to an object of type `T`. See link:spark-sql-ExpressionEncoder.adoc[ExpressionEncoder].

It then executes a link:spark-sql-LogicalPlan-DeserializeToObject.adoc[`DeserializeToObject` logical operator] that will produce a `RDD[InternalRow]` that is converted into the proper `RDD[T]` using the `DataType` and `T`.

NOTE: It is a lazy operation that "produces" a `RDD[T]`.
