== [[DataFrameWriter]] DataFrameWriter

`DataFrameWriter` is the <<methods, public interface>> to write the result of executing a structured query (i.e. the content of a link:spark-sql-Dataset.adoc[Dataset]) to an external storage system in a batch fashion.

[[methods]]
.DataFrameWriter API
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| <<bucketBy, bucketBy>>
a|

[source, scala]
----
bucketBy(numBuckets: Int, colName: String, colNames: String*): DataFrameWriter[T]
----

| <<csv, csv>>
a|

[source, scala]
----
csv(path: String): Unit
----

| <<format, format>>
a|

[source, scala]
----
format(source: String): DataFrameWriter[T]
----

| <<insertInto, insertInto>>
a| Inserts a DataFrame into a table

[source, scala]
----
insertInto(tableName: String): Unit
----

| <<jdbc, jdbc>>
a|

[source, scala]
----
jdbc(url: String, table: String, connectionProperties: Properties): Unit
----

| <<json, json>>
a|

[source, scala]
----
json(path: String): Unit
----

| <<mode, mode>>
a|

[source, scala]
----
mode(saveMode: SaveMode): DataFrameWriter[T]
mode(saveMode: String): DataFrameWriter[T]
----

| <<option, option>>
a|

[source, scala]
----
option(key: String, value: String): DataFrameWriter[T]
option(key: String, value: Boolean): DataFrameWriter[T]
option(key: String, value: Long): DataFrameWriter[T]
option(key: String, value: Double): DataFrameWriter[T]
----

| <<options, options>>
a|

[source, scala]
----
options(options: scala.collection.Map[String, String]): DataFrameWriter[T]
----

| <<orc, orc>>
a|

[source, scala]
----
orc(path: String): Unit
----

| <<parquet, parquet>>
a|

[source, scala]
----
parquet(path: String): Unit
----

| <<partitionBy, partitionBy>>
a|

[source, scala]
----
partitionBy(colNames: String*): DataFrameWriter[T]
----

| <<save, save>>
a|

[source, scala]
----
save(): Unit
save(path: String): Unit
----

| <<saveAsTable, saveAsTable>>
a|

[source, scala]
----
saveAsTable(tableName: String): Unit
----

| <<sortBy, sortBy>>
a|

[source, scala]
----
sortBy(colName: String, colNames: String*): DataFrameWriter[T]
----

| <<text, text>>
a|

[source, scala]
----
text(path: String): Unit
----
|===

`DataFrameWriter` is available using link:spark-sql-DataFrame.adoc#write[write] method of a `Dataset`.

[source, scala]
----
import org.apache.spark.sql.DataFrameWriter
val nums: Dataset[Long] = ...
val writer: DataFrameWriter[Long] = nums.write
----

`DataFrameWriter` has a direct support for many <<writing-dataframes-to-files, file formats>>, <<jdbc, JDBC databases>> and an extension point to plug in <<format, new formats>>.

`DataFrameWriter` defaults to <<parquet, parquet>> data source format. You can change the default format using link:spark-sql-properties.adoc[spark.sql.sources.default] configuration property or <<format, format>> or the format-specific methods.

[source, scala]
----
// see above for writer definition

// Save dataset in Parquet format
writer.save(path = "nums")

// Save dataset in JSON format
writer.format("json").save(path = "nums-json")

// Alternatively, use format-specific method
write.json(path = "nums-json")
----

In the end, you trigger the actual saving of the content of a `Dataset` (i.e. the result of executing a structured query) using <<save, save>> method.

[source, scala]
----
writer.save
----

NOTE: `DataFrameWriter` is really a type constructor in Scala and keeps a reference to a source `DataFrame` during its lifecycle (starting right from the moment it was created).

NOTE: Spark Structured Streaming's `DataStreamWriter` is responsible for writing the content of streaming Datasets in a streaming fashion.

=== [[runCommand]] Running Logical Command -- `runCommand` Internal Method

[source, scala]
----
runCommand
  (session: SparkSession, name: String)
  (command: LogicalPlan): Unit
----

`runCommand`...FIXME

NOTE: `runCommand` is used when `DataFrameWriter` is requested for <<save, save>>, <<insertInto, insertInto>> and <<createTable, createTable>> (that is used exclusively for <<saveAsTable, saveAsTable>>).

=== [[internal-state]] Internal State

`DataFrameWriter` uses the following mutable attributes to build a properly-defined write specification for <<insertInto, insertInto>>, <<saveAsTable, saveAsTable>>, and <<save, save>>:

.Attributes and Corresponding Setters
[cols="1,2",options="header"]
|===
| Attribute
| Setters

| [[source]] `source`
| <<format, format>>

| [[mode]] `mode`
| <<mode, mode>>

| [[extraOptions]] `extraOptions`
| <<option, option>>, <<options, options>>, <<save, save>>

| [[partitioningColumns]] `partitioningColumns`
| <<partitionBy, partitionBy>>

| [[bucketColumnNames]] `bucketColumnNames`
| <<bucketBy, bucketBy>>

| [[numBuckets]] `numBuckets`
| <<bucketBy, bucketBy>>

| [[sortColumnNames]] `sortColumnNames`
| <<sortBy, sortBy>>
|===

=== [[saveAsTable]] `saveAsTable` Method

[source, scala]
----
saveAsTable(tableName: String): Unit
----

`saveAsTable` saves the content of a `DataFrame` as the `tableName` table.

First, `tableName` is parsed to an internal table identifier. `saveAsTable` then checks whether the table exists or not and uses <<mode, save mode>> to decide what to do.

`saveAsTable` uses the link:spark-sql-SessionCatalog.adoc[SessionCatalog] for the current session.

.saveAsTable's Behaviour per Save Mode
[cols="1,1,2",options="header"]
|===
| Does table exist?
| Save Mode
| Behaviour

| yes
| `Ignore`
| Does nothing

| yes
| `ErrorIfExists`
| Reports an `AnalysisException` with `Table [tableIdent] already exists.` error message

| _anything_
| _anything_
| Creates a link:spark-sql-CatalogTable.adoc[CatalogTable] and link:spark-sql-SessionState.adoc#executePlan[executes] the link:spark-sql-LogicalPlan-CreateTable.adoc[CreateTable] logical operator
|===

[source, scala]
----
val ids = spark.range(5)
ids.write.
  option("path", "/tmp/five_ids").
  saveAsTable("five_ids")

// Check out if saveAsTable as five_ids was successful
val q = spark.catalog.listTables.filter($"name" === "five_ids")
scala> q.show
+--------+--------+-----------+---------+-----------+
|    name|database|description|tableType|isTemporary|
+--------+--------+-----------+---------+-----------+
|five_ids| default|       null| EXTERNAL|      false|
+--------+--------+-----------+---------+-----------+
----

=== [[save]] Saving Result of Structured Query (DataFrame) to Data Source -- `save` Method

[source, scala]
----
save(): Unit
----

`save` saves the result of a structured query (a <<spark-sql-Dataset.adoc#, Dataset>>) to a data source.

Internally, `save` uses `DataSource` to <<spark-sql-DataSource.adoc#lookupDataSource, look up the class of the requested data source>> (for the <<source, source>> option and the <<spark-sql-SessionState.adoc#conf, SQLConf>>).

[NOTE]
====
`save` uses <<spark-sql-Dataset.adoc#sparkSession, SparkSession>> to access the <<spark-sql-SparkSession.adoc#sessionState, SessionState>> that is in turn used to access the <<spark-sql-SessionState.adoc#conf, SQLConf>>.

[source, scala]
----
val df: DataFrame = ???
df.sparkSession.sessionState.conf
----
====

If the class is a <<spark-sql-DataSourceV2.adoc#, DataSourceV2>>...FIXME

Otherwise, if not a <<spark-sql-DataSourceV2.adoc#, DataSourceV2>>, `save` simply <<saveToV1Source, saveToV1Source>>.

`save` does not support saving to Hive (i.e. the <<source, source>> is `hive`) and throws an `AnalysisException` when requested so.

```
Hive data source can only be used with tables, you can not write files of Hive data source directly.
```

`save` <<assertNotBucketed, does not support bucketing>> (i.e. when the <<numBuckets, numBuckets>> or <<sortColumnNames, sortColumnNames>> options are defined) and throws an `AnalysisException` when requested so.

```
'[operation]' does not support bucketing right now
```

=== [[jdbc]] `jdbc` Method

[source, scala]
----
jdbc(url: String, table: String, connectionProperties: Properties): Unit
----

`jdbc` method saves the content of the `DataFrame` to an external database table via JDBC.

You can use <<mode, mode>> to control *save mode*, i.e. what happens when an external table exists when `save` is executed.

It is assumed that the `jdbc` save pipeline is not <<partitionBy, partitioned>> and <<bucketBy, bucketed>>.

All <<options, options>> are overriden by the input `connectionProperties`.

The required options are:

* `driver` which is the class name of the JDBC driver (that is passed to Spark's own `DriverRegistry.register` and later used to `connect(url, properties)`).

When `table` exists and the <<mode, override save mode>> is in use, `DROP TABLE table` is executed.

It creates the input `table` (using `CREATE TABLE table (schema)` where `schema` is the schema of the `DataFrame`).

=== [[bucketBy]] `bucketBy` Method

[source, scala]
----
bucketBy(numBuckets: Int, colName: String, colNames: String*): DataFrameWriter[T]
----

`bucketBy` simply sets the internal <<numBuckets, numBuckets>> and <<bucketColumnNames, bucketColumnNames>> to the input `numBuckets` and `colName` with `colNames`, respectively.

[source, scala]
----
val df = spark.range(5)
import org.apache.spark.sql.DataFrameWriter
val writer: DataFrameWriter[java.lang.Long] = df.write

val bucketedTable = writer.bucketBy(numBuckets = 8, "col1", "col2")

scala> :type bucketedTable
org.apache.spark.sql.DataFrameWriter[Long]
----

=== [[partitionBy]] `partitionBy` Method

[source, scala]
----
partitionBy(colNames: String*): DataFrameWriter[T]
----

CAUTION: FIXME

=== [[mode]] Defining Write Behaviour Per Sink's Existence (aka Save Mode) -- `mode` Method

[source, scala]
----
mode(saveMode: String): DataFrameWriter[T]
mode(saveMode: SaveMode): DataFrameWriter[T]
----

`mode` defines the behaviour of <<save, save>> when an external file or table (Spark writes to) already exists, i.e. `SaveMode`.

[[SaveMode]]
.Types of SaveMode
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| `Append`
| Records are appended to existing data.

| `ErrorIfExists`
| Exception is thrown.

| `Ignore`
| Do not save the records and not change the existing data in any way.

| `Overwrite`
| Existing data is overwritten by new records.
|===

=== [[sortBy]] Specifying Sorting Columns -- `sortBy` Method

[source, scala]
----
sortBy(colName: String, colNames: String*): DataFrameWriter[T]
----

`sortBy` simply sets <<sortColumnNames, sorting columns>> to the input `colName` and `colNames` column names.

NOTE: `sortBy` must be used together with <<bucketBy, bucketBy>> or `DataFrameWriter` reports an `IllegalArgumentException`.

NOTE: <<assertNotBucketed, assertNotBucketed>> asserts that bucketing is not used by some methods.

=== [[option]][[options]] Writer Configuration -- `option` and `options` Methods

CAUTION: FIXME

=== [[writing-dataframes-to-files]] Writing DataFrames to Files

CAUTION: FIXME

=== [[format]] Specifying Alias or Fully-Qualified Class Name of DataSource -- `format` Method

CAUTION: FIXME Compare to DataFrameReader.

=== [[parquet]] Parquet

CAUTION: FIXME

NOTE: Parquet is the default data source format.

=== [[insertInto]] Inserting DataFrame into Table -- `insertInto` Method

[source, scala]
----
insertInto(tableName: String): Unit // <1>
insertInto(tableIdent: TableIdentifier): Unit
----
<1> Parses `tableName` and calls the other `insertInto` with a `TableIdentifier`

`insertInto` inserts the content of the `DataFrame` to the specified `tableName` table.

NOTE: `insertInto` ignores column names and just uses a position-based resolution, i.e. the order (not the names!) of the columns in (the output of) the Dataset matters.

Internally, `insertInto` creates an link:spark-sql-LogicalPlan-InsertIntoTable.adoc#creating-instance[InsertIntoTable] logical operator (with link:spark-sql-LogicalPlan-UnresolvedRelation.adoc#creating-instance[UnresolvedRelation] operator as the only child) and <<runCommand, executes>> it right away (that submits a Spark job).

.DataFrameWrite.insertInto Executes SQL Command (as a Spark job)
image::images/spark-sql-DataFrameWrite-insertInto-webui-query-details.png[align="center"]

`insertInto` reports a `AnalysisException` for bucketed DataFrames, i.e. <<numBuckets, buckets>> or <<sortColumnNames, sortColumnNames>> are defined.

```
'insertInto' does not support bucketing right now
```

[source, scala]
----
val writeSpec = spark.range(4).
  write.
  bucketBy(numBuckets = 3, colName = "id")
scala> writeSpec.insertInto("t1")
org.apache.spark.sql.AnalysisException: 'insertInto' does not support bucketing right now;
  at org.apache.spark.sql.DataFrameWriter.assertNotBucketed(DataFrameWriter.scala:334)
  at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:302)
  at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:298)
  ... 49 elided
----

`insertInto` reports a `AnalysisException` for partitioned DataFrames, i.e. <<partitioningColumns, partitioningColumns>> is defined.

[options="wrap"]
----
insertInto() can't be used together with partitionBy(). Partition columns have already been defined for the table. It is not necessary to use partitionBy().
----

[source, scala, options="wrap"]
----
val writeSpec = spark.range(4).
  write.
  partitionBy("id")
scala> writeSpec.insertInto("t1")
org.apache.spark.sql.AnalysisException: insertInto() can't be used together with partitionBy(). Partition columns have already be defined for the table. It is not necessary to use partitionBy().;
  at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:305)
  at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:298)
  ... 49 elided
----

=== [[getBucketSpec]] `getBucketSpec` Internal Method

[source, scala]
----
getBucketSpec: Option[BucketSpec]
----

`getBucketSpec` creates a link:spark-sql-BucketSpec.adoc#creating-instance[BucketSpec] with <<numBuckets, numBuckets>>, <<bucketColumnNames, bucketColumnNames>> and <<sortColumnNames, sortColumnNames>>.

`getBucketSpec` reports a `IllegalArgumentException` when <<numBuckets, numBuckets>> are not defined when <<sortColumnNames, sortColumnNames>> are.

```
sortBy must be used together with bucketBy
```

NOTE: `getBucketSpec` is used exclusively when `DataFrameWriter` is requested to <<createTable, create a table>>.

=== [[createTable]] Creating Table -- `createTable` Internal Method

[source, scala]
----
createTable(tableIdent: TableIdentifier): Unit
----

`createTable` requests `DataSource` to link:spark-sql-DataSource.adoc#buildStorageFormatFromOptions[buildStorageFormatFromOptions] for <<extraOptions, extraOptions>>.

`createTable` assumes `CatalogTableType.EXTERNAL` when link:spark-sql-CatalogStorageFormat.adoc#locationUri[location URI] of `CatalogStorageFormat` is defined and `CatalogTableType.MANAGED` otherwise.

`createTable` creates a link:spark-sql-CatalogTable.adoc#creating-instance[CatalogTable].

In the end, `createTable` creates a link:spark-sql-LogicalPlan-CreateTable.adoc#creating-instance[CreateTable] (with the table description, <<mode, mode>> and the link:spark-sql-Dataset.adoc#logicalPlan[logical query plan] of the dataset) and <<runCommand, runs>> it.

NOTE: `createTable` is used when `DataFrameWriter` does <<saveAsTable, saveAsTable>>.

=== [[assertNotBucketed]] `assertNotBucketed` Internal Method

[source, scala]
----
assertNotBucketed(operation: String): Unit
----

`assertNotBucketed` checks whether <<numBuckets, numBuckets>> or <<sortColumnNames, sortColumnNames>> are defined and if they do reports an `AnalysisException`:

```
'[operation]' does not support bucketing right now
```

NOTE: `assertNotBucketed` is used when `DataFrameWriter` is requested to <<save, save>>, <<insertInto, insertInto>> and <<jdbc, jdbc>>.

=== [[saveToV1Source]] Executing Logical Command for Writing to Data Source V1 -- `saveToV1Source` Internal Method

[source, scala]
----
saveToV1Source(): Unit
----

`saveToV1Source` creates a <<spark-sql-DataSource.adoc#apply, DataSource>> (for the <<source, source>> class name, the <<partitioningColumns, partitioningColumns>> and the <<extraOptions, extraOptions>>) and requests it for the <<spark-sql-DataSource.adoc#planForWriting, logical command for writing>> (with the <<mode, mode>> and the <<spark-sql-Dataset.adoc#logicalPlan, analyzed logical plan>> of the structured query).

NOTE: While requesting the <<spark-sql-Dataset.adoc#logicalPlan, analyzed logical plan>> of the structured query, `saveToV1Source` triggers execution of logical commands.

In the end, `saveToV1Source` <<runCommand, runs the logical command for writing>>.

[NOTE]
====
The <<spark-sql-DataSource.adoc#planForWriting, logical command for writing>> can be one of the following:

* A <<spark-sql-LogicalPlan-SaveIntoDataSourceCommand.adoc#, SaveIntoDataSourceCommand>> for <<spark-sql-CreatableRelationProvider.adoc#, CreatableRelationProviders>>

* An <<spark-sql-LogicalPlan-InsertIntoHadoopFsRelationCommand.adoc#, InsertIntoHadoopFsRelationCommand>> for <<spark-sql-FileFormat.adoc#, FileFormats>>
====

NOTE: `saveToV1Source` is used exclusively when `DataFrameWriter` is requested to <<save, save the rows of a structured query (a DataFrame) to a data source>> (for all but <<spark-sql-DataSourceV2.adoc#, DataSourceV2>> writers with `WriteSupport`).
